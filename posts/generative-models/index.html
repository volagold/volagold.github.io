<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="author" content="Fei Li"><meta name="description" content="data science and programming blogs"><meta name="keywords" content="mathematics,data science,machine learning,deep learning,artificial intelligence,scientific computing,programming,python,julia"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:site" content="@lifeitech"><meta name="twitter:creator" content="@lifeitech"><meta property="og:image" content="https://volagold.github.io/assets/imgs/website-post.jpg"><meta property="og:title" content="volagold.github.io"><meta property="og:description" content="data science and programming blogs"><meta property="og:url" content="https://volagold.github.io"><meta property="og:type" content="article"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.4.1" /><meta property="og:title" content="Generative Models: from Noise to Data" /><meta property="og:locale" content="en" /><meta name="description" content="In this post I give an introduction to several generative models in deep learning literature, including adversarial networks (GANs), variational autoencoder (VAE) and flow models. I also derive the formula for evaluating flow densities on generated samples." /><meta property="og:description" content="In this post I give an introduction to several generative models in deep learning literature, including adversarial networks (GANs), variational autoencoder (VAE) and flow models. I also derive the formula for evaluating flow densities on generated samples." /><link rel="canonical" href="https://volagold.github.io/posts/generative-models/" /><meta property="og:url" content="https://volagold.github.io/posts/generative-models/" /><meta property="og:site_name" content="volagold.github.io" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-01-15T13:25:00+08:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Generative Models: from Noise to Data" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-01-07T21:00:22+08:00","datePublished":"2022-01-15T13:25:00+08:00","description":"In this post I give an introduction to several generative models in deep learning literature, including adversarial networks (GANs), variational autoencoder (VAE) and flow models. I also derive the formula for evaluating flow densities on generated samples.","headline":"Generative Models: from Noise to Data","mainEntityOfPage":{"@type":"WebPage","@id":"https://volagold.github.io/posts/generative-models/"},"url":"https://volagold.github.io/posts/generative-models/"}</script><title>Generative Models: from Noise to Data | volagold.github.io</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/imgs/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/imgs/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/imgs/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/imgs/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/imgs/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="volagold.github.io"><meta name="application-name" content="volagold.github.io"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/imgs/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.1.3/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.11.1/tocbot.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/magnific-popup.js/1.1.0/magnific-popup.min.css"> <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="https://avatars.githubusercontent.com/u/38937278" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">volagold.github.io</a></div><div class="site-subtitle">data science and programming blogs</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <span style="font-size: 1.2em;">üè†</span> <span>Home</span> </a><li class="nav-item"> <a href="/notes/" class="nav-link"> <span style="font-size: 1.2em;">üìï</span> <span>Notes</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <span style="font-size: 1.2em;">üìÅ</span> <span>Categories</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <span style="font-size: 1.2em;">üîñ</span> <span>Tags</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <span style="font-size: 1.2em;">üì¶</span> <span>Archives</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <span style="font-size: 1.2em;">‚ö°</span> <span>About</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://www.linkedin.com/" aria-label="linkedin" target="_blank" rel="noopener"> <i class="fab fa-linkedin"></i> </a> <a href="https://github.com/volagold" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['',''].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Generative Models: from Noise to Data</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:site" content="@lifeitech"><meta name="twitter:creator" content="@lifeitech"><meta property="og:image" content="https://volagold.github.io/assets/imgs/cat-gen.png"><meta property="og:title" content="Generative Models: from Noise to Data"><meta property="og:description" content="<blockquote><p>In this post I give an introduction to several generative models in deep learning literature, including adversarial networks (GANs), variational autoencoder (VAE) and flow models. I also derive the formula for evaluating flow densities on generated samples.</p></blockquote>"><meta property="og:url" content="https://volagold.github.io/posts/generative-models/"><meta property="og:type" content="article"> <img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 800 500'%3E%3C/svg%3E" data-proofer-ignore data-src="/assets/imgs/cat-gen.png" class="preview-img bg" alt="Generative Models: from Noise to Data" width="800" height="500" ><h1 data-toc-skip>Generative Models: from Noise to Data</h1><div class="post-meta text-muted"><div> By <em> <a href="https://volagold.github.io">volagold</a> </em></div><div class="d-flex"><div> <span> Posted <em class="timeago" date="2022-01-15 13:25:00 +0800" data-toggle="tooltip" data-placement="bottom" title="Sat, Jan 15, 2022, 1:25 PM +0800" >Jan 15, 2022</em> </span> <span> Updated <em class="timeago" date="2024-01-07 21:00:22 +0800 " data-toggle="tooltip" data-placement="bottom" title="Sun, Jan 7, 2024, 9:00 PM +0800" >Jan 7, 2024</em> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="2765 words"> <em>17 min</em> read</span></div></div></div><div class="post-content js-toc-content"><blockquote><p>In this post I give an introduction to several generative models in deep learning literature, including adversarial networks (GANs), variational autoencoder (VAE) and flow models. I also derive the formula for evaluating flow densities on generated samples.</p></blockquote><p>Deep generative models concern the problem of finding high dimensional data distribution, and new data generation. There are four major types of generative models in the literature: generative adversarial networks (GANs) <a href="#1">[1]</a>, variational autoencoder (VAE) <a href="#2">[2]</a>, normalizing flows [<a href="#3">3</a>,<a href="#4">4</a>,<a href="#5">5</a>], plus autoregressive models <a href="#6">[6]</a>. In all the first three models, the approach to data generation is to make use of some <em>noise space</em>, denoted as \(\mathcal{Z}\). A mapping from the noise space to data space is learned, using training data and various training methods. To obtain a sample from the generative model, one first samples a noise vector \(z\) from the noise space \(\mathcal{Z}\), and then inputs the noise vector to the learned mapping, and obtains generated sample from the model output. The intuition that why one should generate data from noise space via a learned mapping may come from the observation that for many common distributions like Gaussian, samples can be generated as a transformation of noises.</p><p>The three classes of models differ in interpretation of the noise space, model specification and learning methods. In <strong>GAN</strong>, a direct mapping from the noise space to data space is specified, where the noise space typically has smaller dimension than the data space. To train the model, one adds a classifier that discriminates between real and generated data, and jointly trains the two models through playing a minimax game. In <strong>VAE</strong> the mapping between noise space and data space is formulated in a probabilistic way, so that we can obtain the formula for likelihood on data. The likelihood is intractable to compute, so instead the model is trained by maximizing a lower bound on the log density. <strong>Flow models</strong> come with both exact likelihood evaluation as well as sample generation. Jacobians of the transformations between the two spaces are used to compute the likelihood.</p><p>We now discuss these three generative models in in some detail. We shall use the following notations: \(\mathcal{X}\) denotes data space, which is usually a subset of \(\mathbb{R}^d\) for some dimension \(d\). \(\mathcal{Z}\) denotes noise space, and \(Z\) denotes a generic random variable taking values in \(\mathcal{Z}\) with distribution \(p_Z\), for example a standard multivariate Gaussian.</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1019 404'%3E%3C/svg%3E" data-proofer-ignore data-src="/assets/imgs/gen-models.png" alt="generative models (GAN, VAE and Flow)" width="1019" height="404" /> <em>Three Classes of Generative Models</em></p><h2 id="generative-adversarial-networks">Generative Adversarial Networks<a href="#generative-adversarial-networks"><i class="fas fa-hashtag"></i></a></h2></h2><p>In GAN, a data generator \(g_\alpha\) from noise space to data space is specified:</p>\[\begin{gathered} g_\alpha: \mathcal{Z}\to \mathcal{X}\\ z\mapsto x \end{gathered}\]<p>which is typically parametrized by neural networks. A simple distribution on \(\mathcal{Z}\), like a standard Gaussian \(p_Z\), is also specified. We would like to train the generator \(g_\alpha\) so that it transforms the distribution \(p_Z\) on \(\mathcal{Z}\) to the data distribution \(p_{\text{data}}\) on \(\mathcal{X}\) for as close as possible, i.e., given \(z\sim p_Z\), the output \(g_\alpha(z)\) should closely follow the data distribution. To achieve this goal, one adds a second <em>discriminator</em></p>\[D_\theta: \mathcal{X} \to [0,1]\]<p>that assigns its input a probability score indicating how likely it is from the true data distribution. If \(D_\theta(x)\) is close to \(1\) then it may classify \(x\) as coming from the true data distribution, while if \(D_\theta(x)\) is close to \(0\) then it may classify \(x\) as coming from the generator \(g_\alpha\). One then jointly trains the two model together by performing</p>\[\min_\alpha\max_\theta V(\alpha, \theta)\]<p>with</p><div style="overflow-x:auto;"> $$ V(\alpha, \theta) = \mathbb{E}_{x\sim p_{\text{data}}}\log D_\theta(x) + \mathbb{E}_{z\sim p_Z}\log [1 - D_\theta(g_\alpha(z))]. $$</div><p>We see that \(\max_\theta V(\alpha, \theta)\) means pushing up \(D_\theta\) on data manifold while pushing down \(D_\theta\) on generated output from \(g_\alpha\). On the other hand, \(\min V(\alpha, \theta)\) means finding \(\alpha\) so that \(g_\alpha(z)\) are located at places where \(D_\theta\) has large values, which should ideally be closer and closer to the true data distribution as training proceeds, from the update of \(D_\theta\) in \(\max_\theta V(\alpha, \theta)\) just mentioned.</p><h2 id="variational-autoencoders">Variational Autoencoders<a href="#variational-autoencoders"><i class="fas fa-hashtag"></i></a></h2></h2><p>A variational autoencoder consists of a probabilistic decoder \(p_\alpha(x\mid z)\) from latent space \(\mathcal{Z}\) to data space \(\mathcal{X}\), and a probabilistic encoder \(q_\phi(z\mid x)\) from \(\mathcal{X}\) to \(\mathcal{Z}\), both of which are parametrized by neural networks. Given \(z\) sampled from \(p_Z\), the decoder returns a conditional probability distribution \(p_\alpha(\cdot\mid z)\) on \(\mathcal{X}\). The unconditional density \(p_\alpha(x)\) is</p>\[p_\alpha(x) = \int p_\alpha(x\mid z)p_Z(z)dz\]<p>One would like to use the above equation to perform maximum likelihood estimation on training data. However, the integral is intractable to compute. Instead, one has to use the encoder to obtain a tractable lower bound on the log density, and maximize the lower bound:</p><div style="overflow-x:auto;"> $$ \begin{split} \log p_\alpha(x) &amp;= \log \int p_\alpha(x\mid z)p_Z(z)dz\\ &amp;= \log \int q_\phi(z\mid x)\frac{p_\alpha(x\mid z)p_Z(z)}{q_\phi(z\mid x)}dz\\ &amp;\geq \int q_\phi(z\mid x)\log\frac{p_\alpha(x\mid z)p_Z(z)}{q_\phi(z\mid x)}dz\\[1em] &amp;= \mathbb{E}_{z\sim q_\phi(z\mid x)}\log p_\alpha(x\mid z) - KL[ q_\phi(z\mid x)|| p_Z(z)]\\[1em] &amp;=: \mathcal{L}_{\alpha,\phi}(x), \end{split} $$</div><p>where the inequality is from Jensen‚Äôs inequality. To compute the lower bound \(\mathcal{L}_{\alpha,\phi}(x)\) on training data \(x^{(i)}\), we send the data to the encoder, to obtain a distribution \(q_\phi(z\mid x^{(i)})\) on \(\mathcal{Z}\). Then we can sample \(z\) from this distribution and feed \(z\) to the decoder network and evaluate the decoder distribution on \(x^{(i)}\). This is the first term \(\mathbb{E}_{z\sim q_\phi(z\mid x^{(i)})}\log p_\alpha(x^{(i)}\mid z)\) in \(\mathcal{L}_{\alpha,\phi}(x)\). We are also able to compute the KL divergence between \(q_\phi(z\mid x^{(i)})\) and the prior distribution \(p_Z(z)\) on \(\mathcal{Z}\), for example a standard Gaussian. Design choices of the decoder \(p_\alpha(x\mid z)\) depend on the type of data one is modeling. Common choice is to output multivariate Bernoulli for discrete data and multivariate Gaussian for continuous data. The prior distribution \(p_Z\) as well as the encoder \(q_\phi(z\mid x)\), however, should be assumed to be continuous. The reason is explained below.</p><p>To compute the gradient of \(\mathcal{L}_{\alpha,\phi}(x)\) with respect to \(\phi\), we have to differentiate through an expectation on the latent variables. We can use a technique called the <em>reparameterization trick</em> to avoid computing the integral of the expectation. We first describe it in general terms, then apply it to the current case.</p><p>Given a random variable \(X\) with density \(q_\alpha\), where \(\alpha\) is some set of parameters, and a scalar function \(f\), the problem is to compute the gradient of the expectation of \(f\) with respect to \(\alpha\):</p>\[\nabla_\alpha \mathbb{E}_{x\sim q_\alpha}f(x) = \nabla_\alpha \int f(x)q_\alpha(x)dx.\]<p>\(f\) may or may not depend on parameter \(\alpha\), but for simplicity we assume it doesn‚Äôt. If \(X\) can be expressed as a differentiable transformation of random variable \(Z\) with simple density \(p_Z\), for example a Gaussian or uniform distribution:</p>\[x = g_\alpha(z)\quad \text{for some }z\sim p_Z,\]<p>then we are able to move the expectation operation out:</p>\[\begin{split} \nabla_\alpha \mathbb{E}_{x\sim q_\alpha}f(x) &amp;= \nabla_\alpha \mathbb{E}_{z\sim p_Z}f(g_\alpha(z))\\ &amp;= \mathbb{E}_{z\sim p_Z}\nabla_\alpha f(g_\alpha(z)). \end{split}\]<p>In particular, when we have finite samples \(\{x_\alpha^{(1)},\ldots,x_\alpha^{(N)}\}\) from \(q_\alpha\), and we‚Äôd like to compute \(\nabla_\alpha\frac{1}{N}\sum_{i=1}^Nf\left(x_\alpha^{(i)}\right),\) we can take the random numbers \(\{z^{(1)},\ldots,z^{(N)}\}\) that are used to generate the samples, and compute \(\nabla_\alpha\frac{1}{N}\sum_{i=1}^Nf\left(g_\alpha\left(z^{(i)}\right)\right).\) An example is when \(q_\alpha\) is Gaussian with some mean \(\mu_\alpha\) and variance \(\sigma_\alpha\) that are both some closed-form functions of the parameter \(\alpha\). In this case \(g_\alpha\) is simply an affine transformation of standard Gaussian:</p>\[g_\alpha(z) = \mu_\alpha + \sigma_\alpha\cdot z \quad\text{where } z\sim \mathcal{N}(0,I)\]<p>and the gradient computation becomes</p>\[\nabla_\alpha\frac{1}{N}\sum_{i=1}^Nf\left(\mu_\alpha + z^{(i)}\cdot \sigma_\alpha\right).\]<p>In VAE, the prior distribution of latent space \(p_Z\) is often assumed to be standard Gaussian, and in the training process the aim is to match the encoder network \(q_\phi(z\mid x)\) to this prior for as much as possible. In view of the reparameterization trick explained above, we can directly formulate the encoder network \(q_\phi(z\mid x)\) as \(\mu_\phi(x)\) and \(\sigma_\phi(x)\) where they both can be some complicated nonlinear transformation of the input, for example fully connected neural networks. To sample noise/latent variable \(z\) from \(q_\phi(z\mid x)\), we simply sample \(\epsilon\sim \mathcal{N}(0,I)\) and transform the noise as</p>\[z = \mu_\phi(x) + \epsilon\cdot\sigma_\phi(x).\]<p>Then we can feed the \(z\) to the objective \(\log p(x\mid z)\). In particular, we can write the gradient of the the first term with respect to \(\phi\) in the lower bound on training data \(\{x^{(1)},\ldots,x^{(N)}\}\) as</p>\[\nabla_\phi\frac{1}{N}\sum_{i=1}^N\log p_\alpha\left(x^{(i)}\mid \mu_\phi(x^{(i)}) + \epsilon_i\cdot\sigma_\phi(x^{(i)})\right).\]<p>The reparameterization trick works only if the distribution with respect to which we are differentiating can be expressed as a differentiable (and in particular, continuous) transformation of simple noise distributions. In case the distribution is discrete, it is not possible to express the samples as such differentiable transformations, and one has to rely on other techniques.</p><h2 id="normalizing-flows">Normalizing Flows<a href="#normalizing-flows"><i class="fas fa-hashtag"></i></a></h2></h2><p>A normalizing flow model is an invertible mapping from data space to a noise space with the same dimensionality:</p>\[\begin{gathered} f_\alpha: \mathcal{X} \to \mathcal{Z}\\ x \mapsto z, \end{gathered}\]<p>together with a simple distribution \(p_Z\) on \(\mathcal{Z}\) like a standard Gaussian. To sample from the model, one first samples from the noise distribution \(z\sim p_Z\) and invert the mapping to obtain \(\tilde{x}=f^{-1}_\alpha(z)\). The distinct characteristic of flow models is that one can leverage the change of variable formula for probability distributions to obtain exact likelihood evaluation \(p_\alpha(x)\) on \(\mathcal{X}\):</p>\[p_\alpha(x) = p_Z(f_\alpha(x))\cdot\left|\frac{\partial}{\partial x}f_\alpha(x)\right|,\]<p>or</p>\[\log p_\alpha(x) = \log p_Z(f_\alpha(x))+ \log\left|\frac{\partial}{\partial x}f_\alpha(x)\right|,\]<p>where \(\left\lvert\frac{\partial}{\partial x}f_\alpha(x)\right\rvert\) denotes the absolute value of the determinant of the Jacobian of the transformation \(f_\alpha\) at point \(x\in\mathcal{X}\). Since we have exact log-likelihood, we can use MLE to train the model. One chooses the invertible transformation \(f_\alpha\) in such a way that this log determinant of Jacobian is easy to compute. Recall that if a square matrix \(A\) is triangular, then \(\det(A)\) is the product of the elements along the diagonal, so \(\log\lvert\det(A)\rvert\) is simply the summation of its diagonal entries after the log function is applied element-wise. Thus, one way to design a flow model is to use some basic transformations \(h_{\alpha_1},\ldots,h_{\alpha_m}\) as building blocks, all of which have such triangular Jacobian matrices, and compose them together to obtain the model as</p>\[f_\alpha = h_{\alpha_m}\circ\cdots\circ h_{\alpha_1}.\]<p>To compute model output \(\log p_\alpha(x)\), we map \(x\) through all the transformations to \(f_\alpha(x)\), evaluate \(p_Z\) at \(f_\alpha(x)\), then add all log absolute determinant of Jacobians of all transformations.</p><p>Examples of flow model include NICE <a href="#3">[3]</a>, Real NVP <a href="#4">[4]</a>, and Glow <a href="#5">[5]</a>. The basic building block of such model is the affine coupling layer. For an input \(x=(x_1,\ldots,x_d)\) with dimension \(d\), the output of the affine coupling layer \(y=(y_1,\ldots,y_d)\) is</p>\[\begin{cases} y_{1:d'} = x_{1:d'},\\[0.2cm] y_{d'+1:d} = x_{d'+1:d} \odot s(x_{1:d'}) + t(x_{1:d'}) \end{cases}\]<p>where \(d'&lt;d\). \(s(\cdot)\) is the scale function and \(t(\cdot)\) is the translation function, both of which can be complex transformation of their inputs, for example fully connected neural networks or convolutional neural networks. The Jacobian of the transformation is lower-triangular:</p>\[\frac{\partial y}{\partial x^T} = \begin{pmatrix} I_{d'} &amp; 0 \\[0.2cm] * &amp; \mathrm{diag}(s(x_{1:d'}))\end{pmatrix}\]<p>where \(*\) is the part that is not relevant for density calculation, and \(\mathrm{diag}(s(x_{1:d'}))\) is the diagonal matrix whose diagonal vector is \(s(x_{1:d'})\). We see that the log determinant of the affine coupling transformation is simply \(\texttt{sum}(\log\lvert s\rvert)\), namely we just need to apply absolute value and the the log function element-wise, and then sum the result. The transformation is also easily invertible:</p>\[\begin{cases} x_{1:d'} = y_{1:d'},\\[0.2cm] x_{d'+1:d} = (y_{d'+1:d} - t(x_{1:d'})) \odot s^{-1}(x_{1:d'}). \end{cases}\]<p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 610 308'%3E%3C/svg%3E" data-proofer-ignore data-src="/assets/imgs/affine-coupling.png" alt="Affine Coupling Layer" width="610" height="308" class="shadow" /> <em>Affine Coupling Layer</em></p><h3 id="autoregressive-flows">Autoregressive flows<a href="#autoregressive-flows"><i class="fas fa-hashtag"></i></a></h3></h3><p>Autoregressive flow models [<a href="#7">7</a>,<a href="#8">8</a>,<a href="#9">9</a>] can be seen as a specific class of normalizing flows. In autoregressive flow, an input data \(x=(x_1,x_2,\ldots,x_d)\in\mathbb{R}^d\) is mapped to the output noise \(z=(z_1,z_2,\ldots,z_d)\in\mathbb{R}^d\) through the following structure:</p>\[\begin{aligned} z_1 &amp;= f_\alpha(x_1)\\ z_2 &amp;= f_\alpha(x_2; x_1)\\ z_3 &amp;= f_\alpha(x_3; x_1, x_2)\\ \cdots &amp; \\ \end{aligned}\]<p>Namely, the transformation \(f_\alpha\) is such that the first output dimension only depends on the first input dimension, the second output dimension only depends on the first two input dimensions, and so on. With this design, the Jacobian of \(f_\alpha\) is naturally lower-triangular, so that we can obtain fast and exact likelihood evaluation in one single pass. One way to ensure this dependence of output dimension on input dimension is through multiplying the weights in fully connected networks by binary <em>masks</em>, an approach taken in MADE <a href="#7">[7]</a>.</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1800 1252'%3E%3C/svg%3E" data-proofer-ignore data-src="/assets/imgs/made.png" alt="MADE Model" width="1800" height="1252" style="max-width: 70%" class="shadow" /> <em>MADE Model</em></p><p>On the other hand, sampling time is linear in the number of dimensions: \(x_2\) has to be obtained after \(x_1\) is computed, \(x_3\) has to be obtained after both \(x_1\) and \(x_2\) are computed, and so on:</p>\[\begin{aligned} \tilde{x}_1 &amp;= f_\alpha^{-1}(z_1)\\ \tilde{x}_2 &amp;= f_\alpha^{-1}(z_2; \tilde{x}_1)\\ \tilde{x}_3 &amp;= f_\alpha^{-1}(z_3; \tilde{x}_1,\tilde{x}_2)\\ \cdots &amp; \\ \end{aligned}\]<p>Autoregressive flows are first derived from autoregressive models <a href="#6">[6]</a>, which model data distribution \(p(x)\) as the product of conditionals</p>\[p(x) = \prod_{i=1}^d p(x_i\mid x_1,\ldots, x_{i-1}),\]<p>where each conditional distribution could have some neural network structure with trainable parameters, and MLE is usually used to train the model.</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1000 522'%3E%3C/svg%3E" data-proofer-ignore data-src="/assets/imgs/autoregressive.png" alt="Autoregressive Model" width="1000" height="522" /> <em>Autoregressive Model</em></p><p>Whereas flow models typically only model continuous data, autoregressive models can model both continuous and discrete data.</p><p>Sampling from an autoregressive model is sequential: to obtain a sample \((\tilde{x}_1,\ldots,\tilde{x}_d)\in\mathbb{R}^d\) from the model, one has to first sample \(\tilde{x}_1\) from \(p(x_1)\), then sample \(\tilde{x}_2\) from \(p(x_2\mid \tilde{x}_1)\), then sample \(\tilde{x}_3\) from \(p(x_3\mid \tilde{x_1},\tilde{x}_2)\), and so on. Under some specification, the sample could be a differentiable and invertible transformation of some random noise. For example, if we model each conditional \(p(x_i\mid x_1,\ldots,x_{i-1})\) as Gaussian with mean \(\mu_i=\mu_i(x_1,\ldots,x_{i-1})\) and variance \(\sigma_i=\sigma_i(x_1,\ldots,x_{i-1})\) that are some complex transformations of \(x_1,\ldots,x_{i-1}\), then to sample \(\tilde{x}_i\sim p(x_i\mid x_1,\ldots,x_{i-1})\) is the same as to compute</p><div style="overflow-x:auto;"> $$ \tilde{x}_i=\mu_i + z_i \cdot \sigma_i \quad\text{with } z_i\sim \mathcal{N}(0,1),\quad \forall~i=1,\ldots,d. $$</div><p>In the other way,</p><div style="overflow-x:auto;"> $$ z_i=(x_i - \mu_i) / \sigma_i,\quad \forall~i=1,\ldots,d $$</div><p>and we see that the above two equations have the same structure as for autoregressive flow models. In this case, the autoregressive model is an autoregressive flow. The log absolute determinant of the Jacobian of this transformation is very easy to compute. Incidentally, we also see that the equation</p><div style="overflow-x:auto;"> $$ \tilde{x}_i=\mu_i + z_i \cdot \sigma_i \quad\text{with } z_i\sim \mathcal{N}(0,1),\quad \forall~i=1,\ldots,d. $$</div><p>is very similar in form to the reparameterization trick in VAE. The idea of both is to express samples as transformation of simple noises, whose derivatives are easy to compute.</p><p>Viewing an autoregressive model as a flow model also opens up the possibility of stacking multiple such models together so as to increase the complexity of the model. This is the approach taken in Masked Autoregressive Flow (MAF) <a href="#8">[8]</a>. The drawback of such autoregressive flow models is their lack of flexibility. If it is not the case that</p><div style="overflow-x:auto;"> $$ p(x_i\mid x_1,\ldots,x_{i-1}) \quad\forall i=1,\ldots,d $$</div><p>are Gaussians, which could be highly likely for real world data, then the model can have poor fit. An autoregressive model, on the other hand, does not make such assumptions, so it could be more flexible.</p><h3 id="evaluate-density-on-generated-samples">Evaluate density on generated samples<a href="#evaluate-density-on-generated-samples"><i class="fas fa-hashtag"></i></a></h3></h3><p>Finally, for flow models, we derive the formula for $p_\alpha(\tilde{x})$, where $\tilde{x}=f_\alpha^{-1}(z)$ (with $z\sim p_Z$) is a generated sample. To start, plug in the equation for $\tilde{x}$:</p>\[\begin{split} p_\alpha(\tilde{x}) &amp;= p_Z(f_\alpha(\tilde{x}))\cdot\left|\frac{\partial}{\partial x}f_\alpha(\tilde{x})\right|\\ &amp;= p_Z(f_\alpha\circ f_\alpha^{-1}(z))\cdot\left|\frac{\partial}{\partial x}f_\alpha(f_\alpha^{-1}(z))\right|\\ &amp;= p_Z(z)\cdot\left|\frac{\partial}{\partial x}f_\alpha(f_\alpha^{-1}(z))\right|.\\ \end{split}\]<p>From the chain rule in calculus and the determinant of the inverse, we get</p>\[\begin{split} \left|\frac{\partial}{\partial x}f_\alpha(f_\alpha^{-1}(z))\right| &amp;= \left|\left(\frac{\partial}{\partial z}f_\alpha^{-1}(z)\right)^{-1}\right| \\ &amp;= \left|\frac{\partial}{\partial z}f_\alpha^{-1}(z)\right|^{-1}.\\ \end{split}\]<p>Taking $\log$ on both sides gives us:</p><div style="overflow-x:auto;"> $$ \log\left|\frac{\partial}{\partial x}f_\alpha(f_\alpha^{-1}(z))\right| = -\log\left|\frac{\partial}{\partial z}f_\alpha^{-1}(z)\right|. $$</div><p>We thus arrived at the formula:</p><div style="overflow-x:auto;"> $$ \log p_\alpha(\tilde{x}) = \log p_Z(z) - \log\left|\frac{\partial}{\partial z}f_\alpha^{-1}(z)\right|. $$</div><p>Namely, $\log p_\alpha(\tilde{x})$ can be computed as the log density on noise samples $z$ minus the log-determinant of the Jacobians of the inverse transformation. The later is a standard output in common implementations of flow models.</p><p>The formula tells us that after we generate samples $\tilde{x}$ from the flow model through the inverse transformation, we can evaluate the density $p_\alpha(\tilde{x})$ for free. There is no need to go through additional calculations.</p><hr /><p>Cite as:</p><div class="language-bibtex highlighter-rouge"><div class="code-header"> <span label-text="Bibtex"><i class="fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre><td class="rouge-code"><pre><span class="nc">@article</span><span class="p">{</span><span class="nl">lifei2022generative</span><span class="p">,</span>
  <span class="na">title</span>   <span class="p">=</span> <span class="s">"Generative Models: from Noise to Data"</span><span class="p">,</span>
  <span class="na">author</span>  <span class="p">=</span> <span class="s">"Li, Fei"</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">"https://volagold.github.io"</span><span class="p">,</span>
  <span class="na">year</span>    <span class="p">=</span> <span class="s">"2022"</span><span class="p">,</span>
  <span class="na">url</span>     <span class="p">=</span> <span class="s">"https://volagold.github.io/posts/generative-models/"</span>
<span class="p">}</span>
</pre></table></code></div></div><h2 id="references">References<a href="#references"><i class="fas fa-hashtag"></i></a></h2></h2><p><a id="1">[1]</a> I. Goodfellow et al. ‚ÄúGenerative adversarial nets‚Äù. <em>Advances in neural information processing systems.</em> 2014, pp. 2672‚Äì2680.</p><p><a id="2">[2]</a> D. P. Kingma and M. Welling. ‚ÄúAuto-encoding variational bayes‚Äù. <em>arXiv preprint arXiv: 1312.6114</em> (2013).</p><p><a id="3">[3]</a> L. Dinh, D. Krueger, and Y. Bengio. ‚ÄúNice: Non-linear independent components estimation‚Äù. <em>arXiv preprint arXiv:1410.8516</em> (2014).</p><p><a id="4">[4]</a> L. Dinh, J. Sohl-Dickstein, and S. Bengio. ‚ÄúDensity estimation using real nvp‚Äù. <em>arXiv preprint arXiv:1605.08803</em> (2016)</p><p><a id="5">[5]</a> D. P. Kingma and P. Dhariwal. ‚ÄúGlow: Generative flow with invertible 1x1 convolutions‚Äù. <em>Advances in Neural Information Processing Systems.</em> 2018, pp. 10215‚Äì10224.</p><p><a id="6">[6]</a> A. v. d. Oord, N. Kalchbrenner, and K. Kavukcuoglu. ‚ÄúPixel recurrent neural networks‚Äù. <em>arXiv preprint arXiv:1601.06759</em> (2016).</p><p><a id="7">[7]</a> M. Germain et al. ‚ÄúMADE: Masked autoencoder for distribution estimation‚Äù. <em>International Conference on Machine Learning.</em> 2015, pp. 881‚Äì889</p><p><a id="8">[8]</a> G. Papamakarios, T. Pavlakou, and I. Murray. ‚ÄúMasked autoregressive flow for density estimation‚Äù. <em>Advances in Neural Information Processing Systems.</em> 2017, pp. 2338‚Äì2347.</p><p><a id="9">[9]</a> C.-W. Huang et al. ‚ÄúNeural autoregressive flows‚Äù. <em>arXiv preprint arXiv:1804.00779</em> (2018)</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/data-science/'>Data Science</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/generative-model/" class="post-tag no-text-decoration" >generative-model</a> <a href="/tags/gan/" class="post-tag no-text-decoration" >gan</a> <a href="/tags/vae/" class="post-tag no-text-decoration" >vae</a> <a href="/tags/flow/" class="post-tag no-text-decoration" >flow</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Generative Models: from Noise to Data - volagold.github.io&url=https://volagold.github.io/posts/generative-models/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Generative Models: from Noise to Data - volagold.github.io&u=https://volagold.github.io/posts/generative-models/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=Generative Models: from Noise to Data - volagold.github.io&url=https://volagold.github.io/posts/generative-models/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <a href="https://www.linkedin.com/sharing/share-offsite/?url=https://volagold.github.io/posts/generative-models/" data-toggle="tooltip" data-placement="top" title="Linkedin" target="_blank" rel="noopener" aria-label="Linkedin"> <i class="fa-fw fab fa-linkedin"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/nlp/">nlp</a> <a class="post-tag" href="/tags/app-development/">app-development</a> <a class="post-tag" href="/tags/database/">database</a> <a class="post-tag" href="/tags/generative-model/">generative-model</a> <a class="post-tag" href="/tags/javascript/">javascript</a> <a class="post-tag" href="/tags/math/">math</a> <a class="post-tag" href="/tags/react/">react</a> <a class="post-tag" href="/tags/web-development/">web-development</a> <a class="post-tag" href="/tags/attention/">attention</a> <a class="post-tag" href="/tags/big-data/">big-data</a></div></div></div><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" class="js-toc"></nav></div><script src="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.11.1/tocbot.min.js"></script> <script> tocbot.init({ tocSelector: '.js-toc', contentSelector: '.js-toc-content', headingSelector: 'h1, h2, h3', hasInnerContainers: true, }); </script></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/nce/"><div class="card-body"> <em class="timeago small" date="2022-02-20 15:20:00 +0800" >Feb 20, 2022</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Noise Contrastive Estimation</h3><div class="text-muted small"><p> Noise contrastive estimation (NCE) is a method to estimate high dimensional data distributions. It converts a generative learning problem to a discriminative learning one. Parameters of the dist...</p></div></div></a></div><div class="card"> <a href="/posts/sequence-labeling/"><div class="card-body"> <em class="timeago small" date="2022-03-26 09:00:00 +0800" >Mar 26, 2022</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Sequence Labeling with HMM and CRF</h3><div class="text-muted small"><p> Sequence labeling is a classical task in natural language processing. In this task, a program is expected to recognize certain information given a piece of text. This is challenging, because eve...</p></div></div></a></div><div class="card"> <a href="/posts/rnn/"><div class="card-body"> <em class="timeago small" date="2022-04-08 22:00:00 +0800" >Apr 8, 2022</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>RNN, LSTM and GRU</h3><div class="text-muted small"><p> In this post we introduce the recurrent neural network (RNN) model in natural language processing (NLP), as well as two improvements over it, long short-term memories (LSTM) and gated recurrent ...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/build-a-static-website/" class="btn btn-outline-primary" prompt="Older"><p>Building a website with Jekyll and GitHub Pages</p></a> <a href="/posts/deformation-retraction/" class="btn btn-outline-primary" prompt="Newer"><p>Deformation Retractions are „ÄåContinuous„Äç Retractions</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> ¬© 2025 <a href="https://volagold.github.io">volagold</a> <span></span></p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/nlp/">nlp</a> <a class="post-tag" href="/tags/app-development/">app-development</a> <a class="post-tag" href="/tags/database/">database</a> <a class="post-tag" href="/tags/generative-model/">generative-model</a> <a class="post-tag" href="/tags/javascript/">javascript</a> <a class="post-tag" href="/tags/math/">math</a> <a class="post-tag" href="/tags/react/">react</a> <a class="post-tag" href="/tags/web-development/">web-development</a> <a class="post-tag" href="/tags/attention/">attention</a> <a class="post-tag" href="/tags/big-data/">big-data</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdnjs.cloudflare.com/ajax/libs/simple-jekyll-search/1.9.2/simple-jekyll-search.min.js" referrerpolicy="no-referrer"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" referrerpolicy="no-referrer"></script> <script src="https://cdnjs.cloudflare.com/ajax/libs/magnific-popup.js/1.1.0/jquery.magnific-popup.min.js" integrity="sha512-IsNh5E3eYy3tr/JiX2Yx4vsCujtkhwl7SLqgnwLNgf04Hrt9BT9SXlLlZlWx+OK4ndzAoALhsMNcCmkggjZB1w==" crossorigin="anonymous" referrerpolicy="no-referrer"></script> <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.11/clipboard.min.js" integrity="sha512-7O5pXpc0oCRrxk8RUfDYFgn0nO1t+jLuIOQdOMRp4APB7uZ4vSjspzp5y6YDtDs4VzUSTbWzBFZ/LKJhnyFOKw==" crossorigin="anonymous" referrerpolicy="no-referrer"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { tags: 'ams', inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ], processEscapes: true } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.1/es5/tex-mml-chtml.min.js" integrity="sha512-lt3EkmQb16BgAXR0iCk+JUJyDFmS9NZEMXCXK169qQoWcXu9CS4feejtxkjjUruw/Y0XfL1qxh41xVQPvCxM1A==" crossorigin="anonymous" referrerpolicy="no-referrer"></script> <script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.1.3/js/bootstrap.bundle.min.js" integrity="sha512-pax4MlgXjHEPfCwcJLQhigY7+N8rt6bVvWLFyUMuxShv170X53TRzGPmPkZmGBhk+jikR8WBM4yl7A9WMHHqvg==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
