<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="author" content="Fei Li"><meta name="description" content="data science and programming blogs"><meta name="keywords" content="mathematics,data science,machine learning,deep learning,artificial intelligence,scientific computing,programming,python,julia"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:site" content="@lifeitech"><meta name="twitter:creator" content="@lifeitech"><meta property="og:image" content="https://volagold.github.io/assets/imgs/website-post.jpg"><meta property="og:title" content="volagold.github.io"><meta property="og:description" content="data science and programming blogs"><meta property="og:url" content="https://volagold.github.io"><meta property="og:type" content="article"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.4.1" /><meta property="og:title" content="Embeddings and the word2vec Method" /><meta property="og:locale" content="en" /><meta name="description" content="On its surface, words are not numbers. But words have similarities, differences, associations, relations and connections. For example, ‚Äò‚Äòman‚Äô‚Äô and ‚Äò‚Äòwoman‚Äô‚Äô are words that describe humans, while ‚Äò‚Äòapple‚Äô‚Äô and ‚Äò‚Äòwatermelon‚Äô‚Äô are fruit names. The flow of words in an article always follow certain rules. How to quantify a word so as to reflex its relationship with other words, as well as its statistics in training text data? In this post, we‚Äôll look at word embeddings, i.e. mapping of words in dictionary to multidimensional real vectors. We discuss two specific embedding methods, TF-IDF and word2vec. After words are embedded into a vector space, it is then possible to apply neural network models to them to output probabilities on words." /><meta property="og:description" content="On its surface, words are not numbers. But words have similarities, differences, associations, relations and connections. For example, ‚Äò‚Äòman‚Äô‚Äô and ‚Äò‚Äòwoman‚Äô‚Äô are words that describe humans, while ‚Äò‚Äòapple‚Äô‚Äô and ‚Äò‚Äòwatermelon‚Äô‚Äô are fruit names. The flow of words in an article always follow certain rules. How to quantify a word so as to reflex its relationship with other words, as well as its statistics in training text data? In this post, we‚Äôll look at word embeddings, i.e. mapping of words in dictionary to multidimensional real vectors. We discuss two specific embedding methods, TF-IDF and word2vec. After words are embedded into a vector space, it is then possible to apply neural network models to them to output probabilities on words." /><link rel="canonical" href="https://volagold.github.io/posts/embedding-and-word2vec/" /><meta property="og:url" content="https://volagold.github.io/posts/embedding-and-word2vec/" /><meta property="og:site_name" content="volagold.github.io" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-03-20T09:00:00+08:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Embeddings and the word2vec Method" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-10-19T21:57:24+08:00","datePublished":"2022-03-20T09:00:00+08:00","description":"On its surface, words are not numbers. But words have similarities, differences, associations, relations and connections. For example, ‚Äò‚Äòman‚Äô‚Äô and ‚Äò‚Äòwoman‚Äô‚Äô are words that describe humans, while ‚Äò‚Äòapple‚Äô‚Äô and ‚Äò‚Äòwatermelon‚Äô‚Äô are fruit names. The flow of words in an article always follow certain rules. How to quantify a word so as to reflex its relationship with other words, as well as its statistics in training text data? In this post, we‚Äôll look at word embeddings, i.e. mapping of words in dictionary to multidimensional real vectors. We discuss two specific embedding methods, TF-IDF and word2vec. After words are embedded into a vector space, it is then possible to apply neural network models to them to output probabilities on words.","headline":"Embeddings and the word2vec Method","mainEntityOfPage":{"@type":"WebPage","@id":"https://volagold.github.io/posts/embedding-and-word2vec/"},"url":"https://volagold.github.io/posts/embedding-and-word2vec/"}</script><title>Embeddings and the word2vec Method | volagold.github.io</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/imgs/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/imgs/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/imgs/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/imgs/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/imgs/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="volagold.github.io"><meta name="application-name" content="volagold.github.io"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/imgs/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.1.3/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.11.1/tocbot.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/magnific-popup.js/1.1.0/magnific-popup.min.css"> <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="https://avatars.githubusercontent.com/u/38937278" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">volagold.github.io</a></div><div class="site-subtitle">data science and programming blogs</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <span style="font-size: 1.2em;">üè†</span> <span>Home</span> </a><li class="nav-item"> <a href="/notes/" class="nav-link"> <span style="font-size: 1.2em;">üìï</span> <span>Notes</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <span style="font-size: 1.2em;">üìÅ</span> <span>Categories</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <span style="font-size: 1.2em;">üîñ</span> <span>Tags</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <span style="font-size: 1.2em;">üì¶</span> <span>Archives</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <span style="font-size: 1.2em;">‚ö°</span> <span>About</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://www.linkedin.com/" aria-label="linkedin" target="_blank" rel="noopener"> <i class="fab fa-linkedin"></i> </a> <a href="https://github.com/volagold" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['',''].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Embeddings and the word2vec Method</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:site" content="@lifeitech"><meta name="twitter:creator" content="@lifeitech"><meta property="og:image" content="https://volagold.github.io/assets/imgs/embeddings-head.jpg"><meta property="og:title" content="Embeddings and the word2vec Method"><meta property="og:description" content="<blockquote><p>On its surface, words are not numbers. But words have similarities, differences, associations, relations and connections. For example, ‚Äò‚Äòman‚Äô‚Äô and ‚Äò‚Äòwoman‚Äô‚Äô are words that describe humans, while ‚Äò‚Äòapple‚Äô‚Äô and ‚Äò‚Äòwatermelon‚Äô‚Äô are fruit names. The flow of words in an article always follow certain rules. How to quantify a word so as to reflex its relationship with other words, as well as its statistics in training text data? In this post, we‚Äôll look at word embeddings, i.e. mapping of words in dictionary to multidimensional real vectors. We discuss two specific embedding methods, TF-IDF and word2vec. After words are embedded into a vector space, it is then possible to apply neural network models to them to output probabilities on words.</p></blockquote>"><meta property="og:url" content="https://volagold.github.io/posts/embedding-and-word2vec/"><meta property="og:type" content="article"> <img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 800 500'%3E%3C/svg%3E" data-proofer-ignore data-src="/assets/imgs/embeddings-head.jpg" class="preview-img bg" alt="Embeddings and the word2vec Method" width="800" height="500" ><h1 data-toc-skip>Embeddings and the word2vec Method</h1><div class="post-meta text-muted"><div> By <em> <a href="https://volagold.github.io">volagold</a> </em></div><div class="d-flex"><div> <span> Posted <em class="timeago" date="2022-03-20 09:00:00 +0800" data-toggle="tooltip" data-placement="bottom" title="Sun, Mar 20, 2022, 9:00 AM +0800" >Mar 20, 2022</em> </span> <span> Updated <em class="timeago" date="2023-10-19 17:27:24 +0330 " data-toggle="tooltip" data-placement="bottom" title="Thu, Oct 19, 2023, 5:27 PM +0330" >Oct 19, 2023</em> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="1585 words"> <em>10 min</em> read</span></div></div></div><div class="post-content js-toc-content"><blockquote><p>On its surface, words are not numbers. But words have similarities, differences, associations, relations and connections. For example, ‚Äò‚Äòman‚Äô‚Äô and ‚Äò‚Äòwoman‚Äô‚Äô are words that describe humans, while ‚Äò‚Äòapple‚Äô‚Äô and ‚Äò‚Äòwatermelon‚Äô‚Äô are fruit names. The flow of words in an article always follow certain rules. How to quantify a word so as to reflex its relationship with other words, as well as its statistics in training text data? In this post, we‚Äôll look at word embeddings, i.e. mapping of words in dictionary to multidimensional real vectors. We discuss two specific embedding methods, TF-IDF and word2vec. After words are embedded into a vector space, it is then possible to apply neural network models to them to output probabilities on words.</p></blockquote><p>Word embedding is the first step in most applications of NLP, like text analysis, text mining, machine translation and so on. For example, since a document is consisted of words, once we have word vectors, we can also represent a document by a vector in some way, for example by averaging over all word vectors in the document. Then we can compare similarity between two documents by $\cos(d_1,d_2)$. This is useful for applications like information retrieval, plagiarism detection, and news recommendation system.</p><h2 id="tf-idf">TF-IDF<a href="#tf-idf"><i class="fas fa-hashtag"></i></a></h2></h2><p>How to map or assign each word with a vector in some vector space? This entirely depends on our goal. One comes from <strong>information retrieval</strong>, where dimensions are documents $D$, and numerical value in each dimension $d$ of a word $w$ represents relevance/importance of that word $w$ to the document $d$. The list of word vectors (Table 1) is called a term-document matrix. We mention that dimensions can also be words themselves, where numerical value $(w_1, w_j)$ can represent number of times $w_j$ appears in a $\pm4$ window around $w_1$ in some training corpus.</p><p><img data-proofer-ignore data-src="/assets/imgs/term-document-matrix.png" alt="term-document matrix" /> <em>Table 1: term-document matrix</em></p><p>Once we mapped each word with a vector, the most basic way to measure similarity between two words $v$ and $w$ is to use cosine</p>\[\cos(v, w) = \frac{v\cdot w}{\|v\|\|w\|}.\]<p>Let‚Äôs now think about information retrieval. Given a query $q=(w_1,\ldots,w_n)$, we need to rank the collection of documents $D$. The most straightforward way is to assign each document $d$ a value for $w_1$, a value for $w_2$, and so on, and add them up, to get a final value for $d$. Then ank documents in $D$ according to the outputs of this (linear) ranking function.</p><p>How to assign a value to $(w,d)$ to represent relevance of word $w$ to document $d$? i.e., how to assign entry values in Table 1? The most straightforward approach is to count the number of times that $w$ appears in $d$:</p>\[\mathrm{tf}_{w,d} = \mathrm{count}(w,d).\]<p>Besides counting, there are other calculations (Table 2). Among them, the most common practice is to take $\log_{10}$ of the raw count</p>\[\mathrm{tf}_{w,d} = \log_{10}(\mathrm{count}(w,d)+1).\]<p><img data-proofer-ignore data-src="/assets/imgs/tf-terms.png" alt="" /> <em>Table 2: Variants of tf terms</em></p><p>Next, in the TF-IDF method, the TF term $\mathrm{tf}_{w,d}$ is weighted by the <strong>inverse document frequency</strong> of the word $w$ across a set of documents $D$. This means how common or rare a word is in the entire document set. The closer it is to $0$, the more common a word is. This metric can be calculated by taking the total number of documents, dividing it by the number of documents that contain a word, and calculating the logarithm:</p>\[\mathrm{idf}_{w} = \log_{10}\left(\frac{N}{\mathrm{df}_{w}}\right).\]<p>So, if a word is very common and appears in many documents, e.g. for words like ‚Äò‚Äò<em>the</em>‚Äô‚Äô, ‚Äò‚Äò<em>and</em>‚Äô‚Äô, or ‚Äò‚Äò<em>this</em>‚Äô‚Äô, this number will approach $0$. Otherwise, it will approach infinity.</p><p>Multiplying these two numbers results in the TF-IDF score of a word in a document</p>\[(w,d) = \mathrm{tf}_{w,d} \cdot \mathrm{idf}_{w}\]<p>The higher the score, the more relevant that word is in that particular document.</p><p>To summerize, TF-IDF is basically word count in a document, adjusted by some functions and document frequencies.</p><h2 id="word2vec">word2vec<a href="#word2vec"><i class="fas fa-hashtag"></i></a></h2></h2><p>From the viewpoint of embedding, the TF-IDF method produces <em>sparse</em> vectors (Table 1), with most entries being zero. In contrast, the word2vec method <a href="#1">[1]</a> produces <em>dense</em> word vectors, typically of dimension $50\sim1000$. Actually we are referring to ‚Äúskip-gram with negative sampling‚Äù in the word2vec software package, but it is often loosely referred to as word2vec.</p><p>First randomly initialize some word as a vector $w\in\mathbb{R}^d$. Define a binary classification task as follows: given $w\in\mathbb{R}^d$, predict if $c\in\mathbb{R}^d$ is a context word, i.e. if it should generally appear near $w$ in text data. Define the probability of being positive as</p>\[\mathbb{P}(+\mid w, c) = \sigma(w\cdot c) = \frac{1}{1+\exp(-w\cdot c)}.\]<p>(The probability of being negative is then</p><p>\(\mathbb{P}(-\mid w, c) = 1- \sigma(w\cdot c) = \frac{\exp(-w\cdot c)}{1+\exp(-w\cdot c)}.\) )</p><p>For context $c_{1:L}$, the model makes the assumption that all context words are independent, so the probability is</p>\[\mathbb{P}(+\mid w, c_{1:L}) = \prod_{i=1}^L\sigma(w\cdot c_i).\] \[\log\mathbb{P}(+\mid w, c_{1:L}) = \sum_{i=1}^L\log\sigma(w\cdot c_i).\]<p>For each word $w$, every nearby word within e.g. $L=\pm2$ window in the training data is a positive context. An example is as follows:</p><p><img data-proofer-ignore data-src="/assets/imgs/word2vec-context-window.png" alt="word2vec context window example" /></p><p>For every positive sample $(w,c)$, we generate $k$ noise samples $(w,n_1),\cdots,(w,n_k)$ as negative samples by randomly sampling the lexicon. In practice, it is common to use not the plain word frequencies, but</p>\[\mathbb{P}_{\alpha}(w) = \frac{\mathrm{count}(w)^\alpha}{\sum_{w'}\mathrm{count}(w')^\alpha}\]<p>as the probabilities, with $\alpha=0.75$, to give less frequent words higher probabilities.</p><p>Then, we can train the logistic regression model to classify positive samples and negative samples. For each sample pair \(\{(w,c),(w,n_1),\ldots,(w,n_k)\}\), the negative log-likelihood loss is</p>\[\begin{split} L &amp;= -\log\left[\mathbb{P}(+\mid w,c)\prod_{i=1}^k\mathbb{P}(-\mid w,n_i)\right]\\ &amp;= -\left[\log\sigma(w\cdot c) + \sum_{i=1}^k\log[1-\sigma(w\cdot n_i)]\right]. \end{split}\]<p>The word2vec embedding method is an example of <a href="https://lifei.tech/posts/nce/">Noise Contrastive Estimation</a>. When we need to learn parameters of a non-discriminative model for which we do not have an easy objective, we can convert the problem to a binary classification task and let the model learn to distinguish between positive training samples and noises. Here, a positive sample $(w,c)$ simply consists of a word $w$ and a surrounding word $c$. The weight $w\in\mathbb{R}^d$ is learned so that it is similar (i.e. has a large dot product value) to its surroundings in the training corpus.</p><p>Note that, in the end, for each word, two set of parameters are learned because a word can be either a word for which we want to have an embedding, and a context or noise for other words. Thus if we denote the vocabulary set as $V$, then the output of the algorithm is a matrix of $2\vert V\vert$ vectors, each of dimension $d$, formed by concatenating the target embedding $W$ and the context $+$ noise embedding $C$. It is common to add them together, representing word $i$ with the vector $w_i+c_i$. Alternatively we can throw away the $C$ matrix and just represent each word $i$ by the vector $w_i$.</p><h2 id="neural-language-models">Neural language models<a href="#neural-language-models"><i class="fas fa-hashtag"></i></a></h2></h2><p>Once we have word embeddings, we can use them as inputs to neural networks models, for modeling probabilities on words and sentences. Embeddings should produce better generalization than primitive models like <a href="https://lifei.tech/posts/nlp-prob-ngram/#probabilities-and-the-n-gram-model">$n$-gram models</a>. For example, suppose in the training data we have the sentence</p><p>‚Äú‚Ä¶make sure that the electric <em>car</em> gets charged‚Äù,</p><p>and our test set has</p><p>‚Äú‚Ä¶make sure that the electric <em>vehicle</em> gets [ ]‚Äù.</p><p>Suppose the word ‚Äúvehicle‚Äù is in the training set for the embedding, but there is no ‚Äò‚Äòvehicle gets‚Ä¶.‚Äô‚Äô. An $n$-gram model would be unable to infer a reasonable prediction from the training data. But a neural language model, knowing that ‚Äò‚Äòcar‚Äô‚Äô and ‚Äò‚Äòvehicle‚Äô‚Äô have similar embeddings, should be able to generalize from the ‚Äò‚Äòcar‚Äô‚Äô context to assign a high enough probability to ‚Äò‚Äòcharged‚Äô‚Äô following ‚Äò‚Äòvehicle‚Äô‚Äô.</p><p>Embeddings can also be learned during training. We can initialize $\vert V\vert$ embeddings, each of dimension $d$, and directly feed them into a neural network, without using embeddings pretrained by e.g. word2vec. In other words, we can encode each word as a one-hot vector $x$ of size $\vert V\vert\times1$, with value $1$ in some index and $0$ otherwise, and initialize a random $d\times\vert V\vert$ matrix $E$ as the embedding matrix. The product $Ex$ then selects the embedding for word $x$. The language model becomes</p>\[\begin{align*} e &amp;= (Ex_1,Ex_2,\ldots,Ex_{|V|}),\\ h &amp;= \sigma(We + b),\\ z &amp;= Uh,\\ \hat{y} &amp;= \mathrm{softmax}(z). \end{align*}\]<p>See Figure 1. The model is initialized with random weights. Training proceeds by concatenating all the sentences to a very long text and then iteratively moving through the text predicting each word $w_t$. The neural language model was first proposed by Bengio et al. <a href="#2">[2]</a>.</p><p><img data-proofer-ignore data-src="/assets/imgs/neural-language-model.png" alt="neural language model" /> <em>Figure 1: Neural language model</em></p><h2 id="summary">Summary<a href="#summary"><i class="fas fa-hashtag"></i></a></h2></h2><p>By embedding words into real vector spaces in a way that reflects their relations in training corpus, we are able to utilize model continuity for better generalization over $n$-gram models: a small input variation should yield a small output variation. word2vec is one such method, where a word vector is learned by distinguishing between its surrounding contexts and random noises. On the other hand, we can see from the figure above that the language model is data hungry: a large amount of training data is needed to cover all words and all possible contexts. If a word never appears in the training corpus, its embedding would not be learned. The model is still very ‚Äúcoarse‚Äù, and processes limited capacity such that problems like long term dependency are not addressed.</p><hr /><p>Cite as:</p><div class="language-bibtex highlighter-rouge"><div class="code-header"> <span label-text="Bibtex"><i class="fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre><td class="rouge-code"><pre><span class="nc">@article</span><span class="p">{</span><span class="nl">lifei2022embedding</span><span class="p">,</span>
  <span class="na">title</span>   <span class="p">=</span> <span class="s">"Embeddings and the word2vec Method"</span><span class="p">,</span>
  <span class="na">author</span>  <span class="p">=</span> <span class="s">"Li, Fei"</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">"https://volagold.github.io"</span><span class="p">,</span>
  <span class="na">year</span>    <span class="p">=</span> <span class="s">"2022"</span><span class="p">,</span>
  <span class="na">url</span>     <span class="p">=</span> <span class="s">"https://volagold.github.io/posts/embedding-and-word2vec/"</span>
<span class="p">}</span>
</pre></table></code></div></div><h2 id="references">References<a href="#references"><i class="fas fa-hashtag"></i></a></h2></h2><p><a id="1">[1]</a> Mikolov, Tomas, et al. ‚ÄúEfficient estimation of word representations in vector space.‚Äù <em>arXiv preprint arXiv:1301.3781</em> (2013).</p><p><a id="2">[2]</a> Bengio, Yoshua, R√©jean Ducharme, and Pascal Vincent. ‚ÄúA neural probabilistic language model.‚Äù <em>Advances in Neural Information Processing Systems</em> 13 (2000).</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/data-science/'>Data Science</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/nlp/" class="post-tag no-text-decoration" >nlp</a> <a href="/tags/tf-idf/" class="post-tag no-text-decoration" >tf-idf</a> <a href="/tags/word2vec/" class="post-tag no-text-decoration" >word2vec</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Embeddings and the word2vec Method - volagold.github.io&url=https://volagold.github.io/posts/embedding-and-word2vec/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Embeddings and the word2vec Method - volagold.github.io&u=https://volagold.github.io/posts/embedding-and-word2vec/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=Embeddings and the word2vec Method - volagold.github.io&url=https://volagold.github.io/posts/embedding-and-word2vec/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <a href="https://www.linkedin.com/sharing/share-offsite/?url=https://volagold.github.io/posts/embedding-and-word2vec/" data-toggle="tooltip" data-placement="top" title="Linkedin" target="_blank" rel="noopener" aria-label="Linkedin"> <i class="fa-fw fab fa-linkedin"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/nlp/">nlp</a> <a class="post-tag" href="/tags/app-development/">app-development</a> <a class="post-tag" href="/tags/database/">database</a> <a class="post-tag" href="/tags/generative-model/">generative-model</a> <a class="post-tag" href="/tags/javascript/">javascript</a> <a class="post-tag" href="/tags/math/">math</a> <a class="post-tag" href="/tags/react/">react</a> <a class="post-tag" href="/tags/web-development/">web-development</a> <a class="post-tag" href="/tags/attention/">attention</a> <a class="post-tag" href="/tags/big-data/">big-data</a></div></div></div><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" class="js-toc"></nav></div><script src="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.11.1/tocbot.min.js"></script> <script> tocbot.init({ tocSelector: '.js-toc', contentSelector: '.js-toc-content', headingSelector: 'h1, h2, h3', hasInnerContainers: true, }); </script></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/sequence-labeling/"><div class="card-body"> <em class="timeago small" date="2022-03-26 09:00:00 +0800" >Mar 26, 2022</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Sequence Labeling with HMM and CRF</h3><div class="text-muted small"><p> Sequence labeling is a classical task in natural language processing. In this task, a program is expected to recognize certain information given a piece of text. This is challenging, because eve...</p></div></div></a></div><div class="card"> <a href="/posts/rnn/"><div class="card-body"> <em class="timeago small" date="2022-04-08 22:00:00 +0800" >Apr 8, 2022</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>RNN, LSTM and GRU</h3><div class="text-muted small"><p> In this post we introduce the recurrent neural network (RNN) model in natural language processing (NLP), as well as two improvements over it, long short-term memories (LSTM) and gated recurrent ...</p></div></div></a></div><div class="card"> <a href="/posts/attention-and-transformers/"><div class="card-body"> <em class="timeago small" date="2022-04-11 22:30:00 +0800" >Apr 11, 2022</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Attention and Transformers</h3><div class="text-muted small"><p> In recent years, Transformer-based models are trending and are quickly taking up not only the field of NLP, but also computer vision and many other fields in AI. In this post, we give a tutorial...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/nlp-prob-ngram/" class="btn btn-outline-primary" prompt="Older"><p>Natural Language Processing, Probabilities and the n-gram Model</p></a> <a href="/posts/sequence-labeling/" class="btn btn-outline-primary" prompt="Newer"><p>Sequence Labeling with HMM and CRF</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> ¬© 2025 <a href="https://volagold.github.io">volagold</a> <span></span></p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/nlp/">nlp</a> <a class="post-tag" href="/tags/app-development/">app-development</a> <a class="post-tag" href="/tags/database/">database</a> <a class="post-tag" href="/tags/generative-model/">generative-model</a> <a class="post-tag" href="/tags/javascript/">javascript</a> <a class="post-tag" href="/tags/math/">math</a> <a class="post-tag" href="/tags/react/">react</a> <a class="post-tag" href="/tags/web-development/">web-development</a> <a class="post-tag" href="/tags/attention/">attention</a> <a class="post-tag" href="/tags/big-data/">big-data</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdnjs.cloudflare.com/ajax/libs/simple-jekyll-search/1.9.2/simple-jekyll-search.min.js" referrerpolicy="no-referrer"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" referrerpolicy="no-referrer"></script> <script src="https://cdnjs.cloudflare.com/ajax/libs/magnific-popup.js/1.1.0/jquery.magnific-popup.min.js" integrity="sha512-IsNh5E3eYy3tr/JiX2Yx4vsCujtkhwl7SLqgnwLNgf04Hrt9BT9SXlLlZlWx+OK4ndzAoALhsMNcCmkggjZB1w==" crossorigin="anonymous" referrerpolicy="no-referrer"></script> <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.11/clipboard.min.js" integrity="sha512-7O5pXpc0oCRrxk8RUfDYFgn0nO1t+jLuIOQdOMRp4APB7uZ4vSjspzp5y6YDtDs4VzUSTbWzBFZ/LKJhnyFOKw==" crossorigin="anonymous" referrerpolicy="no-referrer"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { tags: 'ams', inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ], processEscapes: true } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.1/es5/tex-mml-chtml.min.js" integrity="sha512-lt3EkmQb16BgAXR0iCk+JUJyDFmS9NZEMXCXK169qQoWcXu9CS4feejtxkjjUruw/Y0XfL1qxh41xVQPvCxM1A==" crossorigin="anonymous" referrerpolicy="no-referrer"></script> <script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.1.3/js/bootstrap.bundle.min.js" integrity="sha512-pax4MlgXjHEPfCwcJLQhigY7+N8rt6bVvWLFyUMuxShv170X53TRzGPmPkZmGBhk+jikR8WBM4yl7A9WMHHqvg==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
