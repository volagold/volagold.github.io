<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="author" content="Fei Li"><meta name="description" content="data science and programming blogs"><meta name="keywords" content="mathematics,data science,machine learning,deep learning,artificial intelligence,scientific computing,programming,python,julia"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:site" content="@lifeitech"><meta name="twitter:creator" content="@lifeitech"><meta property="og:image" content="https://volagold.github.io/assets/imgs/website-post.jpg"><meta property="og:title" content="volagold.github.io"><meta property="og:description" content="data science and programming blogs"><meta property="og:url" content="https://volagold.github.io"><meta property="og:type" content="article"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.4.1" /><meta property="og:title" content="Natural Language Processing, Probabilities and the n-gram Model" /><meta property="og:locale" content="en" /><meta name="description" content="This is the first one of a series of posts on Natural Language Processing (NLP). We give an introduction to the subject, mention two model evaluation methods, see how to assign probabilities to words and sentences, and learn the most basic model ‚Äî‚Äî the n-gram model in NLP." /><meta property="og:description" content="This is the first one of a series of posts on Natural Language Processing (NLP). We give an introduction to the subject, mention two model evaluation methods, see how to assign probabilities to words and sentences, and learn the most basic model ‚Äî‚Äî the n-gram model in NLP." /><link rel="canonical" href="https://volagold.github.io/posts/nlp-prob-ngram/" /><meta property="og:url" content="https://volagold.github.io/posts/nlp-prob-ngram/" /><meta property="og:site_name" content="volagold.github.io" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-03-05T16:20:00+08:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Natural Language Processing, Probabilities and the n-gram Model" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-04-04T09:53:55+08:00","datePublished":"2022-03-05T16:20:00+08:00","description":"This is the first one of a series of posts on Natural Language Processing (NLP). We give an introduction to the subject, mention two model evaluation methods, see how to assign probabilities to words and sentences, and learn the most basic model ‚Äî‚Äî the n-gram model in NLP.","headline":"Natural Language Processing, Probabilities and the n-gram Model","mainEntityOfPage":{"@type":"WebPage","@id":"https://volagold.github.io/posts/nlp-prob-ngram/"},"url":"https://volagold.github.io/posts/nlp-prob-ngram/"}</script><title>Natural Language Processing, Probabilities and the n-gram Model | volagold.github.io</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/imgs/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/imgs/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/imgs/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/imgs/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/imgs/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="volagold.github.io"><meta name="application-name" content="volagold.github.io"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/imgs/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.1.3/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.11.1/tocbot.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/magnific-popup.js/1.1.0/magnific-popup.min.css"> <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="https://avatars.githubusercontent.com/u/38937278" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">volagold.github.io</a></div><div class="site-subtitle">data science and programming blogs</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <span style="font-size: 1.2em;">üè†</span> <span>Home</span> </a><li class="nav-item"> <a href="/notes/" class="nav-link"> <span style="font-size: 1.2em;">üìï</span> <span>Notes</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <span style="font-size: 1.2em;">üìÅ</span> <span>Categories</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <span style="font-size: 1.2em;">üîñ</span> <span>Tags</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <span style="font-size: 1.2em;">üì¶</span> <span>Archives</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <span style="font-size: 1.2em;">‚ö°</span> <span>About</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://www.linkedin.com/" aria-label="linkedin" target="_blank" rel="noopener"> <i class="fab fa-linkedin"></i> </a> <a href="https://github.com/volagold" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['',''].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Natural Language Processing, Probabilities and the n-gram Model</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:site" content="@lifeitech"><meta name="twitter:creator" content="@lifeitech"><meta property="og:image" content="https://volagold.github.io/assets/imgs/nlp-head.jpg"><meta property="og:title" content="Natural Language Processing, Probabilities and the n-gram Model"><meta property="og:description" content="<blockquote><p>This is the first one of a series of posts on Natural Language Processing (NLP). We give an introduction to the subject, mention two model evaluation methods, see how to assign probabilities to words and sentences, and learn the most basic model ‚Äî‚Äî the n-gram model in NLP.</p></blockquote>"><meta property="og:url" content="https://volagold.github.io/posts/nlp-prob-ngram/"><meta property="og:type" content="article"> <img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 800 500'%3E%3C/svg%3E" data-proofer-ignore data-src="/assets/imgs/nlp-head.jpg" class="preview-img bg" alt="Natural Language Processing, Probabilities and the n-gram Model" width="800" height="500" ><h1 data-toc-skip>Natural Language Processing, Probabilities and the n-gram Model</h1><div class="post-meta text-muted"><div> By <em> <a href="https://volagold.github.io">volagold</a> </em></div><div class="d-flex"><div> <span> Posted <em class="timeago" date="2022-03-05 16:20:00 +0800" data-toggle="tooltip" data-placement="bottom" title="Sat, Mar 5, 2022, 4:20 PM +0800" >Mar 5, 2022</em> </span> <span> Updated <em class="timeago" date="2023-04-04 09:53:55 +0800 " data-toggle="tooltip" data-placement="bottom" title="Tue, Apr 4, 2023, 9:53 AM +0800" >Apr 4, 2023</em> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="1239 words"> <em>8 min</em> read</span></div></div></div><div class="post-content js-toc-content"><blockquote><p>This is the first one of a series of posts on Natural Language Processing (NLP). We give an introduction to the subject, mention two model evaluation methods, see how to assign probabilities to words and sentences, and learn the most basic model ‚Äî‚Äî the n-gram model in NLP.</p></blockquote><h2 id="introduction">Introduction<a href="#introduction"><i class="fas fa-hashtag"></i></a></h2></h2><p>The field of Natural Language Processing (NLP) is concerned with automatic processing of language data. It arose with the widespread usage of the Internet. There are many scenarios for which NLP techniques can be useful. An email service provider may wish to have a program to filter spam emails. Some data companies regularly scrape the web to extract information from millions of text entries. Marketing companies extract sentiments from online comments and reviews. Softwares can be developed to transcribe speech into texts, and translate one language to another. We can even ask machines to generate articles and reports for us.</p><p>If you think about it, it is very hard to transfer our knowledge about human languages to a computer program.</p><p>Languages are formed from the incentive to communicate information among creatures. They evolve from thousands of years of social development and transitions. A piece of text is not a mere permutation of words. Rather, it is a reflection of the 3D world, a condensed representation of much higher dimensional complexities. How could a model really understand the word ‚Äúsea‚Äù when it never have the chance to directly see the ocean like humans do? Much more complicated concepts like ‚Äúthink‚Äù, ‚Äúabstract‚Äù, ‚Äúeconomy‚Äù would be more challenging to learn. It is thus very difficult to train a computer program for it to understand and then manipulate texts that depend on external matters that are beyond language itself.</p><p>Just imagine how you would learn a foreign language solely based on a large amount of texts in that language. You are not given any vocabulary table, but you must learn the meaning of every word, the grammar, and every expression only based on statistics of texts. This sounds intimidating right? Now imagine doing the same task, this time being an infant and not understanding much of the world. You have to learn every concept like ‚Äúpolitics‚Äù and ‚Äútyranny‚Äù on your own‚Ä¶‚Ä¶I‚Äôm not sure if this is feasible at all. This thought experiment shows that, training a model to ‚Äúunderstand‚Äù human languages is never an obvious and easy task.</p><p>But to solve the problem, we don‚Äôt have to try hard to jump to the ultimate and perfect solution at one leap, if there is one. We can always start from the easiest approach, and think about how we can use a better way to improve over it.</p><h2 id="language-models">Language models<a href="#language-models"><i class="fas fa-hashtag"></i></a></h2></h2><p>A language model is a model that <strong>assigns probabilities to words and sentences</strong>.</p><p>Models are often trained to predict the next word give current contexts, i.e. output a probability distribution over vocabularies conditioned on previous texts. The output probabilities should reflect word statistics in the training data. Probabilities can be very useful for many applications. For example, for grammar correction, $\mathbb{P}(\text{This }\textit{means}\text{ in particular}) &gt; \mathbb{P}(\text{This }\textit{mean}\text{ in particular})$.</p><p>For evaluating models, the only way to see if a model improves the task at hand is through <strong>extrinsic evaluation</strong>, where you put two models in a real task, like spelling correction or speech recognition, and measure the performance of the two models. But it is time-consuming. Instead, one often uses <strong>intrinsic evaluation</strong>, namely after training two models, calculate probability outputs on some test set, and see which one gives higher numbers.</p><p>The common metric is the <strong>perplexity</strong>. For a test set $W=w_1\cdots w_N$ (i.e. the whole test set data, including all sentences), it computes</p>\[PP(w) = \sqrt[N]{\frac{1}{\mathbb{P}(w_1\cdots w_N)}}.\]<p>The lower the perplexity, the better. Minimizing perplexity is the same as maximizing probability.</p><h2 id="the-n-gram-model">The n-gram model<a href="#the-n-gram-model"><i class="fas fa-hashtag"></i></a></h2></h2><p>The n-grams model is the simplest model to achieve the goal of assigning probabilities to words: <strong>count the number of occurrences of each word (or word sequence)</strong>. A two-word sequence $w_{i-1}w_i$ is called a bigram, like ‚ÄúI love‚Äù. A three-word sequence is called a trigram, like ‚ÄúI love you‚Äù.</p><p>Specifically, suppose we want to compute $\mathbb{P}(w_{1:n})$, the probability of a word sequence. The chain rule of probability tells us to multiply together the conditional probabilities:</p>\[\mathbb{P}(w_{1:n}) = \mathbb{P}(w_1)\mathbb{P}(w_2\mid w_1)\mathbb{P}(w_3\mid w_{1:2})\cdots\mathbb{P}(w_n\mid w_{1:n-1}).\]<p>At this point, one makes the Markov assumption that the conditional probability of a word $w$ only depends on the previous 2 or 3 words. In <strong>bigram</strong> model, the probability only depends on the previous one word, i.e. $\mathbb{P}(w_n\mid w_{1:n-1})=\mathbb{P}(w_n\mid w_{n-1})$. <strong>Trigram</strong> model looks two words in the past, and so on. Using bigram model for example, the probability above simplifies to</p>\[\mathbb{P}(w_{1:n}) = \prod_{i=1}^n\mathbb{P}(w_i\mid w_{i-1}).\]<p>Now, at this point, the obvious way to estimate $\mathbb{P}(w_i\mid w_{i-1})$ is to count $w_{i-1}w_i$ among all bigrams starting with $w_{i-1}$, and calculate the proportion:</p>\[\mathbb{P}(w_i\mid w_{i-1}) = \frac{\#w_{i-1}w_i}{\sum_{w}\#w_{i-1}w},\]<p>which is also the MLE estimate.</p><p>An implementation detail: we shall add <code class="language-plaintext highlighter-rouge">&lt;s&gt;</code> and <code class="language-plaintext highlighter-rouge">&lt;/s&gt;</code> to the beginning and end of each sentence in the training data, like ‚Äú<code class="language-plaintext highlighter-rouge">&lt;s&gt;</code> I like you the way you are <code class="language-plaintext highlighter-rouge">&lt;/s&gt;</code>‚Äù, to give the bigram context of the first word, and also make the model a true distribution on all sentences.</p><p>This is basically the n-gram model. Just counting.</p><h3 id="drawbacks-and-remedies">Drawbacks and remedies<a href="#drawbacks-and-remedies"><i class="fas fa-hashtag"></i></a></h3></h3><p>n-gram models, while being simple, have obvious drawbacks.</p><ol><li><p>Words in a sentence can have <em>long-distance dependencies</em>, for example I can insert clauses into a sentence, to separate the subject and the object. Just like natural images are not likely to be color gradients, a paragraph is unlikely to develop in a linear, continuous and predictable way. In other words, languages, like many other high dimensional data, are highly <em>nonlinear</em>. The Markov assumption is the very drawback to this.</p><li><p>Since the model is a primitive statistics of the training data, it falls short of predicting anything that are not in the training data. Training data can only be sparse in data space, but any good AI model should have the ability to generalize. One common remedy to this is to do smoothing to the probabilities.</p><p>(a) Laplace (or add-one) smoothing ‚Äì add one to each count:</p>\[\mathbb{P}^*(w_n\mid w_{n-1}) = \frac{\#w_{i-1}w_i+1}{\sum_{w}(\#w_{i-1}w+1)} = \frac{\#w_{i-1}w_i+1}{\sum_{w}\#w_{i-1}w + V}.\]<p>Thus, if the $w_{i-1}$ never appears before, then we give any bigram $w_{i-1}w$ a probability of $1/V$.</p><p>(b) Add-$k$ smoothing, where $k\in[0,1]$:</p>\[\mathbb{P}^*(w_n\mid w_{n-1}) = \frac{\#w_{i-1}w_i+k}{\sum_{w}\#w_{i-1}w + kV}.\]<p>(c) Backoff: use trigram if the evidence is sufficient, otherwise use bigram, otherwise use unigram. In other words, we ‚Äúback off‚Äù if we have zero evidence for a higher-order n-gram.</p><p>(d) Interpolation: use weighted average of trigram, bigram and unigram estimates, for example</p>\[\mathbb{P}^*(w_n\mid w_{n-2}w_{n-1}) = \lambda_1\mathbb{P}(w_n\mid w_{n-2}w_{n-1}) + \lambda_2\mathbb{P}(w_n\mid w_{n-1}) + \lambda_3\mathbb{P}(w_n)\]<p>where $\lambda_1+\lambda_2+\lambda_3=1$.</p></ol><h2 id="summary">Summary<a href="#summary"><i class="fas fa-hashtag"></i></a></h2></h2><p>NLP is about teaching machines to extract information from texts, to classify texts, and to generate texts. The core ability that we want a model to acquire is the ability to generalize. We want a model to have enough intelligence and flexibility to handle all kinds of variations in downstream tasks that may not be present in the training data. Language models output probabilities over words. n-gram models are the simplest kind of language models, which give a primitive statistical summary of the training data. Although insufficient, they serve as a benchmark for more advanced models.</p><hr /><p>Cite as:</p><div class="language-bibtex highlighter-rouge"><div class="code-header"> <span label-text="Bibtex"><i class="fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre><td class="rouge-code"><pre><span class="nc">@article</span><span class="p">{</span><span class="nl">lifei2022ngram</span><span class="p">,</span>
  <span class="na">title</span>   <span class="p">=</span> <span class="s">"Natural Language Processing, Probabilities and the n-gram Model"</span><span class="p">,</span>
  <span class="na">author</span>  <span class="p">=</span> <span class="s">"Li, Fei"</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">"https://volagold.github.io"</span><span class="p">,</span>
  <span class="na">year</span>    <span class="p">=</span> <span class="s">"2022"</span><span class="p">,</span>
  <span class="na">url</span>     <span class="p">=</span> <span class="s">"https://volagold.github.io/posts/nlp-prob-ngram/"</span>
<span class="p">}</span>
</pre></table></code></div></div></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/data-science/'>Data Science</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/nlp/" class="post-tag no-text-decoration" >nlp</a> <a href="/tags/n-gram/" class="post-tag no-text-decoration" >n-gram</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Natural Language Processing, Probabilities and the n-gram Model - volagold.github.io&url=https://volagold.github.io/posts/nlp-prob-ngram/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Natural Language Processing, Probabilities and the n-gram Model - volagold.github.io&u=https://volagold.github.io/posts/nlp-prob-ngram/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=Natural Language Processing, Probabilities and the n-gram Model - volagold.github.io&url=https://volagold.github.io/posts/nlp-prob-ngram/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <a href="https://www.linkedin.com/sharing/share-offsite/?url=https://volagold.github.io/posts/nlp-prob-ngram/" data-toggle="tooltip" data-placement="top" title="Linkedin" target="_blank" rel="noopener" aria-label="Linkedin"> <i class="fa-fw fab fa-linkedin"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/nlp/">nlp</a> <a class="post-tag" href="/tags/app-development/">app-development</a> <a class="post-tag" href="/tags/database/">database</a> <a class="post-tag" href="/tags/generative-model/">generative-model</a> <a class="post-tag" href="/tags/javascript/">javascript</a> <a class="post-tag" href="/tags/math/">math</a> <a class="post-tag" href="/tags/react/">react</a> <a class="post-tag" href="/tags/web-development/">web-development</a> <a class="post-tag" href="/tags/attention/">attention</a> <a class="post-tag" href="/tags/big-data/">big-data</a></div></div></div><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" class="js-toc"></nav></div><script src="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.11.1/tocbot.min.js"></script> <script> tocbot.init({ tocSelector: '.js-toc', contentSelector: '.js-toc-content', headingSelector: 'h1, h2, h3', hasInnerContainers: true, }); </script></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/sequence-labeling/"><div class="card-body"> <em class="timeago small" date="2022-03-26 09:00:00 +0800" >Mar 26, 2022</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Sequence Labeling with HMM and CRF</h3><div class="text-muted small"><p> Sequence labeling is a classical task in natural language processing. In this task, a program is expected to recognize certain information given a piece of text. This is challenging, because eve...</p></div></div></a></div><div class="card"> <a href="/posts/rnn/"><div class="card-body"> <em class="timeago small" date="2022-04-08 22:00:00 +0800" >Apr 8, 2022</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>RNN, LSTM and GRU</h3><div class="text-muted small"><p> In this post we introduce the recurrent neural network (RNN) model in natural language processing (NLP), as well as two improvements over it, long short-term memories (LSTM) and gated recurrent ...</p></div></div></a></div><div class="card"> <a href="/posts/attention-and-transformers/"><div class="card-body"> <em class="timeago small" date="2022-04-11 22:30:00 +0800" >Apr 11, 2022</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Attention and Transformers</h3><div class="text-muted small"><p> In recent years, Transformer-based models are trending and are quickly taking up not only the field of NLP, but also computer vision and many other fields in AI. In this post, we give a tutorial...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/nce/" class="btn btn-outline-primary" prompt="Older"><p>Noise Contrastive Estimation</p></a> <a href="/posts/embedding-and-word2vec/" class="btn btn-outline-primary" prompt="Newer"><p>Embeddings and the word2vec Method</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> ¬© 2025 <a href="https://volagold.github.io">volagold</a> <span></span></p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/nlp/">nlp</a> <a class="post-tag" href="/tags/app-development/">app-development</a> <a class="post-tag" href="/tags/database/">database</a> <a class="post-tag" href="/tags/generative-model/">generative-model</a> <a class="post-tag" href="/tags/javascript/">javascript</a> <a class="post-tag" href="/tags/math/">math</a> <a class="post-tag" href="/tags/react/">react</a> <a class="post-tag" href="/tags/web-development/">web-development</a> <a class="post-tag" href="/tags/attention/">attention</a> <a class="post-tag" href="/tags/big-data/">big-data</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdnjs.cloudflare.com/ajax/libs/simple-jekyll-search/1.9.2/simple-jekyll-search.min.js" referrerpolicy="no-referrer"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" referrerpolicy="no-referrer"></script> <script src="https://cdnjs.cloudflare.com/ajax/libs/magnific-popup.js/1.1.0/jquery.magnific-popup.min.js" integrity="sha512-IsNh5E3eYy3tr/JiX2Yx4vsCujtkhwl7SLqgnwLNgf04Hrt9BT9SXlLlZlWx+OK4ndzAoALhsMNcCmkggjZB1w==" crossorigin="anonymous" referrerpolicy="no-referrer"></script> <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.11/clipboard.min.js" integrity="sha512-7O5pXpc0oCRrxk8RUfDYFgn0nO1t+jLuIOQdOMRp4APB7uZ4vSjspzp5y6YDtDs4VzUSTbWzBFZ/LKJhnyFOKw==" crossorigin="anonymous" referrerpolicy="no-referrer"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { tags: 'ams', inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ], processEscapes: true } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.1/es5/tex-mml-chtml.min.js" integrity="sha512-lt3EkmQb16BgAXR0iCk+JUJyDFmS9NZEMXCXK169qQoWcXu9CS4feejtxkjjUruw/Y0XfL1qxh41xVQPvCxM1A==" crossorigin="anonymous" referrerpolicy="no-referrer"></script> <script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.1.3/js/bootstrap.bundle.min.js" integrity="sha512-pax4MlgXjHEPfCwcJLQhigY7+N8rt6bVvWLFyUMuxShv170X53TRzGPmPkZmGBhk+jikR8WBM4yl7A9WMHHqvg==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
