[ { "title": "Moving to Hugo", "url": "/posts/to-hugo/", "categories": "Programming", "tags": "web-development", "date": "2023-04-04 09:00:00 +0800", "snippet": " As the number of posts grows, build time under Jekyll is now almost unbearable. So I decide to switch to Hugo. Contents on this website will remain, but the domain name of this website will be changed to lifeitech.github.io. See new contents at lifei.ai.See new blog posts at: lifei.ai" }, { "title": "Introducing BeeNote, a Full Stack Web App Built with Next.js 13", "url": "/posts/beenote/", "categories": "Programming", "tags": "typescript, javascript, react, nextjs, app-development", "date": "2023-03-11 09:00:00 +0800", "snippet": " Vercel released Next.js 13 at the end of October 2022, which was quite a big thing in frontend development. One of its biggest feature is the app directory for routing. I decided to try it out, so I built a full stack web application for taking language notes, with Next.js 13 + Tailwind + DaisyUI + PocketBase + AWS SES. In this post I will give you an introduction to this application. New! The app is now equipped with AI writing assistant, powered by OpenAI‚Äôs API. Create an account and try all the features now on https://beenote.app !üíªProject repository: https://github.com/lifeitech/beenote/When I left Italy in 2020 after graduation, I also left my Italian language notes behind in my dormitory, because my luggage was too full, and I couldn‚Äôt find any room for those notebooks and prints. I wish I could conveniently store my language notes on some server so that I never have to worry about physical relocations. That‚Äôs part of the motivation for me to build this language note app. There are already many apps out there for taking notes, including markdown editors like Typora and MarkText, but notes for language learning require special structures. And as your files grow, it is difficult to keep track of them on your computer. I find it‚Äôs better to store all of your notes in one place (app), online, rather than locally.Now let me show you different pages of this app.Walkthrough of the ApplicationSignup and LoginFirst, create an account at the signup page. On this page, there are email and password input validations, implemented with React‚Äôs useState hook. If the email input follows the pattern xxx@xxx.xxx, then a check icon will appear near the input field. For password, input borders will be red, sign-up button will be disabled, unless two conditions are met: first, password is at least 8 characters, and second, confirm password is the same as password input.app/signup/page.tsxAfter created an account, you can login at the login page.app/login/page.tsxCreate Language NotebookAfter login, language notebooks that you have created will be listed. Below is the UI for dark mode.You can create new language notebooks by clicking the ‚ÄúCreate Language‚Äù button. Languages that haven‚Äôt been created will be listed.app/u/page.tsxAlphabetsEach language notebook consists of four sections: Alphabet, Vocabulary, Grammar, and Custom notes. Below is the UI for the Alphabet section. For certain languages, like Japanese and Thai, their alphabets can be divided into two or more categories. For example, in Japanese there are hiragana (Âπ≥‰ªÆÂêç) and katakana (Áâá‰ªÆÂêç). Thus, this section allows user to create categories for alphabets. Click on the alphabet and pronunciation to modify them. When you unfocus, it will be saved to the server.app/u/[lang]/alphabet/page.tsxYou can click the audio recording button to record pronunciation for that alphabet. Your browser will first ask for permission to use your microphone, and if you agree, recording will be started. When you click the stop button, recording will stop and the audio file will be uploaded to the server. Now you can click the speaker icon to play the audio you just recorded. Record again to replace the audio you saved before.app/(utils)/RecordAudio.tsxVocabulariesIn the vocabulary section, you can create vocabulary notebooks. app/u/[lang]/vocabulary/page.tsxYou can add words in the notebook. As with alphabets, you can record audios. Here you can also upload images for visualization. Hovering over the image icon will show the image.app/u/[lang]/vocabulary/[url]/page.tsxGrammarsThe Grammar section allows you to create grammar notebooks. The grammar notebook is an editor implemented with Tiptap. Common typefaces like marks, headings, paragraphs and quotes can be added. You can also include tables, images and youtube videos.app/u/[lang]/grammar/[url]/page.tsx|app/(utils)/Editor.tsxCustom NotesFinally, you can add custom notes that may not fall into the categories of alphabet, vocabulary, and grammar, for example maybe you want to add some learning resources, or a summarization of common colloquial expressions. Currently the implementation is basically the same as for the grammar section, namely a rich text editor.app/u/[lang]/custom/[url]/page.tsxProfile and SettingsOnce you have logged in, a profile icon will appear at the top right corner. Click the icon to see a dropdown mennu. You can navigate the website, go to your profile, and switch between light and dark themes. The theme info is saved to localStorage in your browser, so that the next time you visit the website, it will get the info from the localStorage, and render in the same theme you last saved. It is implemented with React‚Äôs useContext hook.app/(utils)/ProfileDrop.tsxIn the profile page, you can modify your name and upload avatar image. You can also change email and password, upon which emails will be send to your current email address. The email sending service is provided by AWS SES. Finally, you can also delete your account. app/profile/page.tsxapp/profile/email.tsxapp/profile/password.tsxapp/profile/delete.tsxTech StackLet me briefly mention some of tools I used for building this application. I may talk about them in detail in future posts. These are all awesome softwares that you don‚Äôt want to miss. First, Next.js 13 offers app directory for routing, server-side rendering by default, caching, image optimization, client-side navigation and more. For CSS, after using Tailwind, I never go back to write plain CSS again. DaisyUI is a component library that offer pre-made Tailwind components like buttons, inputs, modals, dropdowns, cards and so on. I use PocketBase for the backend (database + user authentication). It is an excellent open sourced BaaS project that is both lightweight and fast (being written in Go). I use AWS SES for email sending service. Among other competitors like mailjet, sendgrid, sendinblue and mailgun, SES offers more free usage limit and better experience. Finally, the app is deployed on Vercel, while the backend is hosted on fly.io.ConclusionThat‚Äôs all for now! If you haven‚Äôt done so, why not give it a try athttps://beenote.appand tell me what you think about it. If you like this project, consider giving a star to the GitHub Repo or simply share it with other people, so that more people can know it.I would like to mention that this app mainly serves as a demo for Next.js 13 + PocketBase. Though it already involved a lot of hard work, its UI and functionalities are still not perfect enough for commercial applications. But I hope you could see from this project how you can use the same awesome tools and libraries to build applications.Finally, I would like to discuss some ideas for startups around language learning. While this demo hardly has any potential to make profits because little people is going to pay for a note app, other ideas could be profitable. First, a social media could be built around language learning. Second, large AI models similar to ChatGPT could offer interactive ways of language learning. For example, instead of responding to user‚Äôs questions, AI could actively ask the user for responding to questions in a foreign language, to improve the user‚Äôs expression skills. AI could help with translation, conversation, memorization in a more interactive and engaging way. In any case, I expect that in the future, learning a foreign language can be much easier than today!" }, { "title": "Building Cross-platform Mobile Apps with React Native", "url": "/posts/pixel-weather/", "categories": "Programming", "tags": "javascript, react, react-native, app-development", "date": "2023-01-18 15:00:00 +0800", "snippet": " I built a mobile app for checking weather ‚Äì called ‚ÄúPixel Weather‚Äù ‚Äì with React Native. I will talk about cross-platform app development, and give a walkthrough of this project in this post.Cross Platform vs Native DevelopmentWhen it comes to cross-platform mobile app development, there are two main choices: one is Facebook‚Äôs React Native, and the other is Google‚Äôs Flutter. In the former, you use the React framework and write JavaScript code to develop your mobile app. In the latter case, you have to work with a new language, Dart. The upper side of those cross-platform framework is that you can now develop your app for different platforms with one code base, and don‚Äôt need to develop iOS and android versions of your app separately. This reduces the time and budget requirements for small teams and individuals at the beginning. However, the downside is that, it is hard to accomplish 100% native performance by using those frameworks. They may give you 60%~70%, but never comparable to true native. Quite often, you will encounter issues like flashings and white margins, and often there is no good way to solve those issues. If you are serious about user experience, and you want to generate real profit from mobile app development, the best choice today is still to go for the natives: swift for iOS, and Kotlin for android.But if you just want to play around with mobile app development, or if you want to quickly prototype your product, then those cross-platform frameworks provide a fast way to get you started.Why I choose React NativeIn my opinion, React Native has some advantages over Flutter. First, you don‚Äôt need to learn a new language. Front end developers know how to write JavaScript, so they automatically know how to build a mobile app with React Native. You can develop your app without leaving JavaScript. This avoids some kind of framework lock-in as in Flutter, where knowledge about Dart would not be very useful elsewhere. Second, React Native is more popular and has a far larger community than Flutter. When it comes to libraries, there are more choices. By contrast, Flutter‚Äôs userbase is far smaller, and that means it may be harder to find solutions for solving your issues. Third, the two frameworks take different approaches. In React Native you write your components as functions, while Flutter has a class-based approach. Personally I like to writing everything as functions, as opposed to classes, so React Native is easier to take up with for me. Lastly, one small point is that React Native Doc has dark mode support, while Flutter‚Äôs Doc doesn‚Äôt. Even though this is only a small detail, it can still reflect the fact that React Native is more mature and has a larger and more diverse community, so that it is a safer bet. But keep in mind that React Native is far from perfect. As of the time of writing this post, React Native is in 0.7x, and there are still lots of bugs and issues. You may often run into problems like flashing and flickering screen transitions, and sometimes it is really difficult to debug those problems. Also, currently, animation is very difficult to implement (e.g. drag and sort) in React Native, which is a very important aspect of a good user experience.The App &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; On start, the app asks for user‚Äôs permission to get the phone‚Äôs current location. If granted, the app will then fetch weather and air quality data for that location from two API sources. While fetching the data, a loading animation will be displayed.The main page displays 6 essential weather stats: temperature, weather condition (sunny, cloudy, rainy, etc.), wind (speed and direction), humidity, air quality index, as well as the local time. Users can tap on each stat to get forecast details. Pressing the temperature will navigate the user to hourly forecast page, pressing the condition will navigate to daily forecast, and so on.There is a button at the bottom for toggling dark and light themes.Users can also check weathers for different cities around the world. Searching, adding and deleting cities are all implemented as in other weather applications.Download the AppAndroidScan the QR code below to download and install the android distribution (.apk) on your phone. It is tested only on Samsung A52G.FeaturesDistinct from many other weather apps that you can find in your app store, this weather app provides alternative features for anyone who wants something different. Big component sizes. This makes it more accessible, and convenient for outdoor usage. Personally I prefer this kind of simple and old-fashioned design over those heavy and delicate ones. Dark theme support. Friendly for usage at night. Varying colors. The color of the numbers will change according to the value. For example, blue colors for colder temperatures, orange and red colors for hotter temperatures. This provides a good visualization for the stats. You could also get an impression of a place‚Äôs weather by its colors. Display local time. You can use this app as a world clock. Provide air quality info. Air quality is a very important factor for human health. It is also an indicator of environmental pollution. Many places on earth, including India, China, parts of Europe, still experience heavy air pollutions very often. DependenciesThis project uses the following packages and libraries: React Navigation, for navigating different screens. expo-location, for getting user‚Äôs current location. axios, for making API requests. AsyncStorage, for saving locations locally on user‚Äôs phone. lottie-react-native, for loading animation during data fetching. react-native-flash-message, for displaying messages. Sentry, for error logging in production.API ProvidersThis weather app uses data from the following two sources: OpenWeather, for weather data; World Air Quality Index Project, for air quality data.How to Build git clone the repository to your local machine. git clone https://github.com/lifeitech/pixel-weather.git cd to the folder. Run npm install to install all dependencies. Apply for API keys from the two data source providers mentioned above. You may need to provide your credit card information for weather data subscription. Create .env in the root folder, and put your API keys into this file, like WEATHER_KEY=your-key-string AIR_KEY=your-key-string Run npm start or expo start You will then see a QR code in the terminal. Open the Expo Go app on your phone, scan the QR code to run this project on your phone. More Screenshots Hourly Forecast Wind Forecast Humidity Forecast Add City Delete City AQI Info Light theme - home Light theme - humidity forecast Light theme - location panel Source Codeüíª https://github.com/lifeitech/pixel-weather" }, { "title": "All about SQL Queries", "url": "/posts/sql/", "categories": "Programming", "tags": "sql, database", "date": "2022-06-01 11:00:00 +0800", "snippet": " This post serves as a reference for SQL syntax. We cover the most frequently used commands, including ‚Äúcase when‚Äù, ‚Äújoin‚Äù and functions. Download a database Download a client Select Data Select columns with ‚Äúselect ‚Ä¶ from ‚Ä¶‚Äù Filter rows with ‚Äúwhere‚Äù Filter rows with ‚Äúdistinct‚Äù, ‚Äúorder by‚Äù and ‚Äúlimit‚Äù Categorize column values with ‚Äúcase when‚Äù Select columns from different tables with ‚Äújoin‚Äù select common column values with ‚Äúinner join‚Äù select rows from one table with ‚Äúleft join‚Äù or ‚Äúright join‚Äù select row union with full Unions Calculating statistics with aggregate functions Apply aggregate functions to different groups of rows with ‚Äúgroup by‚Äù Filter calculated columns with ‚Äúhaving‚Äù Functions Date operators &amp;amp; functions String operators &amp;amp; functions Window functions Examples I/O with PythonA database is a software for storing data. There are many many databases available. They can be broadly classified into two categories: one is relational database, including MySQL and PostgreSQL. They other category is non-relational database, including MongoDB, Cassandra, neo4j, Dgraph and many others. Here we are concerned with SQL, the common syntax for querying relational databases.Download a databaseTo practice writing SQL queries, you need to install a database on your computer first. You can download either MySQL or PostgreSQL. Both are open sourced, but MySQL is more widely used than PostgreSQL. Notably, PostgreSQL has more data types and functions, so it is more powerful, but it is also more complex and more difficult to learn. Query syntax for the two databases are largely similar, but not entirely the same, especially for functions. To install MySQL, download an installer on the official website. Then follow the installation guide and install all the components step by step. To install PostgreSQL, go to the official website and download an installer.Download a clientYou might want a GUI client software to interact with your database. With MySQL installation it comes with a default one called MySQL Workbench. For PostgreSQL the default client that comes with the installation is pgAdmin. I find pgAdmin is ok, but Workbench is hard to use. I personally findBeekeeper Studioto be the best one.To play with MySQL on Mac, there is also Sequel Pro available.Finally, there are also built-in command line clients. To start MySQL client, typemysql --user=user_name --password db_namein your shell. To add mysql to path in windows, add C:\\Program Files\\MySQL\\MySQL Server x.x\\bin to PATH in system environment variable, where x.x. is the version number. For PostgreSQL, the command line client is psql. To add it to path in windows, add C:\\Program Files\\PostgreSQL\\xx\\bin to the system PATH variable, where xx is the version number.Select DataSelect columns with ‚Äúselect ‚Ä¶ from ‚Ä¶‚Äùselect c1, c2, ...from tbFilter rows with ‚Äúwhere‚Äùselect c1, c2, ...from tbwhere condition1 and/or condition2We can use operators on columns in where conditions. Operator Description Example = equal to col = &#39;salary&#39; &amp;lt;&amp;gt; / != not equal to dept != &#39;decision&#39; &amp;gt; greater than date &amp;gt; 2022-02-15 &amp;lt; less than price &amp;lt; 500.00 &amp;gt;= greater than or equal number &amp;gt;= 2 &amp;lt;= less than or equal number &amp;lt;= 2 and and state = &#39;GA&#39; and amount &amp;gt; 1000 or or state = &#39;GA&#39; or amount &amp;gt; 1000 [not] between between an inclusive range price between 100.00 and 500.00 [not] like string pattern name like &#39;Luca%&#39; ilike string pattern, ignoring capitalization name ilike &#39;%cafe%&#39; [not] in set membership test col in (&#39;col1&#39;, &#39;col2&#39;, &#39;col3&#39;) is [not] null compare to null address is not null is [not] true boolean true value test full_time is true is not distinct from is equal to value or both are nulls name is not distinct from full_name as Rename table or column name. Can be omitted select result as r Filter rows with ‚Äúdistinct‚Äù, ‚Äúorder by‚Äù and ‚Äúlimit‚Äùselect distinct c1, c2, ... from tb where conditionorder by col asc/desclimit [num] offset [num] note: distinct can have slow performance.Categorize column values with ‚Äúcase when‚Äùselect ...case when condition1 then label1when condition2 then label2when condition3 then label3else label4end (as col_name)Select columns from different tables with ‚Äújoin‚Äùselect c1, c2, ...from tb1 (inner/left/right/full) join tb2 on tb1.id = tb2.idwhere condition...We illustrate inner, left, right and full join with the following two tables.This is tb1. id c1 a value b value c value d value e value This is tb2. id c2 a value b value x value y value select common column values with ‚Äúinner join‚Äùinner join is tb1 $\\cap$ tb2. id c1 c2 a value value b value value select rows from one table with ‚Äúleft join‚Äù or ‚Äúright join‚Äùleft join selects all values from tb1‚Äôs id column. id c1 c2 a value value b value value c value null d value null e value null right join selects all values from tb2‚Äôs id column. id c1 c2 a value value b value value x null value y null value select row union with fullfull means tb $\\cup$ tb2. id c1 c2 a value value b value value c value null d value null e value null x null value y null value The outer keyword is deprecated. It is kept for backward compatibility with SQL-92 only. left join may be less efficient than (inner) join.Unionsselect col from tb1union allselect col from tb2Sometimes we need to combine rows from two different tables. We can use union or union all for that purpose. union will only retain unique rows, while union all will retain all rows.Union is also different from where ... or ... clause. This queryselect device_id, gender, age, gpa from user_profile where university = &quot;Shandong University&quot; union allselect device_id, gender, age, gpa from user_profile where gender = &#39;male&#39; outputs the set of users that come from Shandong University, then users that are male, in that order. Instead, in the output of this queryselect device_id, gender, age, gpa from user_profile where university = &quot;Shandong University&quot; or gender = &#39;male&#39;users from Shandong University and users that are male are randomly mixed.Calculating statistics with aggregate functionsselect agg(col) as ...from tbwhere condition function description count(col) counts the number of rows min(col) min for all rows max(col) max for all rows avg(col) average for all rows sum(col) sum over all rows Apply aggregate functions to different groups of rows with ‚Äúgroup by‚ÄùInstead of aggregating across all the rows, you can also apply the aggregate functions to different groups of rows with the group by keyword.select c, agg(col) as col_namefrom tbwhere conditiongroup by cRows are classified by values in c. In other words, rows that correspond to the same value in c belong to the same group.Filter calculated columns with ‚Äúhaving‚ÄùThe having keyword is used to filter rows in calculated columns.select id, fun(col) as col_name(group by col)having col_name...FunctionsDate operators &amp;amp; functionsBelow is a list of some date operators in PostgreSQL. operator meaning example result + int add a number of days to a date select date &#39;2021-08-14&#39; + 1 202-08-15 + interval add an interval to a date select date &#39;2001-09-28&#39; + interval &#39;1 hour&#39; 2001-09-28 01:00:00 + time add a time-of-day to a date ¬† ¬† - int subtract a number of days from a date ¬† ¬† - date subtract days, producing the number of days elapsed select date &#39;2001-10-01&#39; - date &#39;2001-09-28&#39; 3 - time subtract times ¬† ¬† Some date functions in PostgreSQL and MySQL: function PostgreSQL MySQL description now() ‚úÖ ‚úÖ date and time now current_date ‚úÖ ¬† date now current_time ‚úÖ ¬† time now curdate() ¬† ‚úÖ date now curtime() ¬† ‚úÖ time now date() ¬† ‚úÖ extract date day() ¬† ‚úÖ extract day month() ¬† ‚úÖ extract month year() ¬† ‚úÖ extract year age(timestamp, timestap) ‚úÖ ¬† subtract two dates datediff(d1, d2) ¬† ‚úÖ subtract two dates extract(unit from timestamp) ‚úÖ ‚úÖ extract unit from date A useful function might be the extract function. It can be used to extract year, month or date from datetime record, which is useful for analysis based on different dates.extract(unit from date)unit can be: microsecond, second, minute, hour, day, week, month, quarter, year.Different databases may have different date functions. Refer to official documentations for complete list of date functions: PostgreSQL date functions MySQL date functionsString operators &amp;amp; functionsHere we list some of the string functions in PostgreSQL. Refer to the documentation for a complete list. function description example result s || s string concatenation &#39;Post&#39; || &#39;greSQL&#39; PostgreSQL length(s) length of string length(&#39;jose&#39;) 4 lower(s) convert string to lower case lower(&#39;FEMALE&#39;) female upper(s) convert string to upper case upper(&#39;female&#39;) FEMALE position(sub in s) location of specified substring position(&#39;z&#39; in &#39;wxyz&#39;) 4 substring(s from [int] for [int]) extract substring substring(&#39;abcde&#39; from 2 for 3) bcd substring(s from [re]) extract substring with regular expression substring(&#39;LxxxR xyz&#39; from &#39;L.*R&#39;) LxxxR left(s, n) return fist n characters in the string. When $n$ is negative, return all but last $\\vert n\\vert$ characters. left(&#39;abcde&#39;, 2) ab right(s, n) return last $n$ characters in the string. right(&#39;vwxyz&#39;, 2) yz replace(s, sub, repl) replace all occurences of substring replace(&#39;WakqSnake&#39;, &#39;ak&#39;, &#39;xx&#39;) WxxqSnxxe repeat(s, n) repeat string $n$ times repeat(&#39;Pg&#39;, 4) PgPgPgPg string_to_array(s, delimiter) split string to array select (string_to_array(&#39;165cm,55kg,26,female&#39;, &#39;,&#39;))[4] female Related: PostgreSQL documentation on array functionsWindow functionsWindow functions are useful when we need to calculate something within a column. We partition a column into windows, and apply a function to each window. The calculated column can then be selected alongside with other columns.select id, fun(col) over (partition by col2 order by col3) as col_namefrom tbfor fun can use aggregate functions: max, min, sum, etc. ranking functions: row_number, rank, dense_rank, ntile, etc. They are useful for selecting the top (min or max) N records per category. mysql&amp;gt; select val, row_number() over w as &#39;row_number&#39;, rank() over w as &#39;rank&#39;, dense_rank() over w as &#39;dense_rank&#39; from numbers window w as (order by val);+------+------------+------+------------+| val | row_number | rank | dense_rank |+------+------------+------+------------+| 1 | 1 | 1 | 1 || 1 | 2 | 1 | 1 || 2 | 3 | 3 | 2 || 3 | 4 | 4 | 3 || 3 | 5 | 4 | 3 || 3 | 6 | 4 | 3 || 4 | 7 | 7 | 4 || 4 | 8 | 7 | 4 || 5 | 9 | 9 | 5 |+------+------------+------+------------+ analytic functions: lag, lead. They are useful for comparing multiple rows and calculate differences between rows. mysql&amp;gt; select t, val, lag(val) over w as &#39;lag&#39;, lead(val) over w as &#39;lead&#39;, val - lag(val) over w as &#39;lag diff&#39;, val - lead(val) over w as &#39;lead diff&#39; from series window w as (order by t);+----------+------+------+------+----------+-----------+| t | val | lag | lead | lag diff | lead diff |+----------+------+------+------+----------+-----------+| 12:00:00 | 100 | NULL | 125 | NULL | -25 || 13:00:00 | 125 | 100 | 132 | 25 | -7 || 14:00:00 | 132 | 125 | 145 | 7 | -13 || 15:00:00 | 145 | 132 | 140 | 13 | 5 || 16:00:00 | 140 | 145 | 150 | -5 | -10 || 17:00:00 | 150 | 140 | 200 | 10 | -50 || 18:00:00 | 200 | 150 | NULL | 50 | NULL |+----------+------+------+------+----------+-----------+ window functions may store information on a temporary database on disk, so their performance can be very slow. In many situations the old fashionedgroup by is way faster than window functions.ExamplesSee https://github.com/lifeitech/sql-nowcoder.I/O with PythonTo read from and write data to database with Python is easy. Here is an example showing how to convert SQL tables to pandas DataFrames and vice versa (using PostgreSQL).First, install the following two packagespip install psycopg2-binary # mysql would be: mysqldb or pymysql etc.pip install sqlalchemyWe can connect to database with the create_engine() function provided by the sqlalchemy package, withdialect+driver://username:password@host:port/databaseas input url, where host is the server address, and database is the name of the database. For more details refer to sqlalchemy docs.import pandas as pdfrom sqlalchemy import create_engineengine = create_engine(&#39;postgresql+psycopg2://username:password@server:port/database&#39;)con = engine.connect()# read table &#39;user/info&#39; into pandas dataframeprofile = pd.read_sql(&#39;select * from user.info&#39;, con=con)# ....# write dataframe to a new table in db named &#39;analysis&#39;df.to_sql(&#39;analysis&#39;, con=engine, schema=&#39;user&#39;, if_exists=&#39;replace&#39;)Here, the name ‚Äúschema‚Äù is somewhat confusing, in practice it basically means the ‚Äúfolder‚Äù that contains the table.Cite as:@article{lifei2022sql, title = &quot;All about SQL Queries&quot;, author = &quot;Li, Fei&quot;, journal = &quot;https://volagold.github.io&quot;, year = &quot;2022&quot;, url= &quot;https://volagold.github.io/posts/sql/&quot;}" }, { "title": "Google&#39;s three big data technologies: GFS, Bigtable and MapReduce", "url": "/posts/google-big-data/", "categories": "Programming", "tags": "database, big-data", "date": "2022-05-15 18:38:00 +0800", "snippet": " Have you wondered about the inner workings of big data? How parallel and distributed computing work? In this post we will have a look at Google‚Äôs three early works in the 2000s that are the basis of Apache Hadoop: the Google File System (GFS), Bigtable and MapReduce.As the Internet rapidly developed in the late 90s, it soon became impractical for many enterprises to store and analyze all of their data on a single machine. Naturally, distributed data storage and processing techniques arose. With gigabytes of data continuously being generated, the only way to store them is to make use of clusters of computers. The task of data storage has to be distributed across many machines, and big data techniques concern how to coordinate those machines to achieve efficiency, reliability, and ease of use. Today, the Hadoop ecosystem encompasses many popular big data technologies, including HDFS (Hadoop Distributed File System), Hive, Pig, YARN, MapReduce, Spark, HBase, Cassandra, Kafka, Storm, and others. Those projects were initially inspired by Google‚Äôs Google File System (GFS) (Ghemawat et al., 2003), Bigtable (Chang et al., 2008) and MapReduce (Dean &amp;amp; Ghemawat, 2008), and were meant to be open source replacements for Google‚Äôs commercial technologies. So to understand how those technologies work, we will look at Google‚Äôs three big data technologies: the Google File System (GFS), MapReduce, and Google Bigtable.The Google File System (GFS)The Google File System (GFS) is a low level file system for distributed data storage. It provides a file system interface like on a single machine. GFS was designed under several principles, the most silient one is that component failures are norm rather than exception, so the system must be robust to all kinds of failures and deliver correct outcomes even in the presence of component failures.A GFS consists of a single master and several chunkservers, and is accessed by multiple clients. All of them are just ordinary Linux machines. Files are divided into fixed-size (64MB) chunks. Each chunk is replicated on 3 chunkservers for reliability.For a client to request data, it first contacts the master, and the master tells the client the chunkserver that has the data, and the client then contacts the chunkserver, and the chunkserver sends data to the client. See the figure below for GFS architecture. In the GFS architecture, there is only a single master, but the master state is replicated for reliability. Note that clients never read and write files through the master, since under that situation the master will become a bottleneck.Google File System (GFS): read dataThe system also keeps an operation log that contains a historical record of important metadata changes. This log is also replicated on multiple machines.To write data, the client first contacts the master about the chunk location that it wants to modify. The master replies with the location of three chunkservers that host the chunk. One replica is designated to be primary. After knowing the three chunkservers, the client sends the data to the server that is closest to it. Then the data are shared within the three servers. After the data has been cached by all three servers, the writing process begins. The primary server writes to its own disk, and notifies other servers to do the same. When finished, the primary server notifies the client that they are done.There are many other details of the system. For a complete description please refer to (Ghemawat et al., 2003).Google BigtableBigtable is a distributed (NoSQL) database. While the Google File System is concerned about how to store files that are too large and too many, Bigtable has a higher level concern about how to efficiently store and retrieve actual data points, just like what other database systems are concerned about. Examples are personnel information, product information, properties of webpages, and so on.A Bigtable is a list of sorted &amp;lt;key, value&amp;gt; pairs. To store a large Bigtable, we divide it into many small tables called tablets, and we store information about those tablets into a metadata tablet. We further divide tablets into SSTables (‚ÄúS‚Äù for ‚Äúsmall‚Äù).So, a tablet contains a list of SSTables and their addresses, and it is each SSTable that stores actual data (key-value pairs). SSTable files are stored in GFS.Google BigtableWrite dataHow to insert a key-value pair &amp;lt;k, v&amp;gt; into Bigtable? Because SSTables reside in hard disk, writing into SSTables directly would be slow, not to mention that we have to sort the SSTable once we inserted the data. Thus, we establish a table called memTable in memory, and we write the data into memTable. When it is full, we save the table on disk, so that it becomes another SSTable.Bigtable WriteTo prevent loss of memTable in memory, we maintain a tablet log on hard disk, and write every step to the log before writing to memTable.Bigtable Tablet LogRead dataWe index each SSTable on hard disk, and load the index into memory when the SSTable is opened. To lookup an SSTable, we can perform a binary search in the in-memory index, and then read the table from disk.To read the value for a key k, the operation is executed on a merged view of the sequence of SSTables and the memTable. Since the SSTables and the memTable are lexicographically sorted data structures, the merged view can be formed efficiently.Bloom filtersA read operation has to read from all SSTables that make up the state of a tablet. If these SSTables are not in memory, we may end up doing many disk accesses. We can further accelerate read operation by using Bloom filters.Bloom filters are useful for testing whether an element is in a set. A Bloom filter is an array of binary bits (0 or 1) together with some hash functions. A hash function maps its input to some position of the array. To add an element, set the value to 1 for all output positions of all the hash functions.If an element is in the set, then all hash functions must map it to 1 because that is how we added it, but if an element is not present, then it is not very likely that all hash outputs happen to correspond to 1. If at least one output is 0, then for sure it is not in the set. On the other hand, there is a small possibility of being false positive.For Bloom filters, the running time for checking an element is proportional to the number of hash functions, but is independent of the size of the set, so it can be fast for large sets.Bloom filter: a key is added by setting 1 for all hash mappings. To check if a key is present, check whether all hash functions return 1Bloom filter: if one hash function maps an input to 0, then the input is not in the set.A bloom filter is added for each SSTable. The final design looks like this:Bigtable read operationTo query the value for a key, we first input the key to each Bloom filter of each SSTable. If a result is negative, then for sure the key is not in that SSTable, so we don‚Äôt have to bother searching in that SSTable. We just need to search in SSTables where the result is positive. This can drastically reduces the number of disk seeks required for read operations.Logic viewsThe logic view of a Bigtable has rows and columns. A row represents an entity, for example a website. Columns contain various properties of the entity. Some columns are grouped together, called a ‚Äúcolumn family‚Äù. Each cell can contain multiple versions of the same data, indexed by timestamps.Bigtable logic viewTo map from the logic view to physical view, (row, column, time) tuples are used as keys that map to corresponding cell values.Bigtable mapping from logic view to physical viewMapReduceAfter we solved the problem of how to distributedly store data, now it is time to consider how to do distributed computation over the data. The idea of MapReduce (Dean &amp;amp; Ghemawat, 2008) is to decompose the data, and recombine. There are six steps: input, split, map, shuffle, reduce and output. A good analogy is the process of making Subway sandwiches. The data are like many kind of breads, vegetables and meat and other ingredients. We distribute them to different chefs. Some person gets some tomatoes while another gets some onions (split). Then each chef cuts his ingredients at hands into pieces (map). Ingredients are then grouped together into boxes (shuffle). When a customer orders a sandwich, the staff would pick up bread, meat and vegetables preferred by the customer from various boxes and put them together to make a sandwich (reduce) and finally serves it to the customer.Let‚Äôs see how MapReduce is useful for Google‚Äôs search engine. Suppose we have tons of articles (webpages) stored across thousands of machines. Let‚Äôs see how to calculate word frequencies, as well as how to establish an inverted index of the articles.Word frequenciesThe split process distributes articles to different workers. Then each worker goes through a for loop and counts each word in the article assigned to him as 1 in the map process. In the shuffle process, we put the same word across different workers into a single box. In reduce, we just need to sum up the 1s in each box together, or in other words count the size of each box. Finally, we output word frequencies as a list of values indexed by words. The figure below illustrates the MapReduce process. To our sandwich analogy, the final output is like a sandwich, where each value is the amount of the ingredient.MapReduce example: count word frequencies in articlesInverted indexAn inverted index is a list indexed by words, where the value for each word is a list of documents id that contain the word. This allows us to quickly find which documents contain a particular input keyword, which is very useful for search engines.The process for establishing an inverted index is similar to counting word frequencies. The output value for each word is a list, instead of an integer.MapReduce example: inverted indexSummaryIn this post, we learned Google‚Äôs three technologies that are of crucial importance in today‚Äôs big data era: Google File System, Bigtable and MapReduce. They are the pioneers of modern big data technologies. Their common purpose is to process data that reside over thousands of machines. The Google File System is concerned about lower level data storage. It divides machines into master and chunkservers. The master keeps information about chunkservers and coordinates read and write operations. Files are replicated over multiple chunkservers. Bigtable stores a table by mapping its entries to key-value pairs. It utilizes memory for speed, and hard disk replication for reliability. To perform computation over such large amount of data, the MapReduce paradigm takes a divide and conquer approach. It divides the problem into many smaller problems. After smaller problems are solved independently by distributed workers, their results are combined for final answer.Cite as:@article{lifei2022googlebigdata, title = &quot;Google&#39;s three big data technologies: GFS, Bigtable and MapReduce&quot;, author = &quot;Li, Fei&quot;, journal = &quot;https://volagold.github.io&quot;, year = &quot;2022&quot;, url = &quot;https://volagold.github.io/posts/google-big-data/&quot;}ReferencesGhemawat, S., Gobioff, H., &amp;amp; Leung, S.-T. (2003). The Google file system. Proceedings of the Nineteenth ACM Symposium on Operating Systems Principles, 29‚Äì43.Chang, F., Dean, J., Ghemawat, S., Hsieh, W. C., Wallach, D. A., Burrows, M., Chandra, T., Fikes, A., &amp;amp; Gruber, R. E. (2008). Bigtable: A distributed storage system for structured data. ACM Transactions on Computer Systems (TOCS), 26(2), 1‚Äì26.Dean, J., &amp;amp; Ghemawat, S. (2008). MapReduce: simplified data processing on large clusters. Communications of the ACM, 51(1), 107‚Äì113." }, { "title": "Julia in 15 Minutes", "url": "/posts/julia/", "categories": "Programming", "tags": "julia", "date": "2022-05-03 15:00:00 +0800", "snippet": " Julia is a fast and elegant programming language. In this post, we record some of its syntax, and we talk about advanced topics like types, methods and metaprogramming with real examples. We also discuss some challenges that new users face for this language.In scientific computing, researchers need to use computers to simulate large and complex systems, from interaction of molecules to modeling of climate and the universe. This is distinct from data analytics or econometrics, where one often deals with small amount of data. Although Python is easy to use, it is too slow for such purposes that require a huge amount of computation. Using Python to simulate such large systems is often impractical. On the other hand, lower level languages like C/C++, while being fast, is very difficult to learn and master. So the two language problem exists for the community for a long time. There is often no available tool for cutting edge research problems. Julia was born in this context. It is as fast as C, while as easy as MATLAB/Python.In this post, we briefly introduce the Julia syntax, highlighting its unique aspects. The introduction is referenced from the official documentation.To install Julia, just go to official website and download it. After installation, you can open your shell, type julia and start the interactive shell (also called ‚ÄúREPL‚Äù, a shorthand for ‚Äúread‚Äìeval‚Äìprint loop‚Äù). To install a package, press ] to enter package mode, then type add [pkg name]. Press Ctrl+C to quit the package mode. The recommended IDE is VS Code, with the Julia extension. Alternatively, if you are used to Jupyter Lab, you can press ] and add IJulia to install the Julia kernel for Jupyter. After that, open Jupyer Lab and you should see that Julia notebooks can now be created.Basic SyntaxVariablesDifferent from Python, some Unicode characters are allowed for variable names. You can use latex in Julia to name variables!julia&amp;gt; œÉ = 1.01.0The above sigma symbol is typed with \\sigma followed by a tab.Below is a list of naming conventions from official docs.Figure 1: variable naming conventions in JuliaStringsStrings should be double quoted in Julia (Single quoted expressions belong to Char type which is something different than strings).In Julia, the multiplication symbol * is used for string concatenation, as opposed to + in Python.julia&amp;gt; &quot;foo&quot; * &quot;bar&quot;&quot;foobar&quot;Use dollar sign for interpolation:julia&amp;gt; name = &quot;Josh&quot;&quot;Josh&quot;julia&amp;gt; &quot;His name is $name&quot;&quot;His name is Josh&quot;NumbersRational numbers are constructed using the // operator:julia&amp;gt; 4//81//2Complex number $i$ is represented using im.julia&amp;gt; (1 + 2im)*(2 - 3im)8 + 1imOperators^ is the power operator, x^3 means raise x to the 3rd power, just as in latex. Other operators are similar to those in Python.Boolean operators: Expression Name !x negation x &amp;amp;&amp;amp; y and x || y or [‚ùó] The Vectorized ‚Äúdot‚Äù operators is a unique feature in Julia. Prefix an operator with a dot turns that operator into an elementwise operator that can be applied to each element of an array. For example, [1,2,3] .^ 3 means [1^3, 2^3, 3^3]. Equivalently, we can put @. in front of an expression to perform elementwise computation.julia&amp;gt; x = [1, 2, 3]3-element Vector{Int64}: 1 2 3julia&amp;gt; @. x^2 + sin(x)3-element Vector{Float64}: 1.8414709848078965 4.909297426825682 9.141120008059866FunctionsFunctions can be defined using the function keyword, enclosed by an end. Conveniently, We can also use the = sign to define functions.julia&amp;gt; f(x) = x^3 - 2x + 7f (generic function with 1 method)julia&amp;gt; f(5)122julia&amp;gt; function f(x, y) cos(x) + y endf (generic function with 2 methods)Control FlowsAs in many other languages, there are conditional evaluation if-elseif-else, ternary operator ?:, while and for loops, and some other control flows. Note that in Julia if blocks also return a value.if x &amp;lt; y &quot;x is less than y&quot;elseif x &amp;gt; y &quot;x is greater than y&quot;else &quot;x is equal to y&quot;endjulia&amp;gt; x = 1; y = 5;julia&amp;gt; println(x &amp;lt; y ? &quot;less than&quot; : &quot;not less than&quot;)less thanjulia&amp;gt; for i = 1:5 println(i) end12345julia&amp;gt; for s ‚àà [&quot;foo&quot;,&quot;bar&quot;,&quot;baz&quot;] println(s) endfoobarbazTypesType declaration is very important for Julia, in distinct contrast with Python. You may already see a lot of them with the :: operator in Julia code. As a real example, let‚Äôs see how the convolutional layer is defined in Flux.jl:Figure 2: types in JuliaAlthough Julia‚Äôs type system is dynamic, by explicitly telling the compiler the types of your variables or expressions, program execution can speed up. Another reason for doing so is to take advantage of Julia‚Äôs multiple-dispatch mechanism, i.e. invoking different function definitions for different argument types. This is a very important reason why Julia is fast.Types are organized in a type graph, with concrete types like Float64 as leaves and more abstract types as nodes above the leaves. The &amp;lt;: operator declares that the left side is a subtype of the right side, e.g. Real &amp;lt;: Number.A type union is an abstract type that includes as objects all instances of any of its argument types, constructed using the special Union keyword. For example, any type T that is either a subtype of Tuple, or NamedTuple, or AbstractVector, is a subtype of Union{Tuple, NamedTuple, AbstractVector}.Figure 3: definition of Chain in Flux.jlA composite type is a collection of types. It is declared with the struct keyword.Figure 4: struct in Julia. This is the struct returned in the first function in Figure 2 above.We can also give a parameter to types. This makes type system more flexible and powerful.struct Point{T} x::T y::TendT can be anything: Float64, Int etc. Why stop at one parameter? Thisstruct Tuple2{A,B} a::A b::Bendmakes the struct more like an abstraction of the arguments of a function.MethodsIn Julia, we can make different definitions of a function for different argument types, all under the same function name. A definition of one possible behavior for a function is called a method. Figure 2 above is a such example.Just like struct, method definition can also be parametric.f(x::T, y::T) where {T} = # do somethingThe above means the method is only defined to do something when all of its argument have the same type T.We see from Figure 2 that methods and types are not so distinct from each other. After we defined a type, we can add methods to that type, using the exact same name. We can further make the type callable.Figure 5: make the convolutional layer callable on input dataMetaprogrammingIn your previous programming experience, have you encountered a situation where you have the need to programmatically generate code? To make an example, suppose you need to create a large number of variables a1=2, a2=4, a3=6 ‚Ä¶‚Ä¶ a100=200 at once, all with some specific values. How do you do that? You may wish to have something like this (in pseudocode)for i = 1 to 100 assign a{i} = i * 2endMetaprogramming is about programing a program. Without metaprogramming, we might have to write many repetitive code, doing a lot of copy and paste. Metaprogramming is a central feature in Julia. It enpowers the development process.Every program is just a piece of text (string). We can turn a string into an expression, and manipulate that expression. There are many ways to do the conversion. Take a + b as an example: Meta.parse(&quot;a + b&quot;) :(a + b) quote a + b end Expr(:call, :+, a, b)An expression has two parts: head and its arguments.julia&amp;gt; ex = :(a + b):(a + b)julia&amp;gt; ex.head:calljulia&amp;gt; ex.args3-element Vector{Any}: :+ :a :bIn this example, all of them are Symbol type, denoted with : at the front. Symbols can also be constructed.julia&amp;gt; Symbol(&quot;a&quot;, 99):a99To manipulate expression, we can interpolate values into an expression with the dollar sign:julia&amp;gt; a = 335;julia&amp;gt; ex = :($a + b):(335 + b)To evaluate an expression, we use the eval function.julia&amp;gt; a = 70; b = 50;julia&amp;gt; eval(ex) # recall that ex = :(a + b)120So, the solution to the problem proposed at the beginning of this subsection would befor i = 1:100 eval(Meta.parse(&quot;a$i = $i * 2&quot;))endTo directly manipulate and run Julia code without resorting to eval every time, we use macros. Macro statements start with an @ sign. It is another characteristic feature of Julia.julia&amp;gt; macro sayhello(name) return :(println(&quot;Hello, &quot;, $name)) end@sayhello (macro with 1 method)julia&amp;gt; @sayhello &quot;Josh&quot;Hello, JoshA simplified definition of the @assert macro that appeared in Figure 2:julia&amp;gt; macro assert(ex) return :( $ex ? nothing : throw(AssertionError($(string(ex)))) ) end@assert (macro with 1 method)julia&amp;gt; @assert 1 == 1.0julia&amp;gt; @assert 1 == 0ERROR: AssertionError: 1 == 0In Figure 5, @functor Conv means Functors.jl is allowed to look into the fields of the instances of the Conv struct and modify them. The purpose of Functors.jl is to easily operate over all neural network model parameters at once.julia&amp;gt; using Functorsjulia&amp;gt; struct Foo x y endjulia&amp;gt; @functor Foojulia&amp;gt; model = Foo(5, [1, 1, 1])Foo(5, [1, 1, 1])julia&amp;gt; fmap(float, model)Foo(5.0, [1.0, 1.0, 1.0])The term ‚Äúfunctor‚Äù comes from category theory in pure mathematics. A category consists of a class of objects as well as functions between those objects. For example, Top is the category of topological spaces and continuous functions between topological spaces. A functor\\[F: C\\to D\\]is a mapping from category $C$ to category $D$ such that relations of functions in $C$ are preserved into mapped functions in $D$. Specifically, for any two functions $f$ and $g$ in $C$, we have $F(g \\circ f) = F(g) \\circ F(f)$ in $D$. See here for full definition. The subject arises from the study of algebraic topology, where topological spaces are studied by mapping them to algebraic structures like groups.When marked with @functor, fmap can change ‚Äúleaves‚Äù of the more abstract convolutional layer type, like its numerical type for weights. So the analogy is that, the convolutional layer now acts like a functor that maps from one numerical type (like a ‚Äúcategory‚Äù) to another type (like another ‚Äúcategory‚Äù), while preserving its own (convolutional layer) type structure (i.e. being consistent for different types).PerspectivesJulia is a very promising language, but as of 2022, it is still not mature enough and it is still too early to say if it will become the language of choice for data science. Many people immediately fall in love with it after first encounter, while others find it hard to make shift to this new language. Here we discuss some challenges for users coming from languages like Python.First, let‚Äôs be clear, Julia is not really object oriented, and it is NOT MEANT to be so. You may be used to something like np.mean(...) or nn.Linear(...) in Python. After all, such OOP patterns are ‚Äúwell behaved‚Äù and ‚Äúpredictable‚Äù, in the sense that methods and data are all bundled under classes. But in Julia, you import packages with using XXX and after that you can just call functions in that package; there is no ‚Äúclasses with dots‚Äù thing. It leans more toward functional programming. At first, you may think it is not as well organized as an OOP language. But if you think about it, models in data science, and mathematics in general, are not related to OOP in any intrinsic way. A neural network model is a function, taking inputs and produce outputs. It is actually more natural to implement it as a function, rather than as some classes. It may be more annoying to write something like class ...., super(..., self).__init__() every time you implement a model, or something like np.linspace for making a plot. In Julia, you focus on your goal with less distractions from something like OOP that is not directly related to it: to implement models and perform calculations.Second, Julia leaves many with the impression that its syntax is full of symbols (!, ., @, $‚Ä¶) that makes the code hard to read. Having to explicitly declare types with double colon sign :: is also frawned upon by new users from Python. Indeed, the ease of learning for Python greatly lowered the entry bar for programming, and it is one reason why data science and machine learning have become so popular in recent years. Many people with no previous computer science and math training are now able to learn a programming language and find relevant jobs. This greatly expanded the Python community and brought more funding and package development and everything. Nowadays with the Python ecosystem, people really don‚Äôt have to worry about lower level details like types when they do machine learning. Julia seems to counter this trend. But let‚Äôs not ignore the fact that, as the field of data science grows, models are becoming larger and larger. We need large capacities to process more data than before. Speed is becoming more and more important, and it is even likely to become a crucial factor in applications and research alike. We can say that, for languages that are fast, Julia is the easiest to learn and use.SummaryIn the post we introduced the Julia syntax and many of its unique features. Multiple dispatch is its most distinct feature than other languages. It will take some time to see whether Julia can become the dominant language of choice for deep learning.Cite as:@article{lifei2022julia, title = &quot;Julia in 15 Minutes&quot;, author = &quot;Li, Fei&quot;, journal = &quot;https://volagold.github.io&quot;, year = &quot;2022&quot;, url = &quot;https://volagold.github.io/posts/julia/&quot;}" }, { "title": "The REINFORCE Algorithm", "url": "/posts/reinforce-algorithm/", "categories": "Data Science", "tags": "reinforce", "date": "2022-04-30 16:30:00 +0800", "snippet": " In this short post, we give an introduction to the REINFORCE algorithm.The REINFORCE algorithm (Williams, 1992) is a method used to evaluate parameter gradients of an expectation that does not depend on the parameter in a differentiable way. It comes from reinforcement learning. In such a setting, an agent is in an environment represented by a set of states, and each of his action leads to a reward and the next state. His objective is to choose action sequence that maximize the rewards. He has a parameterized policy function that samples action given a state, and our goal is to learn the parameters of the model. The set of actions available may be discrete, so in this setting we need to backpropagate through a discrete sampling process.In short, it is about moving the gradient operation inside the expectation operation.Let \\(\\mathcal{X}=\\{0,1\\}^d\\) be a discrete space of dimension $d$, which contains some object of interest. Let $f$ be a real valued function defined on $\\mathcal{X}$, and let $p_\\alpha$ be a discrete probability density on $\\mathcal{X}$ with parameter $\\alpha$. We would like to compute\\[\\nabla_\\alpha \\mathbb{E}_{x\\sim p_\\alpha}f(x),\\]the gradient of the expectation of $f(x)$ under density $p_\\alpha$ with respect to parameter $\\alpha$. The expectation is our objective, and we need the gradient in order to update parameter $\\alpha$. The difficulty is that, the sampling process $x\\sim p_\\alpha$ is discrete, so it is not possible to express the sample $x$ as a differentiable function of the parameter $\\alpha$. Such a function is a step function, with zero derivatives almost everywhere and undefined derivatives at the boundaries. Taking gradient with respect to $\\alpha$ would not give us much useful information.Let‚Äôs see how we can use some trick to solve this. Using the gradient rule for the log function, we can write$$\\begin{equation}\\label{eq:reinforce-intro}\\begin{split}\\nabla_\\alpha \\mathbb{E}_{x\\sim p_\\alpha}f(x) &amp;amp;= \\nabla_\\alpha \\int f(x)p_\\alpha(x)dx\\\\&amp;amp;= \\int f(x)\\nabla_\\alpha p_\\alpha(x)dx\\\\&amp;amp;= \\int f(x)\\frac{\\nabla_\\alpha p_\\alpha(x)}{p_\\alpha(x)}p_\\alpha(x)dx\\\\&amp;amp;= \\int\\left\\{f(x)\\nabla_\\alpha\\log p_\\alpha(x)\\right\\}p_\\alpha(x)dx\\\\[0.2cm]&amp;amp;= \\mathbb{E}_{x\\sim p_\\alpha}[f(x)\\nabla_\\alpha\\log p_\\alpha(x)],\\end{split}\\end{equation}$$where the integrals are Lebesgue integrals, with respect to the discrete measure on $\\mathcal{X}$. The exchange of gradient and integration operation is justified by Leibniz integral rule. The last term in \\eqref{eq:reinforce-intro} can be approximated by sample mean, which is an unbiased Monte Carlo estimator of the gradient. We see that we are now able to circumvent the need to differentiate discontinuous functions. Thus, given samples \\(\\{x^{(i)}\\}_{i=1}^N\\), computing \\(\\nabla_\\alpha\\mathbb{E}_{x\\sim p_\\alpha}f(x)\\) means computing\\[\\begin{equation}\\label{eq:reinforce-intro-samples}\\frac{1}{N}\\sum_{i=1}^Nf\\left(x^{(i)}\\right)\\nabla_\\alpha\\log p_\\alpha\\left(x^{(i)}\\right).\\end{equation}\\]We can now sample from the distribution $p_\\alpha$, treat the samples as fixed, and then evaluate \\eqref{eq:reinforce-intro-samples} on the samples.However, being unbiased usually means the new formula \\eqref{eq:reinforce-intro-samples} can have high variances. To reduce variance, in practice one subtracts a baseline function $b(\\alpha)$ to $f(x)$ that does not depend on each sample. This does not alter the gradient since$$\\begin{split}\\mathbb{E}_{x\\sim p_\\alpha}[(f(x)-b(\\alpha))\\nabla_\\alpha\\log p_\\alpha(x)] &amp;amp;= \\mathbb{E}_{x\\sim p_\\alpha}[f(x)\\nabla_\\alpha\\log p_\\alpha(x)] - \\mathbb{E}_{x\\sim p_\\alpha}[b(\\alpha)\\nabla_\\alpha\\log p_\\alpha(x)]\\\\[1em]&amp;amp;= \\mathbb{E}_{x\\sim p_\\alpha}[f(x)\\nabla_\\alpha\\log p_\\alpha(x)] - 0\\\\[1em]&amp;amp;= \\mathbb{E}_{x\\sim p_\\alpha}[f(x)\\nabla_\\alpha\\log p_\\alpha(x)],\\end{split}$$where we used the fact that$$\\mathbb{E}_{x\\sim p_\\alpha}[\\nabla_\\alpha\\log p_\\alpha(x)]=0.$$To see this,$$\\begin{split}\\mathbb{E}_{x\\sim p_\\alpha}[\\nabla_\\alpha\\log p_\\alpha(x)] &amp;amp;= \\int\\nabla_\\alpha\\log p_\\alpha(x)dx\\\\&amp;amp;= \\int\\frac{\\nabla_\\alpha p_\\alpha(x)}{p_\\alpha(x)}p_\\alpha(x)dx\\\\&amp;amp;= \\int\\nabla_\\alpha p_\\alpha(x)dx\\\\&amp;amp;= \\nabla_\\alpha\\int p_\\alpha(x)dx\\\\[1em]&amp;amp;= \\nabla_\\alpha 1\\\\[1em]&amp;amp;= 0.\\end{split}$$Cite as:@article{lifei2022reinforce, title = &quot;The REINFORCE Algorithm&quot;, author = &quot;Li, Fei&quot;, journal = &quot;https://volagold.github.io&quot;, year = &quot;2022&quot;, url = &quot;https://volagold.github.io/posts/reinforce-algorithm/&quot;}ReferenceWilliams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8(3), 229‚Äì256." }, { "title": "Attention and Transformers", "url": "/posts/attention-and-transformers/", "categories": "Data Science", "tags": "nlp, attention, transformer", "date": "2022-04-11 22:30:00 +0800", "snippet": " In recent years, Transformer-based models are trending and are quickly taking up not only the field of NLP, but also computer vision and many other fields in AI. In this post, we give a tutorial on Transformer, and talk about several state of the arts models based on it, including GPT and BERT.The Attention ModelEven with LSTM units, problems remain for recurrent neural network models: Passing data forward through an extended series of recurrent connections leads to loss of information and to difficulties in training. Moreover, inherently sequential nature of recurrent networks inhibits the use of parallel computational resources.Attention [1] model (Figure 1) takes a different approach. Each output element $y_i$ in the output sequence $Y=y_1\\cdots y_n$ is computed directly as a function of $x_1,\\ldots,x_i$, the input sequence up to element $i$. Namely, $y_i$ is connected to $x_1,\\ldots,x_i$ by direct connections. Thus, computation at $y_i$ is independent of computations for other outputs, so that they can be performed in parallel.Figure 1: Self-Attention LayerSpecifically, output $y_i$ at position $i$ is a weighted sum of input $i$ with all previous inputs, $x_1,\\ldots,x_i$:\\[y_i = \\alpha_{i1}x_1 + \\alpha_{i2}x_2 + \\cdots + \\alpha_{ii}x_i.\\]The weight $\\alpha_{ij}$ is a normalized score $s(x_i,x_j)$ for $x_i$ and $x_j$, reflecting their similarity, the simplest of which is the dot product. Specifically,\\[\\alpha_{ij} = \\frac{ \\exp(s(x_i, x_j)) }{\\sum\\limits_{j&#39;=1}^i\\exp(s(x_i,x_{j&#39;}))} \\qquad \\forall j\\leq i.\\]To allow the possibility for learning, instead of summing raw input values in the above equation for calculating $y_i$, each input $x_i$ is first mapped to three vectors via three weight matrices:\\[\\begin{align}k_i &amp;amp;= W^Kx_i,\\\\q_i &amp;amp;= W^Qx_i,\\\\v_i &amp;amp;= W^Vx_i.\\end{align}\\]And then, the key vector $k_j$ replaces $x_j$ in $s(x_i,\\textcolor{red}{x_j})$ for $j\\leq i$; the query vector $q_i$ replaces $x_i$ in $s(\\textcolor{red}{x_i},x_j)$; the value vector $v_i$ replaces $x_i$ in \\[y_i = \\alpha_{i1}x_1 + \\alpha_{i2}x_2 + \\cdots + \\alpha_{ii}\\textcolor{red}{x_i}.\\]See figure below for an illustration when calculating $y_3$.Figure 2: Illustration for calculating the third output element $y_3$ in an attention model.Practical ConsiderationsIn practice, since the score $s(x_i, x_j)= q_i \\cdot k_j$ can be very large ($+\\infty$) or small ($-\\infty$), it can cause overflow or underflow problem when taking the exponentials in calculating the normalized weight $\\alpha_{ij}$. To mitigate this issue, one may divide the dot product by the square root of the dimensionality of the query and key vectors:\\[s(x_i, x_j) = \\frac{q_i\\cdot k_j}{\\sqrt{d_k}}.\\]Due to the parallel nature of the model, we can concatenate all the input embeddings into a single matrix $X$, and multiply it by the key, query and value matrices to get matrices containing all the $k,q$ and $v$ vectors.\\[\\begin{align}Q &amp;amp;= W^QX\\\\K &amp;amp;= W^KX\\\\V &amp;amp;= W^VX.\\end{align}\\]Finally, we can reduce the entire self-attention step for an entire sequence to the following computation:\\[\\mathrm{SelfAttention}(X) = \\mathrm{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V.\\]This brings up a new problem though. The calculation of matrix product $QK^T$ results in a score $s(x_i,x_j)$ for each $x_i$ and each $x_j$ in input sequence $(x_1,\\ldots,x_n)$, including those $x_j$ for $j&amp;gt;i$. In language modeling, our task is to predict the next word, so we may only use $x_1,\\ldots,x_i$ as inputs to produce output $y_i$. Using $x_{i+1},\\ldots,x_n$ to predict $x_{i+1}$ would not lead to meaningful learning. To fix this, the elements in the upper-triangular portion of the comparisons matrix are set to $-\\infty$, thus eliminating any knowledge of words that follow in the sequence.TransformersThe Transformer [2] model (Figure 3) is built on top of the Attention model, to acheive more potentials. First, in the transformer block, input is added to the output from the self-attention layer and normalized. Then it goes through a feedforward layer and then again residual connection and normalization. The Transformer block can be stacked and combined with other layers, to form a final model. The self-attention layer in Figure 3 can be replaced with multi-head attention layer (Figure 4), in which you map an input to several self-attention layers in parallel, each with its own parameters, and then concatenate them and map the result to an output with the original dimension. The motivation for this is to let those self-attention layers capture different relationships among the inputs $x_1,\\ldots,x_n$. Finally, to further capture the information about the sequential order of the inputs, some positional embeddings may be added to the input sequence. Figure 3: Transformer blockFigure 4: Multi-head attentionGenerative Pre-Training (GPT)The GPT [3] model was proposed by OpenAI in 2018. It is a Transformer model with generative pretraining and discriminative fine-tuning. Namely, it first trains on unlabeled text data to predict the next word, with NLL as the loss. Then for downstream tasks, you add a final output layer to the model, and you train on labeled data (with e.g. NLL as loss) to fine-tune the parameters of the model.The point is to take advantage of the large amount of cheap unlabeled data available in the wild, and to reduce the reliance on human-labeled data. The GPT-2 [4] model architecture is the same with GPT, just much larger. The authors examined performance of the model on discriminative tasks without the fine-tuning step, which is called ‚Äúzero-shot‚Äù.BERTBERT [5], which stands for ‚ÄúBidirectional Encoder Representations from Transformers‚Äù, is built upon GPT, with two improvements. It uses bidirectional Transformer blocks, rather than unidirectional Transformer blocks used in GPT (i.e. Figure 3). Namely, each unit in Figure 1 connects not only to previous and current words, but to all the input words. The reason for this is that, by establishing more direct connections to the inputs, the model could have more capacity. Pretraining consists of two tasks: (1) masked language modeling (Masked LM), where some random tokens are masked and the model learns to predict those tokens. This is to avoid failure of plain language modeling, since in such setting the bi-Transformer block can see the word it is going to predict, so training with such objective leads to collapse, as we have discussed earlier. (2) next sentence prediction (NSP). This is a binary task. Given two sentences $A$ and $B$, predict if $B$ is the next sentence that follows $A$. The purpose is to train a model that understands sentence relationships, so that it can have better performance in downstream tasks like Question Answering (QA) and Natural Language Inference (NLI). To get a more detailed sense of the BERT model, you can read the following quote from the paper [5]: To generate each training input sequence, we sample two spans of text from the corpus, which we refer to as ‚Äúsentences‚Äù even though they are typically much longer than single sentences (but can be shorter also). The first sentence receives the $\\texttt{A}$ embedding and the second receives the $\\texttt{B}$ embedding. $50\\%$ of the time $\\texttt{B}$ is the actual next sentence that follows $\\texttt{A}$ and $50\\%$ of the time it is a random sentence, which is done for the ‚Äúnext sentence prediction‚Äù task. They are sampled such that the combined length is $\\leq512$ tokens. The LM masking is applied after WordPiece tokenization with a uniform masking rate of $15\\%$, and no special consideration given to partial word pieces. We train with batch size of $256$ sequences ($256$ sequences $*$ $512$ tokens = 128,000 tokens/batch) for 1 million steps, which is approximately $40$ epochs over the $3.3$ billion word corpus. We use Adam with learning rate of 1e-4, $\\beta_1=0.9$, $\\beta_2=0.999$, L2 weight decay of $0.01$, learning rate warmup over the first $10,000$ steps, and linear decay of the learning rate. We use a dropout probability of $0.1$ on all layers. We use a $\\texttt{gelu}$ activation rather than the standard $\\texttt{relu}$, following OpenAI GPT. The training loss is the sum of the mean masked LM likelihood and the mean next sentence prediction likelihood. For fine-tuning, most model hyperparameters are the same as in pre-training, with the exception of batch size ($16,32$), learning rate (e.g. 5e-5), and number of training epochs ($2,3,4$). ‚Ä¶‚Ä¶ The core argument of this work is that the bi-directionality and the two pre-training tasks account for the majority of the empirical improvements.Summary &amp;amp; What‚Äôs NextSo here we are. We have studied all the common models in NLP up to now, from the simplest n-gram model, to word2vec, Hidden Markov Model, Conditional Random Field, recurrent neural networks, to Transformer-based models like GPT and BERT in this post. Transformer models achieve better results because more direct connections between inputs and outputs lead to more efficient learning. Not only are they the state-of-the-art models in NLP, but computer vision, for which convolutional neural networks were once dominant, also sees the trend of transitioning to Transformer-based models. See e.g. ViT [6] and MAE [7].Another noticeable trend in AI research is that, big companies are training ever bigger models with billions of parameters. See e.g. Microsoft‚Äôs Florence [8] or Google‚Äôs Pathways [9] model. The cost of training such models is astronomical, which often requires GPU resources that cost hundreds or thousands of millions of dollars. This is simply prohibitive for any individual. Gone are the days when someone can just sit in front of a desk and implement and run a state-of-the-art model on his or her laptop or on a rented machine in the cloud.While pushing the boundary of neural network models by boosting model size is perfectly legitimate, there are many other important problems to consider. One fundamental problem is, how do neural network models actually work? What do they learn? What is the function of each parameter? Second, how should a model acquire robust generalization ability from less data, just as humans do? And is neural network really the ultimate paradigm for Artificial Intelligence? While current models may still have limited capacity, I‚Äôm optimistic that, we should see better and better AI models coming out in the future, and one day machines can have enough intelligence to handle tasks and jobs we do today.Cite as:@article{lifei2022transformers, title = &quot;Attention and Transformers&quot;, author = &quot;Li, Fei&quot;, journal = &quot;https://volagold.github.io&quot;, year = &quot;2022&quot;, url = &quot;https://volagold.github.io/posts/attention-and-transformers/&quot;}References[1] Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. ‚ÄúNeural machine translation by jointly learning to align and translate.‚Äù arXiv preprint arXiv:1409.0473 (2014).[2] Vaswani, Ashish, et al. ‚ÄúAttention is all you need.‚Äù Advances in neural information processing systems 30 (2017).[3] Radford, Alec, et al. ‚ÄúImproving language understanding by generative pre-training.‚Äù (2018).[4] Radford, Alec, et al. ‚ÄúLanguage models are unsupervised multitask learners.‚Äù OpenAI blog 1.8 (2019): 9.[5] Devlin, Jacob, et al. ‚ÄúBert: Pre-training of deep bidirectional transformers for language understanding.‚Äù arXiv preprint arXiv:1810.04805 (2018).[6] Dosovitskiy, Alexey, et al. ‚ÄúAn image is worth 16x16 words: Transformers for image recognition at scale.‚Äù arXiv preprint arXiv:2010.11929 (2020).[7] He, Kaiming, et al. ‚ÄúMasked autoencoders are scalable vision learners.‚Äù arXiv preprint arXiv:2111.06377 (2021).[8] Yuan, Lu, et al. ‚ÄúFlorence: A New Foundation Model for Computer Vision.‚Äù arXiv preprint arXiv:2111.11432 (2021).[9] Chowdhery, Aakanksha, et al. ‚ÄúPaLM: Scaling Language Modeling with Pathways.‚Äù arXiv preprint arXiv:2204.02311 (2022)." }, { "title": "RNN, LSTM and GRU", "url": "/posts/rnn/", "categories": "Data Science", "tags": "nlp, rnn, lstm, gru", "date": "2022-04-08 22:00:00 +0800", "snippet": " In this post we introduce the recurrent neural network (RNN) model in natural language processing (NLP), as well as two improvements over it, long short-term memories (LSTM) and gated recurrent units (GRU).Natural languages are non-linear. Flows of words and sentences in texts often do not follow simple and predictable patterns. The probability of a word appearing at position $i$ may depend on some words at very far past, instead of always depending on 2 or 3 words behind it. The RNN model is an attempt to address the challenge of long term dependencies.Recurrent Neural Networks (RNN)Suppose we are predicting $y_1,\\ldots,y_n$ for input sequence $x_1,\\ldots,x_n$. RNN‚Äôs solution is to re-use $h_{t-1}$, the output of the hidden layer at time $t-1$, in the calculation for $h_t$:\\[\\begin{align}h_t &amp;amp;= g(Uh_{t-1} + Wx_t),\\\\y_t &amp;amp;= f(Vh_t).\\end{align}\\]Since $h_{t-1}$ is a function of $x_{t-1}$, its values should incorporate information about $x_{t-1}$, and so by adding it into the calculation for $h_t$, information about $x_{t-1}$ should be passed to $h_t$ and then to the output $y_t$. See Figure 1 for the illustration of the RNN architecture. By passing information this way, it is hoped that useful information in $x_1,\\ldots,x_{t-1}$ can all be used in the calculation for $y_t$.Figure 1: RNN ArchitectureTo train RNN as a language model (Figure 2), for each word $x_i$ in the input sequence we output a softmax probability distribution for the next word. We minimize the NLL loss (or CrossEntropy loss) of the model output on the training corpus, by gradient descent algorithms.Figure 2: Applications of RNN ‚Äì language modelingTraining RNN for sequence labeling (Figure 3) is similar for training a language model, where we output probabilities over labels for each input and maximize the probability for the correct label.Figure 3: Applications of RNN ‚Äì sequence labelingWe can also apply RNN to sequence classification (Figure 4), where an entire document, paragraph or sentence is classified as belonging to some category of interest. The final hidden layer $h_n$ is taken to constitute a compressed representation of the entire sequence. In the simplest approach, $h_n$ serves as the input to a subsequent feedforward network that outputs softmax probabilities over all categories. The use of loss from a downstream application to adjust weights all the way through the network is referred to as end-to-end training.Figure 4: Applications of RNN ‚Äì sequence classificationBesides simple recurrent networks (SRN), there are stacked RNNs where you can stack multiple RNNs together. There are also bidirectional RNNs (Figure 5 &amp;amp; 6) where two seperate RNNs are trained in forward and backward directions respectively, and outputs from the two models are combined to form final output, by concatenation, element-wise summation, multiplication or averaging.Figure 5: Bidirectional RNN for sequence labelingFigure 6: Bidirectional RNN for sequence classificationNow let‚Äôs talk about drawbacks of recurrent neural networks. First, after going through multiple composite functions, information about the first few inputs $x_1,x_2,\\ldots$ can still be easily lost. Second, each element in input sequence is processed sequentially, which leads to a slow running time.Long Short-Term Memories (LSTM)To address the vanishing information problem in RNN, LSTM: adds an explicit context layer to the architecture; uses gates to control the flow of information.Figure 7: Architecture for the LSTM unitSee Figure 7 for the architecture. It accepts $c_{t-1},h_{t-1}$ and $x_t$ as inputs, and produces $c_t$ and $h_t$ as outputs. Each gate is a layer with its own parameters ${U, W}$, and $\\sigma$ as the activation function. Parameters of the gates are adjusted automatically during training for automatic context management (deleting or adding information). This avoids hand-crafted rules. The hope is that, by multiplying with the outputs from the forget gate, which should all be close to $0$ or $1$, useful information in previous context is retained, and information that is no longer needed is removed. Similar reasoning for the add gate and the output gate.We can summarize LSTM into two equations:\\[\\begin{cases}c_t = c_{t-1} \\odot f + g \\odot a\\\\h_t = \\tanh(c_t) \\odot o\\end{cases}\\]where $f, g, a$ and $o$ are all neural network mappings of inputs $h_{t-1}$ and $x_t$, each with their own parameters.Gated Recurrent Units (GRU)LSTM introduces a considerable amount of additional parameters. To ease this burden, gated recurrent units (GRUs) dispense the use of a separate context vector, and use only two gates, a reset gate $r$ and an update gate $z$:\\[\\begin{align}r_t &amp;amp;= \\sigma(U_rh_{t-1} + W_rx_t)\\\\z_t &amp;amp;= \\sigma(U_zh_{t-1} + W_zx_t)\\\\\\end{align}\\]The reset gate $r_t$ is a binary-like mask that either blocks information with values near zero or allows information to pass through unchanged with values near one. The update gate $z_t$ mixes the old hidden state and the new one.\\[\\begin{align}\\tilde{h}_t &amp;amp;= \\tanh\\left\\{U(r_t\\odot h_{t-1}) + Wx_t\\right\\}\\\\h_t &amp;amp;= (1-z_t)h_{t-1} + z_t\\tilde{h}_t.\\end{align}\\]See Figure 8 for the 4 basic neural units commonly used in RNNs for language modeling. This kind of modularity is key to the applicability of LSTM and GRU units.Figure 8: Comparison of 4 neural unitsSummaryRecurrent neural networks are designed to specifically model sequential data like texts. LSTM and GRU architectures are designed to further retain information. RNNs were once widely used in NLP, but now the focus has been shifted to Transformer based models like BERT, which we will introduce in the next post.Cite as:@article{lifei2022rnn, title = &quot;RNN, LSTM and GRU&quot;, author = &quot;Li, Fei&quot;, journal = &quot;https://volagold.github.io&quot;, year = &quot;2022&quot;, url = &quot;https://volagold.github.io/posts/rnn/&quot;}" }, { "title": "Sequence Labeling with HMM and CRF", "url": "/posts/sequence-labeling/", "categories": "Data Science", "tags": "nlp, ner, hmm, crf", "date": "2022-03-26 09:00:00 +0800", "snippet": " Sequence labeling is a classical task in natural language processing. In this task, a program is expected to recognize certain information given a piece of text. This is challenging, because even though input data items in real application can look similar, there may be small variations here and there that are comprehensible to humans but nevertheless are not present in other data items, so that it is difficult to use hard coded rules for solving the problem.Sequence labeling is the task in which we assign each word $x_i$ in an input word sequence a label $y_i$, so that the output sequence $Y$ has the same length as the input sequence $X$. Two specific scenarios are: Parts of speech tagging. Parts of speech refer to the grammatical property of a word: noun, verb, pronoun, preposition, adverb, conjunction, participle, and article. Named entity recognition (NER). In NER we would like to extract structured information from unstructured text, like name, location, school, time, price and so on. The named entity recognition problem can be converted to a sequence labeling problem, by giving each word a label. Anything outside of interest gets a label like ‚ÄúO‚Äù.Named entity recognition (NER) concerns extracting structured information from unstructured text data. Shown in the figure are resume items of two Chinese politically exposed persons (PEP), where their personal info could be extracted for anti money laundering purposes.We introduce two models for solving sequence labeling problem, the Hidden Markov Model (HMM) and the Conditional Random Field (CRF). Both are probabilistic models that make predictions by selecting tags/labels with largest probabilities. Don‚Äôt be intimidated by the two fancy names. They are something that you are probably already familiar with. HMM corresponds to generative modeling or Bayes‚Äô rule, and CRF corresponds to discriminative modeling or logistic regression. The difference is that, since inputs and outputs come in sequence, rather than treating each input and output element in isolation, previous inputs and outputs are taken into account when making predictions about the current output.Hidden Markov Model (HMM)HMM is generative. Given an input sequence $X=x_1\\ldots x_n$, the prediction about their labels $Y=y_1\\ldots y_n$ is\\[\\begin{split}Y^* &amp;amp;= \\mathop{\\mathrm{argmax}}_{Y}\\mathbb{P}(Y\\mid X)\\\\[1em]&amp;amp;= \\mathop{\\mathrm{argmax}}_{Y}\\frac{\\mathbb{P}(X\\mid Y)\\mathbb{P}(Y)}{\\mathbb{P}(X)}\\\\[1em]&amp;amp;= \\mathop{\\mathrm{argmax}}_{Y}\\mathbb{P}(X\\mid Y)\\mathbb{P}(Y)\\\\&amp;amp;= \\mathop{\\mathrm{argmax}}_{Y}\\prod_{i=1}^n\\mathbb{P}(x_i\\mid y_i)\\mathbb{P}(y_i\\mid y_{i-1}).\\end{split}\\]Namely, To select the most probable output sequence $Y$, we select $Y$ that makes text sequence $X$ that we observe most probable. There are two assumptions: Markov assumption: the probability of being in state $y_i$ only depends on the last state: $\\mathbb{P}(y_i\\mid y_{1},\\ldots,y_{i-1})=\\mathbb{P}(y_i\\mid y_{i-1})$, so that $\\mathbb{P}(y_1\\ldots y_n) = \\prod_{i=1}^n\\mathbb{P}(y_i\\mid y_{i-1})$ Output independence: the probability of observing $x_i$ only depends on the hidden state $y_i$, but not on previous words or hidden states. The name ‚ÄúHidden Markov Model‚Äù comes from the rhetoric that labels are like a hidden Markov chain with certain states and transition probabilities among them. These states ‚Äúemit‚Äù word sequences (text data).MLE estimations of the probabilities in the above equation are frequencies in the training corpus:\\[\\mathbb{P}(x_i\\mid y_i) = \\frac{\\#(x_i,y_i)}{\\# y_i}.\\]\\[\\mathbb{P}(y_i\\mid y_{i-1}) = \\frac{\\#(y_i, y_{i-1})}{\\# y_{i-1}}.\\]The Viterbi algorithmNow, to make inference in HMM, we have to select the best label \\(y_s^*\\) among a set of states \\(\\{y_1,\\ldots,y_S\\}\\) at each time step $i=1,\\ldots,n$. This requires some work. We achieve this, by calculating and saving the maximum value for each state \\(y_s\\) in \\(\\{y_1,\\ldots,y_S\\}\\) at each step $i$, denoted by \\(v(i, y_s)\\), as well as the pointer to the state at step $i-1$ that leads to this maximum value. At the final step, we select the label or state \\(y_n^*\\) with maximal value for \\(v(n, y_s)\\). Then we follow its pointer to backtrack from this label/state to the previous label/state \\(y_{n-1}^*\\), until we reach the beginning, to get the optimal output sequence \\(y_1^*,\\ldots,y_n^*\\). Yes, here we are using a dynamic programming algorithm to solve a graph maximization problem. It is called the Viterbi algorithm.In the course of the algorithm, we populate a table with value $v(i, y_s)$ and a pointer. The Bellman equation is\\[v(i, y_s) = \\max_{y\\in\\{y_1,\\ldots,y_S\\}} v(i-1, y)\\cdot \\mathbb{P}(x_i\\mid y_s)\\cdot \\mathbb{P}(y_s\\mid y).\\]See Figure 1 for an illustration of the algorithm. As it‚Äôs clear from the description above, its running time is \\(\\vert S\\vert^2\\cdot n\\). For a sequence \\(Y=y_1\\ldots y_n\\) with length \\(n\\) where each \\(y_i\\) can be any element in \\(\\{y_1,\\ldots,y_S\\}\\), there are \\(\\vert S\\vert^n\\) such paths in total. Without the Viterbi algorithm, finding the argmax would require an exponential amount of computation. With savings of intermediate results in memory, we are able to reduce a large amount of computation.Figure 1: Illustration of the Viterbi Algorithm for solving Hidden Markov Model (HMM). The model is a generative model. To select the most probable output sequence $Y$, we select $Y$ that makes text sequence $X$ most probable.Conditional Random Field (CRF)CRF is discriminative. It attempts to directly model $\\mathbb{P}(Y\\mid X)$ with multi-class logistic regression. The difference with vanilla logistic regression is that, not only isolated data points $(x_i,y_i)$, but also $y_{i-1}$, the previous label, are incorporated into features. Let $\\mathcal{Y}$ denote the set of all possible output sequences $Y$ with length $n$, which has an exponential size $|S|^n$. The model is\\[\\mathbb{P}(Y\\mid X) = \\dfrac{\\exp\\left(\\sum\\limits_{k=1}^Kw_kF_k(Y,X)\\right)}{ \\sum\\limits_{Y&#39;\\in\\mathcal{Y}}\\exp\\left(\\sum\\limits_{k=1}^Kw_kF_k(Y&#39;,X)\\right)}.\\]Or, write the denominator in the last equation as $Z(X)$,\\[\\mathbb{P}(Y\\mid X) = \\frac{1}{Z(X)}\\exp\\left(\\sum_{k=1}^Kw_kF_k(Y,X)\\right).\\]The $K$ features $F_1(Y,X),\\ldots,F_K(Y,X)$ are called global features. In linear chain CRF, each global feature is a summation of $n$ local features $f_k(\\cdot,\\,i), i=1,\\ldots,n$, where each $f_k(\\cdot,\\,i)$ could be an indicator function:\\[F_k(Y,X) = \\sum_{i=1}^nf_k(y_{i-1}, y_{i}, X, i).\\]Training is done by MLE, thanks to the fact that $n$ is not too large and the normalizing constant $Z(X)$ can be computed in reasonable time. For inference,\\[\\begin{split}Y^* &amp;amp;= \\mathop{\\mathrm{argmax}}_{Y}\\mathbb{P}(Y\\mid X)\\\\&amp;amp;= \\mathop{\\mathrm{argmax}}_{Y}\\sum_{k=1}^Kw_kF_k(Y,X)\\\\&amp;amp;= \\mathop{\\mathrm{argmax}}_{Y}\\sum_{k=1}^Kw_k\\sum_{i=1}^nf_k(y_{i-1}, y_{i}, X, i)\\\\&amp;amp;= \\mathop{\\mathrm{argmax}}_{Y}\\sum_{i=1}^n\\sum_{k=1}^Kw_kf_k(y_{i-1}, y_{i}, X, i),\\\\\\end{split}\\]and again we use the Viterbi algorithm for this sequence maximization problem.SummaryIn this post, we introduced two models for solving sequence labeling problem in NLP: Hidden Markov Model (HMM) and Conditional Random Field (CRF). HMM maximizes $\\mathbb{P}(Y\\mid X)$ by maximizing $\\mathbb{P}(X\\mid Y)\\mathbb{P}(Y)$, while CRF models $\\mathbb{P}(Y\\mid X)$ with features. Both models take $y_{i-1}$ into account when calculating probability on $y_i$. Both use the Viterbi algorithm for deriving the optimal output sequence $Y=y_1\\ldots y_n$.Cite as:@article{lifei2022hmm, title = &quot;Sequence Labeling with HMM and CRF&quot;, author = &quot;Li, Fei&quot;, journal = &quot;https://volagold.github.io&quot;, year = &quot;2022&quot;, url = &quot;https://volagold.github.io/posts/sequence-labeling/&quot;}" }, { "title": "Embeddings and the word2vec Method", "url": "/posts/embedding-and-word2vec/", "categories": "Data Science", "tags": "nlp, tf-idf, word2vec", "date": "2022-03-20 09:00:00 +0800", "snippet": " On its surface, words are not numbers. But words have similarities, differences, associations, relations and connections. For example, ‚Äò‚Äòman‚Äô‚Äô and ‚Äò‚Äòwoman‚Äô‚Äô are words that describe humans, while ‚Äò‚Äòapple‚Äô‚Äô and ‚Äò‚Äòwatermelon‚Äô‚Äô are fruit names. The flow of words in an article always follow certain rules. How to quantify a word so as to reflex its relationship with other words, as well as its statistics in training text data? In this post, we‚Äôll look at word embeddings, i.e. mapping of words in dictionary to multidimensional real vectors. We discuss two specific embedding methods, TF-IDF and word2vec. After words are embedded into a vector space, it is then possible to apply neural network models to them to output probabilities on words.Word embedding is the first step in most applications of NLP, like text analysis, text mining, machine translation and so on. For example, since a document is consisted of words, once we have word vectors, we can also represent a document by a vector in some way, for example by averaging over all word vectors in the document. Then we can compare similarity between two documents by $\\cos(d_1,d_2)$. This is useful for applications like information retrieval, plagiarism detection, and news recommendation system.TF-IDFHow to map or assign each word with a vector in some vector space? This entirely depends on our goal. One comes from information retrieval, where dimensions are documents $D$, and numerical value in each dimension $d$ of a word $w$ represents relevance/importance of that word $w$ to the document $d$. The list of word vectors (Table 1) is called a term-document matrix. We mention that dimensions can also be words themselves, where numerical value $(w_1, w_j)$ can represent number of times $w_j$ appears in a $\\pm4$ window around $w_1$ in some training corpus.Table 1: term-document matrixOnce we mapped each word with a vector, the most basic way to measure similarity between two words $v$ and $w$ is to use cosine\\[\\cos(v, w) = \\frac{v\\cdot w}{\\|v\\|\\|w\\|}.\\]Let‚Äôs now think about information retrieval. Given a query $q=(w_1,\\ldots,w_n)$, we need to rank the collection of documents $D$. The most straightforward way is to assign each document $d$ a value for $w_1$, a value for $w_2$, and so on, and add them up, to get a final value for $d$. Then ank documents in $D$ according to the outputs of this (linear) ranking function.How to assign a value to $(w,d)$ to represent relevance of word $w$ to document $d$? i.e., how to assign entry values in Table 1? The most straightforward approach is to count the number of times that $w$ appears in $d$:\\[\\mathrm{tf}_{w,d} = \\mathrm{count}(w,d).\\]Besides counting, there are other calculations (Table 2). Among them, the most common practice is to take $\\log_{10}$ of the raw count\\[\\mathrm{tf}_{w,d} = \\log_{10}(\\mathrm{count}(w,d)+1).\\]Table 2: Variants of tf termsNext, in the TF-IDF method, the TF term $\\mathrm{tf}_{w,d}$ is weighted by the inverse document frequency of the word $w$ across a set of documents $D$. This means how common or rare a word is in the entire document set. The closer it is to $0$, the more common a word is. This metric can be calculated by taking the total number of documents, dividing it by the number of documents that contain a word, and calculating the logarithm:\\[\\mathrm{idf}_{w} = \\log_{10}\\left(\\frac{N}{\\mathrm{df}_{w}}\\right).\\]So, if a word is very common and appears in many documents, e.g. for words like ‚Äò‚Äòthe‚Äô‚Äô, ‚Äò‚Äòand‚Äô‚Äô, or ‚Äò‚Äòthis‚Äô‚Äô, this number will approach $0$. Otherwise, it will approach infinity.Multiplying these two numbers results in the TF-IDF score of a word in a document\\[(w,d) = \\mathrm{tf}_{w,d} \\cdot \\mathrm{idf}_{w}\\]The higher the score, the more relevant that word is in that particular document.To summerize, TF-IDF is basically word count in a document, adjusted by some functions and document frequencies.word2vecFrom the viewpoint of embedding, the TF-IDF method produces sparse vectors (Table 1), with most entries being zero. In contrast, the word2vec method [1] produces dense word vectors, typically of dimension $50\\sim1000$. Actually we are referring to ‚Äúskip-gram with negative sampling‚Äù in the word2vec software package, but it is often loosely referred to as word2vec.First randomly initialize some word as a vector $w\\in\\mathbb{R}^d$. Define a binary classification task as follows: given $w\\in\\mathbb{R}^d$, predict if $c\\in\\mathbb{R}^d$ is a context word, i.e. if it should generally appear near $w$ in text data. Define the probability of being positive as\\[\\mathbb{P}(+\\mid w, c) = \\sigma(w\\cdot c) = \\frac{1}{1+\\exp(-w\\cdot c)}.\\](The probability of being negative is then\\(\\mathbb{P}(-\\mid w, c) = 1- \\sigma(w\\cdot c) = \\frac{\\exp(-w\\cdot c)}{1+\\exp(-w\\cdot c)}.\\))For context $c_{1:L}$, the model makes the assumption that all context words are independent, so the probability is\\[\\mathbb{P}(+\\mid w, c_{1:L}) = \\prod_{i=1}^L\\sigma(w\\cdot c_i).\\]\\[\\log\\mathbb{P}(+\\mid w, c_{1:L}) = \\sum_{i=1}^L\\log\\sigma(w\\cdot c_i).\\]For each word $w$, every nearby word within e.g. $L=\\pm2$ window in the training data is a positive context. An example is as follows:For every positive sample $(w,c)$, we generate $k$ noise samples $(w,n_1),\\cdots,(w,n_k)$ as negative samples by randomly sampling the lexicon. In practice, it is common to use not the plain word frequencies, but\\[\\mathbb{P}_{\\alpha}(w) = \\frac{\\mathrm{count}(w)^\\alpha}{\\sum_{w&#39;}\\mathrm{count}(w&#39;)^\\alpha}\\]as the probabilities, with $\\alpha=0.75$, to give less frequent words higher probabilities.Then, we can train the logistic regression model to classify positive samples and negative samples. For each sample pair \\(\\{(w,c),(w,n_1),\\ldots,(w,n_k)\\}\\), the negative log-likelihood loss is\\[\\begin{split}L &amp;amp;= -\\log\\left[\\mathbb{P}(+\\mid w,c)\\prod_{i=1}^k\\mathbb{P}(-\\mid w,n_i)\\right]\\\\&amp;amp;= -\\left[\\log\\sigma(w\\cdot c) + \\sum_{i=1}^k\\log[1-\\sigma(w\\cdot n_i)]\\right].\\end{split}\\]The word2vec embedding method is an example of Noise Contrastive Estimation. When we need to learn parameters of a non-discriminative model for which we do not have an easy objective, we can convert the problem to a binary classification task and let the model learn to distinguish between positive training samples and noises. Here, a positive sample $(w,c)$ simply consists of a word $w$ and a surrounding word $c$. The weight $w\\in\\mathbb{R}^d$ is learned so that it is similar (i.e. has a large dot product value) to its surroundings in the training corpus.Note that, in the end, for each word, two set of parameters are learned because a word can be either a word for which we want to have an embedding, and a context or noise for other words. Thus if we denote the vocabulary set as $V$, then the output of the algorithm is a matrix of $2\\vert V\\vert$ vectors, each of dimension $d$, formed by concatenating the target embedding $W$ and the context $+$ noise embedding $C$. It is common to add them together, representing word $i$ with the vector $w_i+c_i$. Alternatively we can throw away the $C$ matrix and just represent each word $i$ by the vector $w_i$.Neural language modelsOnce we have word embeddings, we can use them as inputs to neural networks models, for modeling probabilities on words and sentences. Embeddings should produce better generalization than primitive models like $n$-gram models. For example, suppose in the training data we have the sentence‚Äú‚Ä¶make sure that the electric car gets charged‚Äù,and our test set has‚Äú‚Ä¶make sure that the electric vehicle gets [ ]‚Äù.Suppose the word ‚Äúvehicle‚Äù is in the training set for the embedding, but there is no ‚Äò‚Äòvehicle gets‚Ä¶.‚Äô‚Äô. An $n$-gram model would be unable to infer a reasonable prediction from the training data. But a neural language model, knowing that ‚Äò‚Äòcar‚Äô‚Äô and ‚Äò‚Äòvehicle‚Äô‚Äô have similar embeddings, should be able to generalize from the ‚Äò‚Äòcar‚Äô‚Äô context to assign a high enough probability to ‚Äò‚Äòcharged‚Äô‚Äô following ‚Äò‚Äòvehicle‚Äô‚Äô.Embeddings can also be learned during training. We can initialize $\\vert V\\vert$ embeddings, each of dimension $d$, and directly feed them into a neural network, without using embeddings pretrained by e.g. word2vec. In other words, we can encode each word as a one-hot vector $x$ of size $\\vert V\\vert\\times1$, with value $1$ in some index and $0$ otherwise, and initialize a random $d\\times\\vert V\\vert$ matrix $E$ as the embedding matrix. The product $Ex$ then selects the embedding for word $x$. The language model becomes\\[\\begin{align*}e &amp;amp;= (Ex_1,Ex_2,\\ldots,Ex_{|V|}),\\\\h &amp;amp;= \\sigma(We + b),\\\\z &amp;amp;= Uh,\\\\\\hat{y} &amp;amp;= \\mathrm{softmax}(z).\\end{align*}\\]See Figure 1. The model is initialized with random weights. Training proceeds by concatenating all the sentences to a very long text and then iteratively moving through the text predicting each word $w_t$. The neural language model was first proposed by Bengio et al. [2].Figure 1: Neural language modelSummaryBy embedding words into real vector spaces in a way that reflects their relations in training corpus, we are able to utilize model continuity for better generalization over $n$-gram models: a small input variation should yield a small output variation. word2vec is one such method, where a word vector is learned by distinguishing between its surrounding contexts and random noises. On the other hand, we can see from the figure above that the language model is data hungry: a large amount of training data is needed to cover all words and all possible contexts. If a word never appears in the training corpus, its embedding would not be learned. The model is still very ‚Äúcoarse‚Äù, and processes limited capacity such that problems like long term dependency are not addressed.Cite as:@article{lifei2022embedding, title = &quot;Embeddings and the word2vec Method&quot;, author = &quot;Li, Fei&quot;, journal = &quot;https://volagold.github.io&quot;, year = &quot;2022&quot;, url = &quot;https://volagold.github.io/posts/embedding-and-word2vec/&quot;}References[1] Mikolov, Tomas, et al. ‚ÄúEfficient estimation of word representations in vector space.‚Äù arXiv preprint arXiv:1301.3781 (2013).[2] Bengio, Yoshua, R√©jean Ducharme, and Pascal Vincent. ‚ÄúA neural probabilistic language model.‚Äù Advances in Neural Information Processing Systems 13 (2000)." }, { "title": "Natural Language Processing, Probabilities and the n-gram Model", "url": "/posts/nlp-prob-ngram/", "categories": "Data Science", "tags": "nlp, n-gram", "date": "2022-03-05 16:20:00 +0800", "snippet": " This is the first one of a series of posts on Natural Language Processing (NLP). We give an introduction to the subject, mention two model evaluation methods, see how to assign probabilities to words and sentences, and learn the most basic model ‚Äî‚Äî the n-gram model in NLP.IntroductionThe field of Natural Language Processing (NLP) is concerned with automatic processing of language data. It arose with the widespread usage of the Internet. There are many scenarios for which NLP techniques can be useful. An email service provider may wish to have a program to filter spam emails. Some data companies regularly scrape the web to extract information from millions of text entries. Marketing companies extract sentiments from online comments and reviews. Softwares can be developed to transcribe speech into texts, and translate one language to another. We can even ask machines to generate articles and reports for us.If you think about it, it is very hard to transfer our knowledge about human languages to a computer program.Languages are formed from the incentive to communicate information among creatures. They evolve from thousands of years of social development and transitions. A piece of text is not a mere permutation of words. Rather, it is a reflection of the 3D world, a condensed representation of much higher dimensional complexities. How could a model really understand the word ‚Äúsea‚Äù when it never have the chance to directly see the ocean like humans do? Much more complicated concepts like ‚Äúthink‚Äù, ‚Äúabstract‚Äù, ‚Äúeconomy‚Äù would be more challenging to learn. It is thus very difficult to train a computer program for it to understand and then manipulate texts that depend on external matters that are beyond language itself.Just imagine how you would learn a foreign language solely based on a large amount of texts in that language. You are not given any vocabulary table, but you must learn the meaning of every word, the grammar, and every expression only based on statistics of texts. This sounds intimidating right? Now imagine doing the same task, this time being an infant and not understanding much of the world. You have to learn every concept like ‚Äúpolitics‚Äù and ‚Äútyranny‚Äù on your own‚Ä¶‚Ä¶I‚Äôm not sure if this is feasible at all. This thought experiment shows that, training a model to ‚Äúunderstand‚Äù human languages is never an obvious and easy task.But to solve the problem, we don‚Äôt have to try hard to jump to the ultimate and perfect solution at one leap, if there is one. We can always start from the easiest approach, and think about how we can use a better way to improve over it.Language modelsA language model is a model that assigns probabilities to words and sentences.Models are often trained to predict the next word give current contexts, i.e. output a probability distribution over vocabularies conditioned on previous texts. The output probabilities should reflect word statistics in the training data. Probabilities can be very useful for many applications. For example, for grammar correction, $\\mathbb{P}(\\text{This }\\textit{means}\\text{ in particular}) &amp;gt; \\mathbb{P}(\\text{This }\\textit{mean}\\text{ in particular})$.For evaluating models, the only way to see if a model improves the task at hand is through extrinsic evaluation, where you put two models in a real task, like spelling correction or speech recognition, and measure the performance of the two models. But it is time-consuming. Instead, one often uses intrinsic evaluation, namely after training two models, calculate probability outputs on some test set, and see which one gives higher numbers.The common metric is the perplexity. For a test set $W=w_1\\cdots w_N$ (i.e. the whole test set data, including all sentences), it computes\\[PP(w) = \\sqrt[N]{\\frac{1}{\\mathbb{P}(w_1\\cdots w_N)}}.\\]The lower the perplexity, the better. Minimizing perplexity is the same as maximizing probability.The n-gram modelThe n-grams model is the simplest model to achieve the goal of assigning probabilities to words: count the number of occurrences of each word (or word sequence). A two-word sequence $w_{i-1}w_i$ is called a bigram, like ‚ÄúI love‚Äù. A three-word sequence is called a trigram, like ‚ÄúI love you‚Äù.Specifically, suppose we want to compute $\\mathbb{P}(w_{1:n})$, the probability of a word sequence. The chain rule of probability tells us to multiply together the conditional probabilities:\\[\\mathbb{P}(w_{1:n}) = \\mathbb{P}(w_1)\\mathbb{P}(w_2\\mid w_1)\\mathbb{P}(w_3\\mid w_{1:2})\\cdots\\mathbb{P}(w_n\\mid w_{1:n-1}).\\]At this point, one makes the Markov assumption that the conditional probability of a word $w$ only depends on the previous 2 or 3 words. In bigram model, the probability only depends on the previous one word, i.e. $\\mathbb{P}(w_n\\mid w_{1:n-1})=\\mathbb{P}(w_n\\mid w_{n-1})$. Trigram model looks two words in the past, and so on. Using bigram model for example, the probability above simplifies to\\[\\mathbb{P}(w_{1:n}) = \\prod_{i=1}^n\\mathbb{P}(w_i\\mid w_{i-1}).\\]Now, at this point, the obvious way to estimate $\\mathbb{P}(w_i\\mid w_{i-1})$ is to count $w_{i-1}w_i$ among all bigrams starting with $w_{i-1}$, and calculate the proportion:\\[\\mathbb{P}(w_i\\mid w_{i-1}) = \\frac{\\#w_{i-1}w_i}{\\sum_{w}\\#w_{i-1}w},\\]which is also the MLE estimate.An implementation detail: we shall add &amp;lt;s&amp;gt; and &amp;lt;/s&amp;gt; to the beginning and end of each sentence in the training data, like ‚Äú&amp;lt;s&amp;gt; I like you the way you are &amp;lt;/s&amp;gt;‚Äù, to give the bigram context of the first word, and also make the model a true distribution on all sentences.This is basically the n-gram model. Just counting.Drawbacks and remediesn-gram models, while being simple, have obvious drawbacks. Words in a sentence can have long-distance dependencies, for example I can insert clauses into a sentence, to separate the subject and the object. Just like natural images are not likely to be color gradients, a paragraph is unlikely to develop in a linear, continuous and predictable way. In other words, languages, like many other high dimensional data, are highly nonlinear. The Markov assumption is the very drawback to this. Since the model is a primitive statistics of the training data, it falls short of predicting anything that are not in the training data. Training data can only be sparse in data space, but any good AI model should have the ability to generalize. One common remedy to this is to do smoothing to the probabilities. (a) Laplace (or add-one) smoothing ‚Äì add one to each count:\\[\\mathbb{P}^*(w_n\\mid w_{n-1}) = \\frac{\\#w_{i-1}w_i+1}{\\sum_{w}(\\#w_{i-1}w+1)} = \\frac{\\#w_{i-1}w_i+1}{\\sum_{w}\\#w_{i-1}w + V}.\\] Thus, if the $w_{i-1}$ never appears before, then we give any bigram $w_{i-1}w$ a probability of $1/V$. (b) Add-$k$ smoothing, where $k\\in[0,1]$:\\[\\mathbb{P}^*(w_n\\mid w_{n-1}) = \\frac{\\#w_{i-1}w_i+k}{\\sum_{w}\\#w_{i-1}w + kV}.\\] (c) Backoff: use trigram if the evidence is sufficient, otherwise use bigram, otherwise use unigram. In other words, we ‚Äúback off‚Äù if we have zero evidence for a higher-order n-gram. (d) Interpolation: use weighted average of trigram, bigram and unigram estimates, for example\\[\\mathbb{P}^*(w_n\\mid w_{n-2}w_{n-1}) = \\lambda_1\\mathbb{P}(w_n\\mid w_{n-2}w_{n-1}) + \\lambda_2\\mathbb{P}(w_n\\mid w_{n-1}) + \\lambda_3\\mathbb{P}(w_n)\\] where $\\lambda_1+\\lambda_2+\\lambda_3=1$. SummaryNLP is about teaching machines to extract information from texts, to classify texts, and to generate texts. The core ability that we want a model to acquire is the ability to generalize. We want a model to have enough intelligence and flexibility to handle all kinds of variations in downstream tasks that may not be present in the training data. Language models output probabilities over words. n-gram models are the simplest kind of language models, which give a primitive statistical summary of the training data. Although insufficient, they serve as a benchmark for more advanced models.Cite as:@article{lifei2022ngram, title = &quot;Natural Language Processing, Probabilities and the n-gram Model&quot;, author = &quot;Li, Fei&quot;, journal = &quot;https://volagold.github.io&quot;, year = &quot;2022&quot;, url = &quot;https://volagold.github.io/posts/nlp-prob-ngram/&quot;}" }, { "title": "Noise Contrastive Estimation", "url": "/posts/nce/", "categories": "Data Science", "tags": "nce, generative-model, energy-model", "date": "2022-02-20 15:20:00 +0800", "snippet": " Noise contrastive estimation (NCE) is a method to estimate high dimensional data distributions. It converts a generative learning problem to a discriminative learning one. Parameters of the distribution are learned by doing a classification between data and noise. In this post, we introduce NCE in detail, in the setting of estimating energy models.The problem that NCE [1] addresses is to estimate some data density $p_\\theta(x)$. It can be used to estimate language models [2], in which case an input $x$ is a word in a dictionary, and the probability is a conditional probability that conditions on some context of the input, for example certain amount of words preceeding the input in some training corpus. It can also be used to to estimate distributions on natural images.First, let‚Äôs formulate the problem as estimating an energy based model.Energy Based ModelsAny probability distribution can be specified as an energy based model (EBM). In energy model, we represent probability density on data as\\[p_\\theta(x) = \\frac{e^{-f_\\theta(x)}}{Z(\\theta)}\\]where $f_\\theta(x)$ is the energy function, and\\[Z(\\theta)=\\int e^{-f_\\theta(x)}dx\\]is the normalizing constant. In language models like word2vec, the energy function may simply be the inner product of the input word vector with some context word vector, i.e. $f^c(w) = w\\cdot c$, and the denominator is a sum over all words in the dictionary. In other applications, the energy function may be a neural network that maps input data to real valued numebers.The density $p_\\theta$ has high values on which the energy function $f_\\theta$ has low values, so the landscape of the energy $f_\\theta$ has low valleys along the data distribution and high hills outside the data distribution. Estimating an energy based model amounts to find ways to push down the energy function on the data manifold and push up elsewhere.Performing Maximum Likelihood Estimation (MLE) or sampling requires computation of the constant $Z(\\theta)$. However, for high dimensional data like languages and images, it is intractable to compute, and one has to rely on methods such as Langevin dynamics or Markov Chain Monte Carlo (MCMC) methods such as Hamiltonian Monte Carlo. The convergence of such methods may be slow, and may suffer especially if the dataset has multiple modes.Derivation of the MethodNCE is a method to estimate energy based models that circumvents the need to compute the normalizing constant. The normalizing constant is treated as a trainable parameter, so that the model is specified as\\[\\log p_\\theta(x)= -f_\\theta(x) - c\\]where $c=\\log Z(\\theta)$. In this case, it is not possible to do MLE because one can simply drive the log density towards infinity by letting $c\\to-\\infty$.Instead, the model is trained by doing a classification between the training data and some generated noises, in other words a (nonlinear) logistic regression. Label the training data as $\\xi=1$ and label the noises as $\\xi=0$. Let $q$ denote some noise distribution on the data space. Model $p(x\\mid \\xi=1)$ by $p_\\theta(x)$ and model $p(x\\mid \\xi=0)$ by $q(x)$. Suppose the prior distribution of the labels is $p(\\xi=1)=p(\\xi=0)=1/2$, i.e., we assume equal amount of data and noise are supplied. Then using Bayes rule, the posterior of the class labels are: posterior of $\\xi=1$ given $x\\in\\mathcal{X}$ from training dataset:$$p(\\xi=1\\mid x) = \\frac{p(x\\mid \\xi=1)p(\\xi=1)}{p(x\\mid \\xi=1)p(\\xi=1)+p(x\\mid \\xi=0)p(\\xi=0)} = \\frac{p_\\theta(x)}{p_\\theta(x)+q(x)},$$ posterior of $\\xi=0$ given generated sample $\\tilde{x}$:$$p(\\xi=0\\mid \\tilde{x}) = \\frac{p(\\tilde{x}\\mid \\xi=0)p(\\xi=0)}{p(\\tilde{x}\\mid \\xi=0)p(\\xi=0)+p(\\tilde{x}\\mid \\xi=1)p(\\xi=1)} = \\frac{q(\\tilde{x})}{q(\\tilde{x})+p_\\theta(\\tilde{x})}.$$Then, one can maximize the posterior log likelihood of training data \\(\\{x_i\\}_{i=1}^n\\) and noise \\(\\{\\tilde{x}\\}_{i=1}^n\\) by maximizing the objective$$J(\\theta) = \\mathbb{E}_{x\\sim p_{\\text{data}}}\\log\\frac{p_\\theta(x)}{p_\\theta(x)+q(x)} + \\mathbb{E}_{\\tilde{x}\\sim q}\\log\\frac{q(\\tilde{x})}{p_\\theta(\\tilde{x}) + q(\\tilde{x})}.$$where the expectations are approximated by sample means.We generally have three requirements for the noise distribution $q$: the density function $q(x)$ is tractable to compute, so that it can be evaluated on any input $x$; we can easily obtain samples $\\tilde{x}\\sim q$ from the noise distribution; the support of $q$ should cover the support of data distribution, so in particular $q(x)\\neq0$ for all $x$ such that $p_{\\text{data}}(x)\\neq 0$.Typically, for simple tasks one could use Gaussian as the noise distribution. Besides the three requirements above, the noise distribution should not be too far away from the data distribution, otherwise the learning task is too easy and the energy model may not be able to learn anything meaningful. From the perspective of the energy landscape, NCE amounts to push up the energy landscape only at the specific locations of the noises. Thus, it would be useful for the noise mode to be close to the data mode (but may not necessary to be too close) so that learning can be more efficient.Calculating the posteriorsIn implementations, we often output log probabilities. We mention here an alternative formula for the two posteriors\\[r(x):=\\frac{p_\\theta(x)}{p_\\theta(x)+q(x)}\\]and$$1-r(x)=1-\\frac{p_\\theta(x)}{p_\\theta(x)+q(x)} = \\frac{q(x)}{p_\\theta(x)+q(x)}.$$It is\\[r(x)=\\sigma(\\log p_\\theta(x) - \\log q(x)).\\]where $\\sigma$ is the sigmoid function. The derivation is as follows:$$\\begin{split}\\sigma(\\log p_\\theta(x) - \\log q(x)) &amp;amp;= \\left(1+\\exp\\left\\{-\\log\\frac{p_\\theta(x)}{q(x)}\\right\\}\\right)^{-1}\\\\[0.2cm]&amp;amp;= \\left(1 + \\frac{q(x)}{p_\\theta(x)}\\right)^{-1}\\\\[0.2cm]&amp;amp;= \\frac{p_\\theta(x)}{p_\\theta(x) + q(x)}.\\end{split}$$ImplementationSee my github repo for an implementation of NCE on 2D data. Below are several examples. The left column shows samples from target data distributions, the middle column is the Gaussian noise, and the right column shows the estimated densities during training.Cite as:@article{lifei2022nce, title = &quot;Noise Contrastive Estimation&quot;, author = &quot;Li, Fei&quot;, journal = &quot;https://volagold.github.io&quot;, year = &quot;2022&quot;, url = &quot;https://volagold.github.io/posts/nce/&quot;}References[1] M. Gutmann and A. Hyv√§rinen. ‚ÄúNoise-contrastive estimation: A new estimation principle for unnormalized statistical models‚Äù. Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics. Vol. 9. Proceedings of Machine Learning Research. PMLR, 2010, pp. 297‚Äì304.[2] Mnih, Andriy, and Yee Whye Teh. ‚ÄúA fast and simple algorithm for training neural probabilistic language models.‚Äù arXiv preprint arXiv:1206.6426 (2012)." }, { "title": "Automate Your Routines with Task Scheduler", "url": "/posts/windows-task-scheduler/", "categories": "Programming", "tags": "windows, cmd", "date": "2022-02-04 10:33:00 +0800", "snippet": " If you are using the windows operating system, do you know you can use the Task Scheduler application to run tasks at specific time? In this post I will show you how to do this. We‚Äôll also look at how to mute and unmute computer, play music, and open websites, all through the command line.Set up a task in Task SchedulerType ‚Äútask scheduler‚Äù in search bar. You should see the app appearing. Open the app.Step 1. create a taskClick ‚ÄúCreate Basic Task‚Äù on the right hand side.Give your task a name and a description. Click ‚ÄúNext‚Äù.Step 2. set up a trigger conditionSet up the time you want your script to be executed. You can run a task daily, weekly, monthly, or when the computer starts. Here we select ‚ÄúDaily‚Äù. Click ‚ÄúNext‚Äù, and set up your timeStep 3. specify your program pathClick ‚ÄúStart a program‚Äù and ‚ÄúNext‚Äù. Input the location for your script, and click ‚ÄúNext‚Äù.Click ‚ÄúFinish‚Äù, and you are done.Instead of ‚ÄúCreate Basic Task‚Äù, with ‚ÄúCreate Task‚Äù you can execute more than one script with more than one trigger time. You also have more options about when to run a task.That‚Äôs it! Having learned the steps for using the application, let‚Äôs now look at some programs that we can execute in Task Scheduler.Example 1: play a piece of music everydayHere is a real example. Suppose you are asked to play some music at 10:00AM everyday in your office, to remind your colleagues for a break. Specifically, your computer is muted, and you need to perform three steps: unmute your computer; play the music; mute your computer.How would you do that? Let‚Äôs go through a script step by step.Step 1. unmute your computerAs for now, there is no ‚Äúclean‚Äù way to adjust volume in windows through command line. A workaround solution is(echo Set WshShell = Wscript.CreateObject^(&quot;Wscript.Shell&quot;^) echo WshShell.Sendkeys &quot;¬°¬≠&quot;)&amp;gt;VolumeSwitch.VBS VolumeSwitch.VBS&amp;amp;del /f /q VolumeSwitch.VBSwhich is to write pressing the virtual Volume Mute key (value &amp;lt;0xAD&amp;gt;) command to a .vbs file, execute the file and then delete it.Step 2. play the musicstart music.mp3timeout /t 200 /nobreaktaskkill /im wmplayer.exe /t /f Simply call start with the filename to play the music with the default player (suppose it is windows media player). After you finished playing the music, you want to end the process, so you call taskkill. The argument /im &amp;lt;imagename&amp;gt; specifies the image name of the process to be terminated, in this case it is wmplayer.exe. /t ends the process and any child processes started by it. /f specifies that processes be forcefully ended. You need to wait for the music to finish playing before killing the process. Use timeout to pause the execution of the script for the length of the music. Otherwise, the process will be immediately killed and so the music would not be played. The argument /t &amp;lt;seconds&amp;gt; specifies the time to wait before next command is executed. In this case it is 200 seconds. /nobreak means ignore user key strokes. Step 3. mute your computerRun the same code in step 1 again to switch off your volume.(echo Set WshShell = Wscript.CreateObject^(&quot;Wscript.Shell&quot;^) echo WshShell.Sendkeys &quot;¬°¬≠&quot;)&amp;gt;VolumeSwitch.VBS VolumeSwitch.VBS&amp;amp;del /f /q VolumeSwitch.VBSThe complete script is shown below. Name your file with extention .bat (for ‚Äúbatch‚Äù file).@echo off (echo Set WshShell = Wscript.CreateObject^(&quot;Wscript.Shell&quot;^) echo WshShell.Sendkeys &quot;¬°¬≠&quot;)&amp;gt;VolumeSwitch.VBS VolumeSwitch.VBS&amp;amp;del /f /q VolumeSwitch.VBSstart music.mp3timeout /t 200 /nobreaktaskkill /im wmplayer.exe /t /f(echo Set WshShell = Wscript.CreateObject^(&quot;Wscript.Shell&quot;^) echo WshShell.Sendkeys &quot;¬°¬≠&quot;)&amp;gt;VolumeSwitch.VBS VolumeSwitch.VBS&amp;amp;del /f /q VolumeSwitch.VBSexitExample 2: open several websitesIf you are learning a new language, you might have the routine of opening several learning websites at once, for example a dictionary site, a Google Translate page, and a tutorial. Or you might open several news websites every morning in order to check the latest news. You can have a simple windows script for doing that, without opening every website manually every time.start www.wsj.comstart www.reuters.comstart www.twitter.comExample 3: write script in PythonIf you want to execute more complicated functionalities, or if you are not comfortable with windows commands but rather prefer to write your tasks in Python, you can use the pyinstaller package to convert your Python script to an executable .exe file. Then you are able to run it either through double click, or the Task Scheduler.To install the package, runpip install pyinstallerThen simply callpyinstaller --onefile yourscript.pyin the command line to convert your Python script to a single .exe executable file.SummaryIn this post, I showed how to use the Task Scheduler application in windows. With Task Scheduler, it is possible to execute scripts and programs when specific conditions are met. This can automate your routines and save your time and energy, setting you free from repetitive tasks. I discussed some examples, like playing an audio file and open several websites at once. If you find yourself repeating the same tasks very often on your PC, it may be a good idea to write them down in a script, and execute the script as needed.Cite as:@article{lifei2022scheduler, title = &quot;Automate Your Routines with Task Scheduler&quot;, author = &quot;Li, Fei&quot;, journal = &quot;https://volagold.github.io&quot;, year = &quot;2022&quot;, url = &quot;https://volagold.github.io/posts/windows-task-scheduler/&quot;}" }, { "title": "Visualizing the Double Dual", "url": "/posts/double-dual/", "categories": "Math & Natural Sciences", "tags": "math, linear-algebra, functional-analysis", "date": "2022-01-29 19:05:00 +0800", "snippet": " We provide a picture for visualizing the concept of double dual in linear algebra and functional analysis. We show that, every vector in a vector space $V$ naturally corresponds to a function that maps functionals on $V$ to numbers, namely evaluation of functionals on that vector. Conversely, thanks to a finite basis, every element in the double dual can be represented as such evaluation function for finite dimensional vector spaces.Definition (Linear Functional)¬†¬† Let $V$ be a finite dimensional real vector space. A linear functional on $V$ is a linear map from $V$ to $\\mathbb{R}$. Namely, $f:V\\to\\mathbb{R}$ is a linear functional if $f(v_1 + v_2) = f(v_1) + f(v_2)$ for all $v_1,v_2\\in V$; $f(\\alpha v) = \\alpha\\cdot f(v)$ for all $\\alpha\\in\\mathbb{R}$ and $v\\in V$.Definition (Dual Space)¬†¬† Let $V$ be a finite dimensional real vector space. Its dual space, denoted by $V^*$, is the set of all linear functionals on $V$.\\[V^* = \\big\\{f:V\\to\\mathbb{R}\\mid f \\text{ is linear}\\big\\}\\]The dual space is also a vector space: if \\(f,g\\in V^*\\), then \\(f+g\\in V^*\\) and \\(\\alpha f\\in V^*\\) for any \\(\\alpha\\in\\mathbb{R}\\). Furthermore, given a basis $(v_1,\\ldots, v_n)$ of $V$, it is possible to construct the dual basis $(f_1,\\ldots,f_n)$ in $V^*$ where for any $v=\\alpha_1v_1+\\cdots+\\alpha_nv_n\\in V$,\\[f_1(v)=\\alpha_1,\\,\\ldots,\\,f_n(v)=\\alpha_n.\\]This implies that $V$ and its dual space $V^*$ are isomorphic.Definition (Double Dual)¬†¬† The dual space of \\(V^*\\), denoted by \\(V^{**}\\), is called the double dual. Namely, the double dual is the set of all linear functionals on \\(V^*\\).\\[V^{**} = \\big\\{\\varphi:V^*\\to\\mathbb{R}\\mid \\varphi \\text{ is linear}\\big\\}\\]As before, the double dual is a vector space. If $\\varphi_1$ and $\\varphi_2$ are linear mappings defined on $V^*$, then for $a,b\\in\\mathbb{R}$, the mapping $a\\,\\varphi_1+b\\,\\varphi_2$ is again linear, so \\(a\\,\\varphi_1+b\\,\\varphi_2\\in V^{**}\\). Since $V$ and its dual \\(V^*\\) are isomorphic, it follows immediately that \\(V^*\\) and \\(V^{**}\\) are isomorphic, and consequently $V$ and its double dual \\(V^{**}\\) are isomorphic.The connection between $V$ and \\(V^{**}\\) can be seen in a more direct way. For each $v\\in V$, there is a \\(\\varphi_v\\in V^{**}\\) defined by\\[\\begin{align}\\varphi_v: V^*&amp;amp;\\to\\mathbb{R},\\\\[1em]f&amp;amp;\\mapsto f(v)\\in\\mathbb{R},\\quad f\\in V^*\\end{align}\\]The function $\\varphi_v$ maps any functional $f\\in V^*$ to a real value, while $f$ maps any vector $v\\in V$ to a real value. This isomorphism $v\\mapsto\\varphi_v$ is canonical. The picture below provides a visualization of this canonical isomorphism.Double Dual Visualization. $V$ and $V^{**}$ are isomorphic. For every $v\\in V$ there corresponds a \\(\\varphi_v\\in V^{**}\\) that maps any $f\\in V^*$ to $f(v)\\in\\mathbb{R}$.Each element in $V$ is like a ‚Äúmagnet‚Äù that can ‚Äúattract‚Äù linear functionals in \\(V^*\\). Imagine we grab some linear functionals \\(\\{f_1,f_2,\\ldots\\}\\) from the bag \\(V^*\\), and throw them toward a vector $v\\in V$. After they hit $v$, they are converted to their ‚Äúvalues‚Äù at $v$.\\(v_1\\) and \\(v_2\\) in the figure are like ‚Äútouchstones‚Äù that ‚Äútest‚Äù values for any \\(f\\in V^*\\). Since \\(f: V\\to\\mathbb{R}\\) can have different values at different vectors in \\(V\\), when we throw the same set of \\(f\\)s to a different vector, say \\(v_3\\in V\\), the resulting values would be different. Each \\(v\\in V\\) acts like a mapping from the domain \\(V^*\\) to real numbers. In this sense, we can identify each vector in \\(V\\) with an element in its double dual \\(V^{**}\\).On the other hand, given any \\(\\varphi\\in V^{**}\\), it is also possible to find a $v\\in V$ such that\\[\\varphi(f)=f(v)\\]for any \\(f\\in V^*\\).Let $(v_1,\\ldots,v_n)$ be a basis in $V$, and let $(f_1,\\ldots,f_n)$ be the dual basis in $V^*$, and finally let $(\\varphi_1,\\ldots,\\varphi_n)$ be the dual basis in \\(V^{**}\\). For\\[\\varphi=\\alpha_1\\,\\varphi_1+\\cdots+\\alpha_n\\,\\varphi_n\\]we prove that\\[v=\\alpha_1\\,v_1+\\cdots+\\alpha_n\\,v_n\\in V\\]is the desired vector. We can represent any \\(f\\in V^*\\) as $f=\\beta_1f_1 + \\cdots + \\beta_n f_n$ for some scalars $\\beta_1,\\ldots,\\beta_n\\in\\mathbb{R}$. Then,\\[\\begin{split}\\varphi(f) &amp;amp;= \\alpha_1\\varphi_1(f) + \\cdots + \\alpha_n\\varphi_n(f)\\\\[1em]&amp;amp;= \\alpha_1\\varphi_1(\\beta_1f_1 + \\cdots + \\beta_n f_n) + \\cdots + \\alpha_n\\varphi_n(\\beta_1f_1 + \\cdots + \\beta_n f_n)\\\\[1em]&amp;amp;= \\alpha_1\\beta_1 + \\cdots + \\alpha_n\\beta_n.\\end{split}\\]On the right hand side, we have\\[\\begin{split}f(v) &amp;amp;= \\beta_1f_1(v) + \\cdots + \\beta_nf_n(v)\\\\[1em]&amp;amp;= \\beta_1f_1(\\alpha_1\\,v_1+\\cdots+\\alpha_n\\,v_n) + \\cdots + \\beta_nf_n(\\alpha_1\\,v_1+\\cdots+\\alpha_n\\,v_n)\\\\[1em]&amp;amp;= \\beta_1\\alpha_1 + \\cdots + \\beta_n\\alpha_n.\\end{split}\\]We find that $\\varphi(f)=f(v)$ for any \\(f\\in V^*\\). The proof is now complete.Finally, we mention that, the finite dimensional case that we have discussed above does not generalize to infinite dimensions. If $V$ is an infinite dimensional vector space, then its (algebraic) dual space always has a larger dimension than $V$. The double dual is thus also strictly larger than $V$.Cite as:@article{lifei2022dual, title = &quot;Visualizing the Double Dual&quot;, author = &quot;Li, Fei&quot;, journal = &quot;https://volagold.github.io&quot;, year = &quot;2022&quot;, url = &quot;https://volagold.github.io/posts/double-dual/&quot;}" }, { "title": "Deformation Retractions are „ÄåContinuous„Äç Retractions", "url": "/posts/deformation-retraction/", "categories": "Math & Natural Sciences", "tags": "math, topology", "date": "2022-01-20 22:37:00 +0800", "snippet": " In this post we discuss retractions and deformation retractions in algebraic topology. Positive as well as negative examples are presented to illustrate the two concepts.I. RetractionsDefinition (Homotopy)¬†¬† Two continuous maps \\(f,g:X\\to Y\\) are said to be homotopic if there is a continuous map \\(H:X\\times[0,1]\\to Y\\) such that \\(H(x,0)=f(x)\\) and \\(H(x,1)=g(x)\\) for all \\(x\\in X\\). In this case \\(H\\) is called a homotopy between \\(f\\) and \\(g\\), and we write \\(f\\simeq g\\).Definition (Retraction)¬†¬† Let \\(X\\) be a topological space and let \\(A\\) be a subspace of \\(X\\). Then the map \\(r:X\\to A\\) is called a retraction if it is continuous, and \\(r(a)=a\\) for all \\(a\\in A\\) (i.e. \\(A\\) is kept fixed).Note that the range of the retraction map is the subspace \\(A\\), not \\(X\\).Example 1.1¬†¬† Let \\(X\\) be any nonempty topological space and let \\(x_0\\in X\\) be an arbitrary point. Then \\(r:X\\to\\{x_0\\}\\) defined by sending all points of \\(X\\) to \\(\\{x_0\\}\\) is trivially a retraction.Example 1.2¬†¬† Another trivial case: the identity map \\(\\mathbb{I}:X\\to X\\) is a retraction.Example 1.3¬†¬† A common example is to map the unit disk \\(D^2\\) to a smaller closed concentric disk \\(\\bar{D}(0,\\varepsilon)\\) of radius \\(\\varepsilon&amp;lt;1\\). Every \\(v\\in D^2, v\\notin \\bar{D}(0,\\varepsilon)\\) is mapped to \\((\\varepsilon/\\|v\\|)v\\) on the boundary, while points of \\(\\bar{D}(0,\\varepsilon)\\) are kept fixed.In all examples above, the subspace \\(A\\) always seems to be closed. In fact, this is typical:Proposition¬†¬† If \\(r:X\\to A\\) is a retraction, and \\(X\\) is Hausdorff, then \\(A\\) is closed in \\(X\\).Proof 1.¬† We prove \\(X\\setminus A\\) is open. Let \\(x\\in X\\setminus A\\). Then \\(r(x)\\in A\\) so that \\(x\\neq r(x)\\). Since \\(X\\) is Hausdorff, we can find two open neighborhoods \\(U\\) and \\(V\\) of \\(x\\) and \\(r(x)\\) respectively, such that \\(U\\cap V=\\varnothing.\\) Then \\(r^{-1}(V)\\) is open, since \\(r\\) is continuous, and further more \\(x\\in r^{-1}(V)\\), so that \\(r^{-1}(V)\\cap U\\) is an open neighborhood of \\(x\\). We claim that \\(r^{-1}(V)\\cap U\\) is disjoint from \\(A\\), from which the closeness of \\(A\\) follows. Let \\(y\\in r^{-1}(V)\\cap U\\). Then \\(y\\in U\\), while at the same time \\(r(y)\\in V\\). Since \\(U\\cap V=\\varnothing\\), \\(y\\neq r(y)\\), so that \\(y\\notin A\\). This completes the proof. Proof 2.¬† Define a continuous function \\(F\\) from \\(X\\) to \\(X\\times X\\) by sending each \\(x\\in X\\) to \\((x,r(x))\\). Let \\(D=\\{(x,x)\\,\\vert\\, x\\in X\\}\\) denote the diagonal of \\(X\\times X\\), which is closed in \\(X\\times X\\). Then \\(F^{-1}(D)=\\{x\\in X\\,\\vert\\, x=r(x)\\}=A\\) is closed. A retraction may be ‚Äútoo abrupt‚Äù, like example 1.1, even if the map itself is technically continuous. If \\(r\\) can continuously shrink space \\(X\\) to subspace \\(A\\) (or ‚Äúslowly‚Äù, ‚Äúsmoothly‚Äù, ‚Äúgently‚Äù in a loose sense), then we say that \\(r\\) is a deformation retraction.II. Deformation RetractionsNotation¬†¬† For \\(A\\subset X\\), we use \\(\\iota:A\\to X\\) to denote the inclusion map. (i.e., \\(\\iota(a)=a\\) for all \\(a\\in A\\))Definition (Deformation Retraction)¬†¬† Let \\(X\\) be a topological space and let \\(A\\) be a subspace of \\(X\\). Let \\(r:X\\to A\\) be a retraction. \\(r\\) is called a deformation retraction if the identity map \\(\\mathbb{I}:X\\to X\\) and \\(\\iota\\circ r:X\\to X\\) are homotopic, namely there exists a homotopy \\(H:X\\times[0,1]\\to X\\) such that \\(H(x,0)=x\\) and \\(H(x,1)=(\\iota\\circ r)(x)=r(x)\\) for all \\(x\\in X\\).Note 1.¬†¬† The inclusion map \\(\\iota\\) here is just a convenient tool for placing \\(A\\) in \\(X\\), to make \\(\\mathbb{I}\\) and \\(r\\) ‚Äúcomparable‚Äù (have the same range), so that we can talk about homotopy between them.Note 2.¬†¬† The composite map \\(\\iota\\circ r\\) is continuous from \\(X\\) to \\(X\\). For example, let \\(r:X\\to\\{x_0\\}\\) be a retraction to a point, so that \\(\\iota\\circ r\\) is a constant map from \\(X\\) to \\(X\\). Then given an arbitrary open set \\(V\\) in \\(X\\), there are only two possibilities: either \\(V\\) does not contain \\(x_0\\), in which case \\((\\iota\\circ r)^{-1}(V)=\\varnothing\\), or \\(x\\) is in \\(V\\), so that \\((\\iota\\circ r)^{-1}(V)=X\\). In either case, the inverse image is (trivially) open.One should distinguish between retraction and deformation retraction. The former only requires \\(r\\) itself to be continuous, which can be trivially achieved as in Example 1.1. The later requires \\(H\\) to be continuous, thus the notion of ‚Äúcontinuously shrinkage‚Äù. Deformation retraction has stronger requirement on the space \\(X\\).Example 2.1¬†¬† Let \\(U\\) and \\(V\\) be two disjoint open disks in \\(\\mathbb{R}^2\\) and let \\(X=U\\cup V\\). We can imagine \\(X\\) to be two plates separately placed on on a table. Pick a point \\(x_0\\) in \\(V.\\) Then the map \\(r:U\\cup V\\to\\{x_0\\}\\) that sends all points of \\(X\\) to \\(\\{x_0\\}\\) is a retraction. However, it cannot be a deformation retraction. If you just keep shrinking the two disks without moving one into the other, then the resulting space will be two points. You could also map all points of \\(U\\) into \\(V\\) at some time, and then shrink \\(V\\) afterwards, but this construction cannot be continuous. Let‚Äôs say at some time \\(t_0\\in[0,1]\\), \\(U\\) is mapped into an open neighborhood \\(W\\subset V\\) of \\(x_0\\). Then \\(H^{-1}(W)=U\\times[t_0,1]\\cup V\\), which is not open. The problem is that this sudden change causes discontinuity in \\([0,1]\\), contrary to the notion of ‚Äúcontinuously deforming the space‚Äù.Example 2.2¬†¬† The following picture shows that the unit disk \\(D^2\\) in \\(\\mathbb{R}^2\\) can be continuously shrunk to the origin.You might want to carefully distinguish between the homotopy \\(H\\) and the map \\(r\\). The homotopy \\(H\\) is a collection of functions from \\(D^2\\) to a subspace of \\(D^2\\), including all functions shown above, each with a different range. In particular, \\(H\\) includes the identity \\(\\mathbb{I}\\) as well as \\(r\\). The homotopy \\(H\\) organizes them together ‚Äúin a continuous way‚Äù. \\(r:D^2\\to\\{0\\}\\) is a always a retraction. But deformation retraction requires that such an \\(H\\) exists so that we can start from the identity and continuously evolve to \\(r\\).Example 2.3¬†¬† The following picture shows that \\(D^2\\setminus\\{0\\}\\) can be continuously retracted to the unit circle \\(S^1\\).This is pretty intuitive, but why we are not able to do the same thing for \\(D^2\\)?From algebraic topology we know that \\(D^2\\) and \\(S^1\\) have different fundamental groups: for \\(S^1\\) it is \\(\\mathbb{Z}\\), while for \\(D^2\\) it is trivial (namely \\(\\{0\\}\\)). But here let‚Äôs look at an elementary argument: the map \\(r:D^2\\to S^1\\) defined by mapping each ray emanated from the origin to its intersection with \\(S^1\\), and by mapping the origin to any point \\(z\\in S^1\\) cannot be continuous. For a neighborhood \\(V\\) of \\(z\\) such that \\(V\\neq S^1\\), the inverse image of \\(V\\) is the union of an open set in \\(D^2\\) with the (closed) point \\(\\{0\\}\\), thus it cannot be open. Of course, there may be other possibilities, and this is why fundamental groups are useful. But one can see that the origin \\(\\{0\\}\\) is ‚Äútoo annoying‚Äù, and place it in anywhere will cause discontinuity.Example 2.4¬†¬† It is important to note that, in the process of shrinking a space \\(X\\), we must always stay whinin the space \\(X\\) and not go outside of it. So, for example, the following ‚Äúshrinkage‚Äù is not a deformation retraction:Here the space \\(X\\) is a segment of the torus \\(S^1\\times S^1\\). As one tries to shrink it, the resulting spaces are no longer part of \\(X\\), so by our definition such a mapping is not deformation retraction.From the algebra side, the torus segment has \\(\\mathbb{Z}\\times\\{0\\}\\) as its fundamental group, while for a line segment it is trivial, so there cannot be a deformation retraction. Likewise, a full torus \\(S^1\\times S^1\\) has fundamental group \\(\\mathbb{Z}\\times\\mathbb{Z}\\), so it cannot continuously shrink to a circle, whose fundamental group is $\\mathbb{Z}$.By the way, If we replace \\(S^1\\times S^1\\) by \\(D^2\\times S^1\\), i.e., the solid torus, then such shrinkage like the figure above is a deformation retraction.In 2D, we might think of shrinking a circle to its center. The center is not part of the circle, so such shrinkage is also not a deformation retraction. From the algebra side, \\(S^1\\) has fundamental group \\(\\mathbb{Z}\\), while a point has the trivial fundamental group, so no such deformation retraction would exist.III. Homotopy EquivalenceFrom the above discussion we find that, for a deformation retraction \\(r:X\\to A\\), we have \\(\\iota\\circ r\\simeq\\mathbb{I}\\) and \\(r\\circ\\iota\\simeq\\mathbb{I}_{A}\\) (in fact \\(r\\circ\\iota=\\mathbb{I}_{A}\\)). Since in this case, \\(X\\) can be ‚Äúcontinuously deformed‚Äù into \\(A\\), the two spaces are in some sense ‚Äúequivalent‚Äù. We call this equivalence ‚Äúhomotopy equivalence‚Äù:Definition (Homotopy Equivalence)¬†¬† Two topological spaces \\(X\\) and \\(Y\\) are called homotopy equivalent if there exist a continuous map \\(f:X\\to Y\\) and a continuous map \\(g:Y\\to X\\) such that \\(g\\circ f\\simeq\\mathbb{I}_{X}\\) and \\(f\\circ g\\simeq\\mathbb{I}_{Y}\\). The map \\(f\\) is called a homotopy equivalence.SummaryIn this post, I presented definitions of retractions and deformation retractions in algebraic topology, and illustrated their differences by several examples. We can generalize deformation retraction to the concept of homotopy equivalence.Cite as:@article{lifei2022retraction, title = &quot;Deformation Retractions are „ÄåContinuous„Äç Retractions&quot;, author = &quot;Li, Fei&quot;, journal = &quot;https://volagold.github.io&quot;, year = &quot;2022&quot;, url = &quot;https://volagold.github.io/posts/deformation-retraction/&quot;}" }, { "title": "Generative Models: from Noise to Data", "url": "/posts/generative-models/", "categories": "Data Science", "tags": "generative-model, gan, vae, flow", "date": "2022-01-15 13:25:00 +0800", "snippet": " In this post I give an introduction to several generative models in deep learning literature, including adversarial networks (GANs), variational autoencoder (VAE) and flow models. I also derive the formula for evaluating flow densities on generated samples.Deep generative models concern the problem of finding high dimensional data distribution, and new data generation. There are four major types of generative models in the literature: generative adversarial networks (GANs) [1], variational autoencoder (VAE) [2], normalizing flows [3,4,5], plus autoregressive models [6]. In all the first three models, the approach to data generation is to make use of some noise space, denoted as \\(\\mathcal{Z}\\). A mapping from the noise space to data space is learned, using training data and various training methods. To obtain a sample from the generative model, one first samples a noise vector \\(z\\) from the noise space \\(\\mathcal{Z}\\), and then inputs the noise vector to the learned mapping, and obtains generated sample from the model output. The intuition that why one should generate data from noise space via a learned mapping may come from the observation that for many common distributions like Gaussian, samples can be generated as a transformation of noises.The three classes of models differ in interpretation of the noise space, model specification and learning methods. In GAN, a direct mapping from the noise space to data space is specified, where the noise space typically has smaller dimension than the data space. To train the model, one adds a classifier that discriminates between real and generated data, and jointly trains the two models through playing a minimax game. In VAE the mapping between noise space and data space is formulated in a probabilistic way, so that we can obtain the formula for likelihood on data. The likelihood is intractable to compute, so instead the model is trained by maximizing a lower bound on the log density. Flow models come with both exact likelihood evaluation as well as sample generation. Jacobians of the transformations between the two spaces are used to compute the likelihood.We now discuss these three generative models in in some detail. We shall use the following notations: \\(\\mathcal{X}\\) denotes data space, which is usually a subset of \\(\\mathbb{R}^d\\) for some dimension \\(d\\). \\(\\mathcal{Z}\\) denotes noise space, and \\(Z\\) denotes a generic random variable taking values in \\(\\mathcal{Z}\\) with distribution \\(p_Z\\), for example a standard multivariate Gaussian.Three Classes of Generative ModelsGenerative Adversarial NetworksIn GAN, a data generator \\(g_\\alpha\\) from noise space to data space is specified:\\[\\begin{gathered}g_\\alpha: \\mathcal{Z}\\to \\mathcal{X}\\\\z\\mapsto x\\end{gathered}\\]which is typically parametrized by neural networks. A simple distribution on \\(\\mathcal{Z}\\), like a standard Gaussian \\(p_Z\\), is also specified. We would like to train the generator \\(g_\\alpha\\) so that it transforms the distribution \\(p_Z\\) on \\(\\mathcal{Z}\\) to the data distribution \\(p_{\\text{data}}\\) on \\(\\mathcal{X}\\) for as close as possible, i.e., given \\(z\\sim p_Z\\), the output \\(g_\\alpha(z)\\) should closely follow the data distribution. To achieve this goal, one adds a second discriminator\\[D_\\theta: \\mathcal{X} \\to [0,1]\\]that assigns its input a probability score indicating how likely it is from the true data distribution. If \\(D_\\theta(x)\\) is close to \\(1\\) then it may classify \\(x\\) as coming from the true data distribution, while if \\(D_\\theta(x)\\) is close to \\(0\\) then it may classify \\(x\\) as coming from the generator \\(g_\\alpha\\). One then jointly trains the two model together by performing\\[\\min_\\alpha\\max_\\theta V(\\alpha, \\theta)\\]with$$V(\\alpha, \\theta) = \\mathbb{E}_{x\\sim p_{\\text{data}}}\\log D_\\theta(x) + \\mathbb{E}_{z\\sim p_Z}\\log [1 - D_\\theta(g_\\alpha(z))].$$We see that \\(\\max_\\theta V(\\alpha, \\theta)\\) means pushing up \\(D_\\theta\\) on data manifold while pushing down \\(D_\\theta\\) on generated output from \\(g_\\alpha\\). On the other hand, \\(\\min V(\\alpha, \\theta)\\) means finding \\(\\alpha\\) so that \\(g_\\alpha(z)\\) are located at places where \\(D_\\theta\\) has large values, which should ideally be closer and closer to the true data distribution as training proceeds, from the update of \\(D_\\theta\\) in \\(\\max_\\theta V(\\alpha, \\theta)\\) just mentioned.Variational AutoencodersA variational autoencoder consists of a probabilistic decoder \\(p_\\alpha(x\\mid z)\\) from latent space \\(\\mathcal{Z}\\) to data space \\(\\mathcal{X}\\), and a probabilistic encoder \\(q_\\phi(z\\mid x)\\) from \\(\\mathcal{X}\\) to \\(\\mathcal{Z}\\), both of which are parametrized by neural networks. Given \\(z\\) sampled from \\(p_Z\\), the decoder returns a conditional probability distribution \\(p_\\alpha(\\cdot\\mid z)\\) on \\(\\mathcal{X}\\). The unconditional density \\(p_\\alpha(x)\\) is\\[p_\\alpha(x) = \\int p_\\alpha(x\\mid z)p_Z(z)dz\\]One would like to use the above equation to perform maximum likelihood estimation on training data. However, the integral is intractable to compute. Instead, one has to use the encoder to obtain a tractable lower bound on the log density, and maximize the lower bound:$$\\begin{split}\\log p_\\alpha(x) &amp;amp;= \\log \\int p_\\alpha(x\\mid z)p_Z(z)dz\\\\&amp;amp;= \\log \\int q_\\phi(z\\mid x)\\frac{p_\\alpha(x\\mid z)p_Z(z)}{q_\\phi(z\\mid x)}dz\\\\&amp;amp;\\geq \\int q_\\phi(z\\mid x)\\log\\frac{p_\\alpha(x\\mid z)p_Z(z)}{q_\\phi(z\\mid x)}dz\\\\[1em]&amp;amp;= \\mathbb{E}_{z\\sim q_\\phi(z\\mid x)}\\log p_\\alpha(x\\mid z) - KL[ q_\\phi(z\\mid x)|| p_Z(z)]\\\\[1em]&amp;amp;=: \\mathcal{L}_{\\alpha,\\phi}(x),\\end{split}$$where the inequality is from Jensen‚Äôs inequality. To compute the lower bound \\(\\mathcal{L}_{\\alpha,\\phi}(x)\\) on training data \\(x^{(i)}\\), we send the data to the encoder, to obtain a distribution \\(q_\\phi(z\\mid x^{(i)})\\) on \\(\\mathcal{Z}\\). Then we can sample \\(z\\) from this distribution and feed \\(z\\) to the decoder network and evaluate the decoder distribution on \\(x^{(i)}\\). This is the first term \\(\\mathbb{E}_{z\\sim q_\\phi(z\\mid x^{(i)})}\\log p_\\alpha(x^{(i)}\\mid z)\\) in \\(\\mathcal{L}_{\\alpha,\\phi}(x)\\). We are also able to compute the KL divergence between \\(q_\\phi(z\\mid x^{(i)})\\) and the prior distribution \\(p_Z(z)\\) on \\(\\mathcal{Z}\\), for example a standard Gaussian. Design choices of the decoder \\(p_\\alpha(x\\mid z)\\) depend on the type of data one is modeling. Common choice is to output multivariate Bernoulli for discrete data and multivariate Gaussian for continuous data. The prior distribution \\(p_Z\\) as well as the encoder \\(q_\\phi(z\\mid x)\\), however, should be assumed to be continuous. The reason is explained below.To compute the gradient of \\(\\mathcal{L}_{\\alpha,\\phi}(x)\\) with respect to \\(\\phi\\), we have to differentiate through an expectation on the latent variables. We can use a technique called the reparameterization trick to avoid computing the integral of the expectation. We first describe it in general terms, then apply it to the current case.Given a random variable \\(X\\) with density \\(q_\\alpha\\), where \\(\\alpha\\) is some set of parameters, and a scalar function \\(f\\), the problem is to compute the gradient of the expectation of \\(f\\) with respect to \\(\\alpha\\):\\[\\nabla_\\alpha \\mathbb{E}_{x\\sim q_\\alpha}f(x) = \\nabla_\\alpha \\int f(x)q_\\alpha(x)dx.\\]\\(f\\) may or may not depend on parameter \\(\\alpha\\), but for simplicity we assume it doesn‚Äôt. If \\(X\\) can be expressed as a differentiable transformation of random variable \\(Z\\) with simple density \\(p_Z\\), for example a Gaussian or uniform distribution:\\[x = g_\\alpha(z)\\quad \\text{for some }z\\sim p_Z,\\]then we are able to move the expectation operation out:\\[\\begin{split}\\nabla_\\alpha \\mathbb{E}_{x\\sim q_\\alpha}f(x) &amp;amp;= \\nabla_\\alpha \\mathbb{E}_{z\\sim p_Z}f(g_\\alpha(z))\\\\&amp;amp;= \\mathbb{E}_{z\\sim p_Z}\\nabla_\\alpha f(g_\\alpha(z)).\\end{split}\\]In particular, when we have finite samples \\(\\{x_\\alpha^{(1)},\\ldots,x_\\alpha^{(N)}\\}\\) from \\(q_\\alpha\\), and we‚Äôd like to compute\\(\\nabla_\\alpha\\frac{1}{N}\\sum_{i=1}^Nf\\left(x_\\alpha^{(i)}\\right),\\)we can take the random numbers \\(\\{z^{(1)},\\ldots,z^{(N)}\\}\\) that are used to generate the samples, and compute\\(\\nabla_\\alpha\\frac{1}{N}\\sum_{i=1}^Nf\\left(g_\\alpha\\left(z^{(i)}\\right)\\right).\\)An example is when \\(q_\\alpha\\) is Gaussian with some mean \\(\\mu_\\alpha\\) and variance \\(\\sigma_\\alpha\\) that are both some closed-form functions of the parameter \\(\\alpha\\). In this case \\(g_\\alpha\\) is simply an affine transformation of standard Gaussian:\\[g_\\alpha(z) = \\mu_\\alpha + \\sigma_\\alpha\\cdot z \\quad\\text{where } z\\sim \\mathcal{N}(0,I)\\]and the gradient computation becomes\\[\\nabla_\\alpha\\frac{1}{N}\\sum_{i=1}^Nf\\left(\\mu_\\alpha + z^{(i)}\\cdot \\sigma_\\alpha\\right).\\]In VAE, the prior distribution of latent space \\(p_Z\\) is often assumed to be standard Gaussian, and in the training process the aim is to match the encoder network \\(q_\\phi(z\\mid x)\\) to this prior for as much as possible. In view of the reparameterization trick explained above, we can directly formulate the encoder network \\(q_\\phi(z\\mid x)\\) as \\(\\mu_\\phi(x)\\) and \\(\\sigma_\\phi(x)\\) where they both can be some complicated nonlinear transformation of the input, for example fully connected neural networks. To sample noise/latent variable \\(z\\) from \\(q_\\phi(z\\mid x)\\), we simply sample \\(\\epsilon\\sim \\mathcal{N}(0,I)\\) and transform the noise as\\[z = \\mu_\\phi(x) + \\epsilon\\cdot\\sigma_\\phi(x).\\]Then we can feed the \\(z\\) to the objective \\(\\log p(x\\mid z)\\). In particular, we can write the gradient of the the first term with respect to \\(\\phi\\) in the lower bound on training data \\(\\{x^{(1)},\\ldots,x^{(N)}\\}\\) as\\[\\nabla_\\phi\\frac{1}{N}\\sum_{i=1}^N\\log p_\\alpha\\left(x^{(i)}\\mid \\mu_\\phi(x^{(i)}) + \\epsilon_i\\cdot\\sigma_\\phi(x^{(i)})\\right).\\]The reparameterization trick works only if the distribution with respect to which we are differentiating can be expressed as a differentiable (and in particular, continuous) transformation of simple noise distributions. In case the distribution is discrete, it is not possible to express the samples as such differentiable transformations, and one has to rely on other techniques.Normalizing FlowsA normalizing flow model is an invertible mapping from data space to a noise space with the same dimensionality:\\[\\begin{gathered}f_\\alpha: \\mathcal{X} \\to \\mathcal{Z}\\\\x \\mapsto z,\\end{gathered}\\]together with a simple distribution \\(p_Z\\) on \\(\\mathcal{Z}\\) like a standard Gaussian. To sample from the model, one first samples from the noise distribution \\(z\\sim p_Z\\) and invert the mapping to obtain \\(\\tilde{x}=f^{-1}_\\alpha(z)\\). The distinct characteristic of flow models is that one can leverage the change of variable formula for probability distributions to obtain exact likelihood evaluation \\(p_\\alpha(x)\\) on \\(\\mathcal{X}\\):\\[p_\\alpha(x) = p_Z(f_\\alpha(x))\\cdot\\left|\\frac{\\partial}{\\partial x}f_\\alpha(x)\\right|,\\]or\\[\\log p_\\alpha(x) = \\log p_Z(f_\\alpha(x))+ \\log\\left|\\frac{\\partial}{\\partial x}f_\\alpha(x)\\right|,\\]where \\(\\left\\lvert\\frac{\\partial}{\\partial x}f_\\alpha(x)\\right\\rvert\\) denotes the absolute value of the determinant of the Jacobian of the transformation \\(f_\\alpha\\) at point \\(x\\in\\mathcal{X}\\). Since we have exact log-likelihood, we can use MLE to train the model. One chooses the invertible transformation \\(f_\\alpha\\) in such a way that this log determinant of Jacobian is easy to compute. Recall that if a square matrix \\(A\\) is triangular, then \\(\\det(A)\\) is the product of the elements along the diagonal, so \\(\\log\\lvert\\det(A)\\rvert\\) is simply the summation of its diagonal entries after the log function is applied element-wise. Thus, one way to design a flow model is to use some basic transformations \\(h_{\\alpha_1},\\ldots,h_{\\alpha_m}\\) as building blocks, all of which have such triangular Jacobian matrices, and compose them together to obtain the model as\\[f_\\alpha = h_{\\alpha_m}\\circ\\cdots\\circ h_{\\alpha_1}.\\]To compute model output \\(\\log p_\\alpha(x)\\), we map \\(x\\) through all the transformations to \\(f_\\alpha(x)\\), evaluate \\(p_Z\\) at \\(f_\\alpha(x)\\), then add all log absolute determinant of Jacobians of all transformations.Examples of flow model include NICE [3], Real NVP [4], and Glow [5]. The basic building block of such model is the affine coupling layer. For an input \\(x=(x_1,\\ldots,x_d)\\) with dimension \\(d\\), the output of the affine coupling layer \\(y=(y_1,\\ldots,y_d)\\) is\\[\\begin{cases}y_{1:d&#39;} = x_{1:d&#39;},\\\\[0.2cm]y_{d&#39;+1:d} = x_{d&#39;+1:d} \\odot s(x_{1:d&#39;}) + t(x_{1:d&#39;})\\end{cases}\\]where \\(d&#39;&amp;lt;d\\). \\(s(\\cdot)\\) is the scale function and \\(t(\\cdot)\\) is the translation function, both of which can be complex transformation of their inputs, for example fully connected neural networks or convolutional neural networks. The Jacobian of the transformation is lower-triangular:\\[\\frac{\\partial y}{\\partial x^T} = \\begin{pmatrix} I_{d&#39;} &amp;amp; 0 \\\\[0.2cm] * &amp;amp; \\mathrm{diag}(s(x_{1:d&#39;}))\\end{pmatrix}\\]where \\(*\\) is the part that is not relevant for density calculation, and \\(\\mathrm{diag}(s(x_{1:d&#39;}))\\) is the diagonal matrix whose diagonal vector is \\(s(x_{1:d&#39;})\\). We see that the log determinant of the affine coupling transformation is simply \\(\\texttt{sum}(\\log\\lvert s\\rvert)\\), namely we just need to apply absolute value and the the log function element-wise, and then sum the result. The transformation is also easily invertible:\\[\\begin{cases}x_{1:d&#39;} = y_{1:d&#39;},\\\\[0.2cm]x_{d&#39;+1:d} = (y_{d&#39;+1:d} - t(x_{1:d&#39;})) \\odot s^{-1}(x_{1:d&#39;}).\\end{cases}\\]Affine Coupling LayerAutoregressive flowsAutoregressive flow models [7,8,9] can be seen as a specific class of normalizing flows. In autoregressive flow, an input data \\(x=(x_1,x_2,\\ldots,x_d)\\in\\mathbb{R}^d\\) is mapped to the output noise \\(z=(z_1,z_2,\\ldots,z_d)\\in\\mathbb{R}^d\\) through the following structure:\\[\\begin{aligned}z_1 &amp;amp;= f_\\alpha(x_1)\\\\z_2 &amp;amp;= f_\\alpha(x_2; x_1)\\\\z_3 &amp;amp;= f_\\alpha(x_3; x_1, x_2)\\\\\\cdots &amp;amp; \\\\\\end{aligned}\\]Namely, the transformation \\(f_\\alpha\\) is such that the first output dimension only depends on the first input dimension, the second output dimension only depends on the first two input dimensions, and so on. With this design, the Jacobian of \\(f_\\alpha\\) is naturally lower-triangular, so that we can obtain fast and exact likelihood evaluation in one single pass. One way to ensure this dependence of output dimension on input dimension is through multiplying the weights in fully connected networks by binary masks, an approach taken in MADE [7].MADE ModelOn the other hand, sampling time is linear in the number of dimensions: \\(x_2\\) has to be obtained after \\(x_1\\) is computed, \\(x_3\\) has to be obtained after both \\(x_1\\) and \\(x_2\\) are computed, and so on:\\[\\begin{aligned}\\tilde{x}_1 &amp;amp;= f_\\alpha^{-1}(z_1)\\\\\\tilde{x}_2 &amp;amp;= f_\\alpha^{-1}(z_2; \\tilde{x}_1)\\\\\\tilde{x}_3 &amp;amp;= f_\\alpha^{-1}(z_3; \\tilde{x}_1,\\tilde{x}_2)\\\\\\cdots &amp;amp; \\\\\\end{aligned}\\]Autoregressive flows are first derived from autoregressive models [6], which model data distribution \\(p(x)\\) as the product of conditionals\\[p(x) = \\prod_{i=1}^d p(x_i\\mid x_1,\\ldots, x_{i-1}),\\]where each conditional distribution could have some neural network structure with trainable parameters, and MLE is usually used to train the model.Autoregressive ModelWhereas flow models typically only model continuous data, autoregressive models can model both continuous and discrete data.Sampling from an autoregressive model is sequential: to obtain a sample \\((\\tilde{x}_1,\\ldots,\\tilde{x}_d)\\in\\mathbb{R}^d\\) from the model, one has to first sample \\(\\tilde{x}_1\\) from \\(p(x_1)\\), then sample \\(\\tilde{x}_2\\) from \\(p(x_2\\mid \\tilde{x}_1)\\), then sample \\(\\tilde{x}_3\\) from \\(p(x_3\\mid \\tilde{x_1},\\tilde{x}_2)\\), and so on. Under some specification, the sample could be a differentiable and invertible transformation of some random noise. For example, if we model each conditional \\(p(x_i\\mid x_1,\\ldots,x_{i-1})\\) as Gaussian with mean \\(\\mu_i=\\mu_i(x_1,\\ldots,x_{i-1})\\) and variance \\(\\sigma_i=\\sigma_i(x_1,\\ldots,x_{i-1})\\) that are some complex transformations of \\(x_1,\\ldots,x_{i-1}\\), then to sample \\(\\tilde{x}_i\\sim p(x_i\\mid x_1,\\ldots,x_{i-1})\\) is the same as to compute$$\\tilde{x}_i=\\mu_i + z_i \\cdot \\sigma_i \\quad\\text{with } z_i\\sim \\mathcal{N}(0,1),\\quad \\forall~i=1,\\ldots,d.$$In the other way,$$z_i=(x_i - \\mu_i) / \\sigma_i,\\quad \\forall~i=1,\\ldots,d$$and we see that the above two equations have the same structure as for autoregressive flow models. In this case, the autoregressive model is an autoregressive flow. The log absolute determinant of the Jacobian of this transformation is very easy to compute. Incidentally, we also see that the equation$$\\tilde{x}_i=\\mu_i + z_i \\cdot \\sigma_i \\quad\\text{with } z_i\\sim \\mathcal{N}(0,1),\\quad \\forall~i=1,\\ldots,d.$$is very similar in form to the reparameterization trick in VAE. The idea of both is to express samples as transformation of simple noises, whose derivatives are easy to compute.Viewing an autoregressive model as a flow model also opens up the possibility of stacking multiple such models together so as to increase the complexity of the model. This is the approach taken in Masked Autoregressive Flow (MAF) [8]. The drawback of such autoregressive flow models is their lack of flexibility. If it is not the case that$$p(x_i\\mid x_1,\\ldots,x_{i-1}) \\quad\\forall i=1,\\ldots,d$$are Gaussians, which could be highly likely for real world data, then the model can have poor fit. An autoregressive model, on the other hand, does not make such assumptions, so it could be more flexible.Evaluate density on generated samplesFinally, for flow models, we derive the formula for $p_\\alpha(\\tilde{x})$, where $\\tilde{x}=f_\\alpha^{-1}(z)$ (with $z\\sim p_Z$) is a generated sample. To start, plug in the equation for $\\tilde{x}$:\\[\\begin{split}p_\\alpha(\\tilde{x}) &amp;amp;= p_Z(f_\\alpha(\\tilde{x}))\\cdot\\left|\\frac{\\partial}{\\partial x}f_\\alpha(\\tilde{x})\\right|\\\\&amp;amp;= p_Z(f_\\alpha\\circ f_\\alpha^{-1}(z))\\cdot\\left|\\frac{\\partial}{\\partial x}f_\\alpha(f_\\alpha^{-1}(z))\\right|\\\\&amp;amp;= p_Z(z)\\cdot\\left|\\frac{\\partial}{\\partial x}f_\\alpha(f_\\alpha^{-1}(z))\\right|.\\\\\\end{split}\\]From the chain rule in calculus and the determinant of the inverse, we get\\[\\begin{split}\\left|\\frac{\\partial}{\\partial x}f_\\alpha(f_\\alpha^{-1}(z))\\right| &amp;amp;= \\left|\\left(\\frac{\\partial}{\\partial z}f_\\alpha^{-1}(z)\\right)^{-1}\\right| \\\\&amp;amp;= \\left|\\frac{\\partial}{\\partial z}f_\\alpha^{-1}(z)\\right|^{-1}.\\\\\\end{split}\\]Taking $\\log$ on both sides gives us:$$\\log\\left|\\frac{\\partial}{\\partial x}f_\\alpha(f_\\alpha^{-1}(z))\\right| = -\\log\\left|\\frac{\\partial}{\\partial z}f_\\alpha^{-1}(z)\\right|.$$We thus arrived at the formula:$$\\log p_\\alpha(\\tilde{x}) = \\log p_Z(z) - \\log\\left|\\frac{\\partial}{\\partial z}f_\\alpha^{-1}(z)\\right|.$$Namely, $\\log p_\\alpha(\\tilde{x})$ can be computed as the log density on noise samples $z$ minus the log-determinant of the Jacobians of the inverse transformation. The later is a standard output in common implementations of flow models.The formula tells us that after we generate samples $\\tilde{x}$ from the flow model through the inverse transformation, we can evaluate the density $p_\\alpha(\\tilde{x})$ for free. There is no need to go through additional calculations.Cite as:@article{lifei2022generative, title = &quot;Generative Models: from Noise to Data&quot;, author = &quot;Li, Fei&quot;, journal = &quot;https://volagold.github.io&quot;, year = &quot;2022&quot;, url = &quot;https://volagold.github.io/posts/generative-models/&quot;}References[1] I. Goodfellow et al. ‚ÄúGenerative adversarial nets‚Äù. Advances in neural information processing systems. 2014, pp. 2672‚Äì2680.[2] D. P. Kingma and M. Welling. ‚ÄúAuto-encoding variational bayes‚Äù. arXiv preprint arXiv: 1312.6114 (2013).[3] L. Dinh, D. Krueger, and Y. Bengio. ‚ÄúNice: Non-linear independent components estimation‚Äù. arXiv preprint arXiv:1410.8516 (2014).[4] L. Dinh, J. Sohl-Dickstein, and S. Bengio. ‚ÄúDensity estimation using real nvp‚Äù. arXiv preprint arXiv:1605.08803 (2016)[5] D. P. Kingma and P. Dhariwal. ‚ÄúGlow: Generative flow with invertible 1x1 convolutions‚Äù. Advances in Neural Information Processing Systems. 2018, pp. 10215‚Äì10224.[6] A. v. d. Oord, N. Kalchbrenner, and K. Kavukcuoglu. ‚ÄúPixel recurrent neural networks‚Äù. arXiv preprint arXiv:1601.06759 (2016).[7] M. Germain et al. ‚ÄúMADE: Masked autoencoder for distribution estimation‚Äù. International Conference on Machine Learning. 2015, pp. 881‚Äì889[8] G. Papamakarios, T. Pavlakou, and I. Murray. ‚ÄúMasked autoregressive flow for density estimation‚Äù. Advances in Neural Information Processing Systems. 2017, pp. 2338‚Äì2347.[9] C.-W. Huang et al. ‚ÄúNeural autoregressive flows‚Äù. arXiv preprint arXiv:1804.00779 (2018)" }, { "title": "Building a website with Jekyll and GitHub Pages", "url": "/posts/build-a-static-website/", "categories": "Programming", "tags": "web-development, html", "date": "2022-01-06 18:00:00 +0800", "snippet": " An overview of the Jekyll framework, instructions about hosting your website on GitHub Pages, and a comparison among different options for writing technical blogs.The Jekyll FrameworkJekyll is a website generator. It is a software package written in Ruby. To use it on your own machine, first install Ruby, and then install the package via Ruby gems:gem install jekyll bundlerMore detailed installation instructions are available on the official website:https://jekyllrb.com/With the framework, you don‚Äôt have to write bare HTML code. You write your blog posts in markdown files, and structure layouts and elements of your website according to the framework. Then you can calljekyll sand Jekyll will convert the sources to static website files (HTML, CSS, JS) that are ready to be served.LiquidDefine layouts of different pages of your website using the Liquid template language. Liquid lets you write HTML pages in a programmable way. An example below is clear:&amp;lt;ul&amp;gt; {% for post in site.posts %} &amp;lt;li&amp;gt; &amp;lt;a href=&quot;{{ post.url }}&quot;&amp;gt;{{ post.title }}&amp;lt;/a&amp;gt; &amp;lt;/li&amp;gt; {% endfor %}&amp;lt;/ul&amp;gt;You have all the basics of a programming language, e.g. arithmetic operations, logic operators, control flows like if, else , for, as well as built-in functions (called ‚Äúfilters‚Äù in Liquid) like abs and upcase.Jekyll also makes available some variables to reference data on your website. The site variable for site wise information. For example, site.posts returns a list of all posts. You can define custom variables in _config.yml. For example, with foo: bar defined in _config.yml, site.foo will have value bar. The page variable for page specific information. For example, page.title is the title of the page, page.url is the relative url of the page. You can define custom variables in front matter.For a complete list of available variables go to official docs on variables.Site StructureThe basic structure in your website folder is typically as follows..|-- /_includes|-- /_layouts|-- /_posts|-- /_sass|-- /assets|-- _config.yml|-- index.html The _config.yml file is where you put the metadata about the website, like url, site title, author, social links, and all other configurations. You can reference values defined here using the site variable. The _posts folder is where you put all your markdown blog posts. You may have several pages on your website, like a home page, an about page, a tag page showing all posts with some tag, or some other pages you wish to add. Define each page layout in the _layouts folder. Define page styles in _sass folder. You can put all your files like images and pdf documents in an assets folder. You may wish to add some partials to your website, like a banner, footer, paginator, word counts, share buttons and so on. You define each of them in _includes folder. Then you can include them in your layout by putting {% include something.html %} in your layout file. The include tag is deprecated in Liquid 5.00, and it is advised to use the render tag instead. As of 2022, Jekyll is still using Liquid version 4, but may switch to version 5 in the future. If you wonder how Jekyll does the magic under the hood, well, there is no magic. It uses regular expression extensively to recognize each piece of liquid tags and html tags. Rules for recognition and conversion are all hard coded. The framework does all the dirty work so you don‚Äôt have to worry about them. Refer to jekyll and kramdown source code for detail.ThemesAt this point, you might have realized that, you still need to write a good amount of code if you want to build a full-fledged website. Luckily, many people have shared their website templates as themes. You can directly use a theme so that you don‚Äôt have to start from scratch and worry every details about HTML, CSS and JavaScript, but instead just focus on your content creation. View Jekyll themes at the following websites: jamstackthemes.dev jekyllthemes.org jekyllthemes.io jekyll-themes.comOn the other hand, it may be awkward to have your website look exactly the same as someone else‚Äôs. After downloaded a theme, you can start messing around and modify the code in a theme for customization. Don‚Äôt forget to put all the dependencies of a theme into your Gemfile or _config.yml. See the official docs ‚Äúconverting gem-based themes to regular themes‚Äù for more details.Finally, I should mention that when you initialize your web project usingjekyll new my-sitethe minima theme will be installed by default. It is a good starter and is already good enough if you just want to write blogs and do not intend to add much other pages. You can easily customize your website based on the minima theme.When you are finished developing your website, the next step is to push your folder to GitHub.Publish your site on GitHub PagesHosting your website on GitHub Pages is really easy.Name your repository username.github.io where username is your GitHub username. Push this repository to GitHub and you are done. Now you can open your browser, go to uername.github.io and see your website! There is no need to worry about web server or database or anything.With a default GitHub account, the repository for your website has to be a public repository. If you change it to a private one, your published website will no longer be available. If you want to host your website in a private repository while still making your website available, you need to upgrade to a GitHub pro account, which costs you $4 per month as of 2022. This is still cheaper than many website builder like wix and Squarespace, which would cost you around $12 per month.Using a custom domain nameYou can use your own domain name by putting the domain name in a file named CNAME in your repository.In addition, you need to tell your DNS provider to redirect your domain to GitHub servers. You DNS provider is usually the same as your domain provider, e.g. NameCheap, GoDaddy or Google Domains. After all, they don‚Äôt know where you intent to host your website, being it AWS, Google Cloud, your own machine or GitHub. You have to tell them this information. To do so, login on your domain provider‚Äôs website and create an A resource record, adding IP addresses for GitHub Pages.185.199.108.153185.199.109.153185.199.110.153185.199.111.153More instructions are available at GitHub Pages documentation.Should you use Wix/Squarespace?Should you build your blog website using drag-and-drop website builders like wix and Squarespace? My point of view is, if you know programming, then you should use one of the frameworks like Jekyll (others include Gatsby, Hexo, Hugo). After all, you are building a static website for blogs, not a millionaire web app. You are not dealing with the backends. If you are satisfied with using a theme, just go ahead and use it, which doesn‚Äôt require much coding effort at all. If you want customization, you have the freedom to tweak a little bit HTML and CSS so you control exactly your website‚Äôs appearance. You can decide on your own where to host your website. You can host it via GitHub Pages, or cloud services like AWS and Google Cloud if you prefer. This flexibility can be very useful if you decide to change your decision later.If you simply want to write tech blogs, their price is probably a bit too expensive. It can be even more time-consuming to drag around and adjust elements until you are satisfied with the design. It is also difficult to save your work. What if you want to switch to another platform, like from wix to Squarespace, or from wix to WordPress, or to some other platforms? Basically you have to start over again. But you never worry about losing your work if you have all your source code at your disposal.Should you write your blogs on Medium?Medium is probably the most popular online publishing platform. With publications like towards data science, it also has a large community of ML/AI lovers. There could be many potential benefits for writing blogs on medium. First, you can actually make money on medium, through the Medium Partnership Program. For many people it‚Äôs even their full-time job. Second, you benefit from a large pool of audience. It‚Äôs easier for people to come across and discover your articles. And third, you are free from all the hassles of writing code in order to build your website. You can just focus on your writings.However, the biggest problem with medium for tech writers is that, they have zero support for LaTeX, as well as code highlighting. The reason is simple. The majority of users write or read about cars, food, emotions, money and more, but never differential equations or Python programming. People who need to write math equations occupy only a small portion. Adding those supports means loading additional JS libraries, which can slow down the website and cause troubles for all users. They decide that it‚Äôs not worthy to do that. The result in the end is that, your article could look terrible, especially if you have a lot of inline math and code snippets. Even though you can embed GitHub Gist for some code highlighting, the overall format quality for a math-intensive article would look too bad. Other drawbacks include lack of support of the Medium Partnership Program for certain countries.Which one to chooseSo I‚Äôm not advocating writing math intensive contents like deep learning on current third party platforms. Rather the best way to do so is to create your own website so that you can have exact control of your content qualities. With that said, building your website is not without any effort. There can be bugs in your code that can cost you hours to solve. Sometimes unexpected things happen, like the CDNs you are using are down, for which you have to fix it on your own. Also, it can be more difficult for people to discover and recognize your new website.Below is a comparison between the three ways of writing technical blogs. Every way has its pros and cons. When choosing among different options, you have to be clear what is the most important principle to you. Although third party services have made it so convenient to publish your own contents, there are trade-offs for depending on such services, and you have to keep that in mind. method pros cons build your own üîπflexibility and controlüîπmany available themesüîπlow cost üîπrequire programming knowledgeüîπhave to debugüîπdifficult to acquire audience wix/squarespace üîπno coding requiredüîπplatform support üîπhigh costüîπlack of flexibilityüîπdifficult to transfer to other platforms medium üîπno coding requiredüîπmore audienceüîπ potential to generate income üîπlack of support for mathüîπlack of support for code highlightingüîπdifficult to transfer to other platforms SummaryIn this post I introduced how to use Jekyll and GitHub Pages to build and host a static website. In Jekyll you can use the Liquid template language to write your HTML page layouts. Many themes are also available for free. Push your repo named username.github.io to GitHub to serve your website to the public. I also compared developing your own website versus the option of using a third-party website builder or writing on medium. My suggestion is to develop your own site, since this gives you the ultimate flexibility and control.Cite as:@article{lifei2022website, title = &quot;Building a website with Jekyll and GitHub Pages&quot;, author = &quot;Li, Fei&quot;, journal = &quot;https://volagold.github.io&quot;, year = &quot;2022&quot;, url = &quot;https://volagold.github.io/posts/build-a-static-website/&quot;}" } ]
